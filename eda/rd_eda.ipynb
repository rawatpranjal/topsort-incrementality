{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression Discontinuity Design (RDD) Analysis - Tables Only\n",
    "## Topsort Incrementality - Causal Inference via Rank Discontinuity\n",
    "\n",
    "This notebook implements RDD analysis using auction rank as the running variable to identify causal effects of ad impressions on purchases.\n",
    "All outputs are in tabular format for text-based analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Connected to Snowflake\n",
      "ðŸ“… Analysis period: 2025-07-01 00:00:00 to 2025-07-02 00:00:00\n",
      "ðŸ”¬ Sample rate for heavy queries: 10.0%\n",
      "â° Attribution window: 60 minutes\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "from dotenv import load_dotenv\n",
    "import snowflake.connector\n",
    "from tabulate import tabulate\n",
    "\n",
    "# Setup\n",
    "load_dotenv()\n",
    "\n",
    "# PARAMETERIZATION - Easy to modify time windows\n",
    "ANALYSIS_START = '2025-07-01 00:00:00'\n",
    "ANALYSIS_END = '2025-07-02 00:00:00'\n",
    "SAMPLE_RATE = 0.1  # For TABLESAMPLE when needed\n",
    "ATTRIBUTION_WINDOW_MINUTES = 60  # Post-click attribution\n",
    "\n",
    "# Connection\n",
    "conn = snowflake.connector.connect(\n",
    "    user=os.getenv('SNOWFLAKE_USER'),\n",
    "    password=os.getenv('SNOWFLAKE_PASSWORD'),\n",
    "    account=os.getenv('SNOWFLAKE_ACCOUNT'),\n",
    "    warehouse=os.getenv('SNOWFLAKE_WAREHOUSE', 'COMPUTE_WH'),\n",
    "    database='INCREMENTALITY',\n",
    "    schema='INCREMENTALITY_RESEARCH'\n",
    ")\n",
    "cursor = conn.cursor()\n",
    "\n",
    "def run_query(query):\n",
    "    \"\"\"Execute query and return DataFrame\"\"\"\n",
    "    cursor.execute(query)\n",
    "    results = cursor.fetchall()\n",
    "    if results:\n",
    "        columns = [desc[0] for desc in cursor.description]\n",
    "        return pd.DataFrame(results, columns=columns)\n",
    "    return pd.DataFrame()\n",
    "\n",
    "def show_table(df, title=\"\", max_rows=None):\n",
    "    \"\"\"Display DataFrame as formatted table\"\"\"\n",
    "    if title:\n",
    "        print(f\"\\n{title}\")\n",
    "        print(\"=\"*len(title))\n",
    "    if max_rows and len(df) > max_rows:\n",
    "        df_display = df.head(max_rows)\n",
    "        print(tabulate(df_display, headers='keys', tablefmt='grid', showindex=False))\n",
    "        print(f\"... ({len(df) - max_rows} more rows)\")\n",
    "    else:\n",
    "        print(tabulate(df, headers='keys', tablefmt='grid', showindex=False))\n",
    "\n",
    "print(f\"âœ… Connected to Snowflake\")\n",
    "print(f\"ðŸ“… Analysis period: {ANALYSIS_START} to {ANALYSIS_END}\")\n",
    "print(f\"ðŸ”¬ Sample rate for heavy queries: {SAMPLE_RATE*100}%\")\n",
    "print(f\"â° Attribution window: {ATTRIBUTION_WINDOW_MINUTES} minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Running Variable Distribution Analysis\n",
    "\n",
    "Check for manipulation of RANKING around potential cutoffs. A smooth distribution implies no strategic manipulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running Variable Distribution (RANKING)\n",
      "=======================================\n",
      "+-----------+------------+------------------+-----------------+\n",
      "|   RANKING | num_bids   | prev_rank_bids   |   RATIO_TO_PREV |\n",
      "+===========+============+==================+=================+\n",
      "|         1 | 6,724,843  | N/A              |                 |\n",
      "+-----------+------------+------------------+-----------------+\n",
      "|         2 | 6,653,098  | 6,724,843.0      |           0.989 |\n",
      "+-----------+------------+------------------+-----------------+\n",
      "|         3 | 6,605,526  | 6,653,098.0      |           0.993 |\n",
      "+-----------+------------+------------------+-----------------+\n",
      "|         4 | 6,569,492  | 6,605,526.0      |           0.995 |\n",
      "+-----------+------------+------------------+-----------------+\n",
      "|         5 | 6,540,178  | 6,569,492.0      |           0.996 |\n",
      "+-----------+------------+------------------+-----------------+\n",
      "|         6 | 6,514,679  | 6,540,178.0      |           0.996 |\n",
      "+-----------+------------+------------------+-----------------+\n",
      "|         7 | 6,491,926  | 6,514,679.0      |           0.997 |\n",
      "+-----------+------------+------------------+-----------------+\n",
      "|         8 | 6,471,956  | 6,491,926.0      |           0.997 |\n",
      "+-----------+------------+------------------+-----------------+\n",
      "|         9 | 6,453,601  | 6,471,956.0      |           0.997 |\n",
      "+-----------+------------+------------------+-----------------+\n",
      "|        10 | 6,436,904  | 6,453,601.0      |           0.997 |\n",
      "+-----------+------------+------------------+-----------------+\n",
      "|        11 | 6,401,791  | 6,436,904.0      |           0.995 |\n",
      "+-----------+------------+------------------+-----------------+\n",
      "|        12 | 6,354,835  | 6,401,791.0      |           0.993 |\n",
      "+-----------+------------+------------------+-----------------+\n",
      "|        13 | 6,307,318  | 6,354,835.0      |           0.993 |\n",
      "+-----------+------------+------------------+-----------------+\n",
      "|        14 | 6,262,286  | 6,307,318.0      |           0.993 |\n",
      "+-----------+------------+------------------+-----------------+\n",
      "|        15 | 6,219,939  | 6,262,286.0      |           0.993 |\n",
      "+-----------+------------+------------------+-----------------+\n",
      "|        16 | 6,179,792  | 6,219,939.0      |           0.994 |\n",
      "+-----------+------------+------------------+-----------------+\n",
      "|        17 | 6,140,894  | 6,179,792.0      |           0.994 |\n",
      "+-----------+------------+------------------+-----------------+\n",
      "|        18 | 6,103,962  | 6,140,894.0      |           0.994 |\n",
      "+-----------+------------+------------------+-----------------+\n",
      "|        19 | 6,069,017  | 6,103,962.0      |           0.994 |\n",
      "+-----------+------------+------------------+-----------------+\n",
      "|        20 | 6,035,489  | 6,069,017.0      |           0.994 |\n",
      "+-----------+------------+------------------+-----------------+\n",
      "... (10 more rows)\n",
      "\n",
      "ðŸ” Discontinuity Detection:\n",
      "No significant discontinuities detected (all ratios within 0.8-1.2 range)\n"
     ]
    }
   ],
   "source": [
    "# Running variable (RANKING) distribution\n",
    "query_ranking_dist = f\"\"\"\n",
    "SELECT \n",
    "    RANKING,\n",
    "    COUNT(*) AS num_bids,\n",
    "    -- Calculate ratio to previous rank for discontinuity detection\n",
    "    LAG(COUNT(*)) OVER (ORDER BY RANKING) as prev_rank_bids,\n",
    "    ROUND(COUNT(*) * 1.0 / LAG(COUNT(*)) OVER (ORDER BY RANKING), 3) as ratio_to_prev\n",
    "FROM AUCTIONS_RESULTS\n",
    "WHERE CREATED_AT >= '{ANALYSIS_START}'\n",
    "  AND CREATED_AT < '{ANALYSIS_END}'\n",
    "  AND RANKING <= 30\n",
    "GROUP BY RANKING\n",
    "ORDER BY RANKING\n",
    "\"\"\"\n",
    "\n",
    "df_ranking = run_query(query_ranking_dist)\n",
    "\n",
    "# Format numbers for display\n",
    "df_ranking['num_bids'] = df_ranking['NUM_BIDS'].apply(lambda x: f\"{x:,}\")\n",
    "df_ranking['prev_rank_bids'] = df_ranking['PREV_RANK_BIDS'].apply(lambda x: f\"{x:,}\" if pd.notna(x) else \"N/A\")\n",
    "\n",
    "show_table(df_ranking[['RANKING', 'num_bids', 'prev_rank_bids', 'RATIO_TO_PREV']], \n",
    "           \"Running Variable Distribution (RANKING)\", max_rows=20)\n",
    "\n",
    "# Detect potential manipulation\n",
    "print(\"\\nðŸ” Discontinuity Detection:\")\n",
    "suspicious = df_ranking[(df_ranking['RATIO_TO_PREV'] < 0.8) | (df_ranking['RATIO_TO_PREV'] > 1.2)]\n",
    "if len(suspicious) > 0:\n",
    "    print(\"Potential discontinuities detected at ranks:\")\n",
    "    for _, row in suspicious.iterrows():\n",
    "        print(f\"  Rank {row['RANKING']}: ratio = {row['RATIO_TO_PREV']:.3f}\")\n",
    "else:\n",
    "    print(\"No significant discontinuities detected (all ratios within 0.8-1.2 range)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Treatment Assignment Discontinuity\n",
    "\n",
    "### 2a. IS_WINNER Probability by Rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "IS_WINNER Probability by Rank\n",
      "=============================\n",
      "+-----------+--------------+-----------+----------------+-------------------+\n",
      "|   RANKING | total_bids   | winners   |   WIN_RATE_PCT |   WIN_RATE_CHANGE |\n",
      "+===========+==============+===========+================+===================+\n",
      "|         1 | 6,724,843    | 6,683,772 |          99.39 |                   |\n",
      "+-----------+--------------+-----------+----------------+-------------------+\n",
      "|         2 | 6,653,098    | 6,565,781 |          98.69 |             -0.7  |\n",
      "+-----------+--------------+-----------+----------------+-------------------+\n",
      "|         3 | 6,605,526    | 6,477,264 |          98.06 |             -0.63 |\n",
      "+-----------+--------------+-----------+----------------+-------------------+\n",
      "|         4 | 6,569,492    | 6,404,639 |          97.49 |             -0.57 |\n",
      "+-----------+--------------+-----------+----------------+-------------------+\n",
      "|         5 | 6,540,178    | 6,341,923 |          96.97 |             -0.52 |\n",
      "+-----------+--------------+-----------+----------------+-------------------+\n",
      "|         6 | 6,514,679    | 6,286,244 |          96.49 |             -0.48 |\n",
      "+-----------+--------------+-----------+----------------+-------------------+\n",
      "|         7 | 6,491,926    | 6,235,513 |          96.05 |             -0.44 |\n",
      "+-----------+--------------+-----------+----------------+-------------------+\n",
      "|         8 | 6,471,956    | 6,188,819 |          95.63 |             -0.42 |\n",
      "+-----------+--------------+-----------+----------------+-------------------+\n",
      "|         9 | 6,453,601    | 6,146,202 |          95.24 |             -0.39 |\n",
      "+-----------+--------------+-----------+----------------+-------------------+\n",
      "|        10 | 6,436,904    | 6,106,171 |          94.86 |             -0.38 |\n",
      "+-----------+--------------+-----------+----------------+-------------------+\n",
      "|        11 | 6,401,791    | 6,068,876 |          94.8  |             -0.06 |\n",
      "+-----------+--------------+-----------+----------------+-------------------+\n",
      "|        12 | 6,354,835    | 6,033,960 |          94.95 |              0.15 |\n",
      "+-----------+--------------+-----------+----------------+-------------------+\n",
      "|        13 | 6,307,318    | 6,000,765 |          95.14 |              0.19 |\n",
      "+-----------+--------------+-----------+----------------+-------------------+\n",
      "|        14 | 6,262,286    | 5,969,390 |          95.32 |              0.18 |\n",
      "+-----------+--------------+-----------+----------------+-------------------+\n",
      "|        15 | 6,219,939    | 5,939,162 |          95.49 |              0.17 |\n",
      "+-----------+--------------+-----------+----------------+-------------------+\n",
      "|        16 | 6,179,792    | 5,909,965 |          95.63 |              0.14 |\n",
      "+-----------+--------------+-----------+----------------+-------------------+\n",
      "|        17 | 6,140,894    | 5,882,630 |          95.79 |              0.16 |\n",
      "+-----------+--------------+-----------+----------------+-------------------+\n",
      "|        18 | 6,103,962    | 5,856,490 |          95.95 |              0.16 |\n",
      "+-----------+--------------+-----------+----------------+-------------------+\n",
      "|        19 | 6,069,017    | 5,831,579 |          96.09 |              0.14 |\n",
      "+-----------+--------------+-----------+----------------+-------------------+\n",
      "|        20 | 6,035,489    | 5,807,104 |          96.22 |              0.13 |\n",
      "+-----------+--------------+-----------+----------------+-------------------+\n",
      "... (10 more rows)\n",
      "\n",
      "ðŸŽ¯ Sharp Discontinuities in Win Rate:\n",
      "  Rank 21: 96.2% â†’ 52.4% (drop: 43.8%)\n"
     ]
    }
   ],
   "source": [
    "# IS_WINNER discontinuity analysis\n",
    "query_winner = f\"\"\"\n",
    "SELECT \n",
    "    RANKING,\n",
    "    COUNT(*) AS total_bids,\n",
    "    SUM(CASE WHEN IS_WINNER THEN 1 ELSE 0 END) AS winners,\n",
    "    ROUND(100.0 * SUM(CASE WHEN IS_WINNER THEN 1 ELSE 0 END) / COUNT(*), 2) AS win_rate_pct,\n",
    "    -- Calculate drop from previous rank\n",
    "    LAG(ROUND(100.0 * SUM(CASE WHEN IS_WINNER THEN 1 ELSE 0 END) / COUNT(*), 2)) \n",
    "        OVER (ORDER BY RANKING) as prev_win_rate,\n",
    "    ROUND(100.0 * SUM(CASE WHEN IS_WINNER THEN 1 ELSE 0 END) / COUNT(*), 2) - \n",
    "        LAG(ROUND(100.0 * SUM(CASE WHEN IS_WINNER THEN 1 ELSE 0 END) / COUNT(*), 2)) \n",
    "        OVER (ORDER BY RANKING) as win_rate_change\n",
    "FROM AUCTIONS_RESULTS\n",
    "WHERE CREATED_AT >= '{ANALYSIS_START}'\n",
    "  AND CREATED_AT < '{ANALYSIS_END}'\n",
    "  AND RANKING <= 30\n",
    "GROUP BY RANKING\n",
    "ORDER BY RANKING\n",
    "\"\"\"\n",
    "\n",
    "df_winner = run_query(query_winner)\n",
    "\n",
    "# Format for display\n",
    "df_display = df_winner.copy()\n",
    "df_display['total_bids'] = df_display['TOTAL_BIDS'].apply(lambda x: f\"{x:,}\")\n",
    "df_display['winners'] = df_display['WINNERS'].apply(lambda x: f\"{x:,}\")\n",
    "\n",
    "show_table(df_display[['RANKING', 'total_bids', 'winners', 'WIN_RATE_PCT', 'WIN_RATE_CHANGE']], \n",
    "           \"IS_WINNER Probability by Rank\", max_rows=20)\n",
    "\n",
    "# Identify sharp drops\n",
    "print(\"\\nðŸŽ¯ Sharp Discontinuities in Win Rate:\")\n",
    "sharp_drops = df_winner[df_winner['WIN_RATE_CHANGE'] < -10]\n",
    "if len(sharp_drops) > 0:\n",
    "    for _, row in sharp_drops.iterrows():\n",
    "        print(f\"  Rank {row['RANKING']}: {row['PREV_WIN_RATE']:.1f}% â†’ {row['WIN_RATE_PCT']:.1f}% (drop: {abs(row['WIN_RATE_CHANGE']):.1f}%)\")\n",
    "else:\n",
    "    print(\"No sharp drops (>10%) detected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2b. Impression Rate by Rank (Fold Detection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impression rate analysis with proper type conversion\n",
    "query_impression = f\"\"\"\n",
    "WITH winner_impressions AS (\n",
    "    SELECT\n",
    "        ar.RANKING,\n",
    "        COUNT(*) as total_winners,\n",
    "        COUNT(i.INTERACTION_ID) as got_impression\n",
    "    FROM AUCTIONS_RESULTS ar\n",
    "    LEFT JOIN IMPRESSIONS i \n",
    "        ON TO_VARCHAR(ar.AUCTION_ID) = i.AUCTION_ID\n",
    "        AND ar.PRODUCT_ID = i.PRODUCT_ID\n",
    "    WHERE ar.CREATED_AT >= '{ANALYSIS_START}'\n",
    "      AND ar.CREATED_AT < '{ANALYSIS_END}'\n",
    "      AND ar.IS_WINNER = TRUE\n",
    "      AND ar.RANKING <= 30\n",
    "    GROUP BY ar.RANKING\n",
    ")\n",
    "SELECT \n",
    "    RANKING,\n",
    "    total_winners,\n",
    "    got_impression,\n",
    "    ROUND(100.0 * got_impression / total_winners, 2) as impression_rate_pct,\n",
    "    -- Mark if below fold threshold\n",
    "    CASE WHEN ROUND(100.0 * got_impression / total_winners, 2) < 50 \n",
    "         THEN 'BELOW_FOLD' \n",
    "         ELSE 'ABOVE_FOLD' END as fold_position\n",
    "FROM winner_impressions\n",
    "ORDER BY RANKING\n",
    "\"\"\"\n",
    "\n",
    "df_impression = run_query(query_impression)\n",
    "\n",
    "# Format for display\n",
    "df_imp_display = df_impression.copy()\n",
    "df_imp_display['total_winners'] = df_imp_display['TOTAL_WINNERS'].apply(lambda x: f\"{x:,}\")\n",
    "df_imp_display['got_impression'] = df_imp_display['GOT_IMPRESSION'].apply(lambda x: f\"{x:,}\")\n",
    "\n",
    "show_table(df_imp_display[['RANKING', 'total_winners', 'got_impression', 'IMPRESSION_RATE_PCT', 'FOLD_POSITION']], \n",
    "           \"Impression Rate by Rank - Fold Detection\", max_rows=20)\n",
    "\n",
    "# Identify the fold\n",
    "fold_rank = None\n",
    "for idx, row in df_impression.iterrows():\n",
    "    if row['IMPRESSION_RATE_PCT'] < 50:\n",
    "        fold_rank = row['RANKING']\n",
    "        break\n",
    "\n",
    "print(\"\\nðŸ“ FOLD ANALYSIS:\")\n",
    "if fold_rank:\n",
    "    above_fold = df_impression[df_impression['RANKING'] < fold_rank]\n",
    "    below_fold = df_impression[df_impression['RANKING'] >= fold_rank]\n",
    "    \n",
    "    print(f\"Fold detected at rank {fold_rank}\")\n",
    "    print(f\"\\nAbove fold (rank < {fold_rank}):\")\n",
    "    print(f\"  Average impression rate: {above_fold['IMPRESSION_RATE_PCT'].mean():.1f}%\")\n",
    "    print(f\"  Min impression rate: {above_fold['IMPRESSION_RATE_PCT'].min():.1f}%\")\n",
    "    print(f\"  Max impression rate: {above_fold['IMPRESSION_RATE_PCT'].max():.1f}%\")\n",
    "    \n",
    "    print(f\"\\nBelow fold (rank â‰¥ {fold_rank}):\")\n",
    "    print(f\"  Average impression rate: {below_fold['IMPRESSION_RATE_PCT'].mean():.1f}%\")\n",
    "    print(f\"  Min impression rate: {below_fold['IMPRESSION_RATE_PCT'].min():.1f}%\")\n",
    "    print(f\"  Max impression rate: {below_fold['IMPRESSION_RATE_PCT'].max():.1f}%\")\n",
    "    \n",
    "    print(f\"\\nDiscontinuity size: {above_fold['IMPRESSION_RATE_PCT'].mean() - below_fold['IMPRESSION_RATE_PCT'].mean():.1f} percentage points\")\n",
    "else:\n",
    "    print(\"No fold detected (all ranks have >50% impression rate)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Covariate Balance Tests\n",
    "\n",
    "Check if pre-treatment characteristics vary smoothly around the cutoff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using APPROX_COUNT_DISTINCT for efficiency with heavy data\n",
    "query_covariates = f\"\"\"\n",
    "SELECT \n",
    "    RANKING,\n",
    "    COUNT(*) AS total_bids,\n",
    "    APPROX_COUNT_DISTINCT(PRODUCT_ID) AS unique_products,\n",
    "    APPROX_COUNT_DISTINCT(VENDOR_ID) AS unique_vendors,\n",
    "    APPROX_COUNT_DISTINCT(CAMPAIGN_ID) AS unique_campaigns,\n",
    "    -- Normalized metrics\n",
    "    ROUND(APPROX_COUNT_DISTINCT(PRODUCT_ID) * 1.0 / COUNT(*), 4) AS products_per_bid,\n",
    "    ROUND(APPROX_COUNT_DISTINCT(VENDOR_ID) * 1.0 / COUNT(*), 4) AS vendors_per_bid,\n",
    "    ROUND(APPROX_COUNT_DISTINCT(CAMPAIGN_ID) * 1.0 / COUNT(*), 4) AS campaigns_per_bid\n",
    "FROM AUCTIONS_RESULTS\n",
    "WHERE CREATED_AT >= '{ANALYSIS_START}'\n",
    "  AND CREATED_AT < '{ANALYSIS_END}'\n",
    "  AND RANKING <= 20\n",
    "GROUP BY RANKING\n",
    "ORDER BY RANKING\n",
    "\"\"\"\n",
    "\n",
    "df_covariates = run_query(query_covariates)\n",
    "\n",
    "# Format for display\n",
    "df_cov_display = df_covariates.copy()\n",
    "df_cov_display['total_bids'] = df_cov_display['TOTAL_BIDS'].apply(lambda x: f\"{x:,}\")\n",
    "df_cov_display['unique_products'] = df_cov_display['UNIQUE_PRODUCTS'].apply(lambda x: f\"{x:,}\")\n",
    "df_cov_display['unique_vendors'] = df_cov_display['UNIQUE_VENDORS'].apply(lambda x: f\"{x:,}\")\n",
    "\n",
    "show_table(df_cov_display[['RANKING', 'total_bids', 'unique_products', 'unique_vendors', \n",
    "                           'PRODUCTS_PER_BID', 'VENDORS_PER_BID']], \n",
    "           \"Covariate Balance by Rank\", max_rows=20)\n",
    "\n",
    "# Test for smoothness\n",
    "print(\"\\nðŸ”¬ Covariate Balance Tests:\")\n",
    "for col in ['PRODUCTS_PER_BID', 'VENDORS_PER_BID', 'CAMPAIGNS_PER_BID']:\n",
    "    values = df_covariates[col].values[:10]  # Focus on top 10 ranks\n",
    "    mean_val = np.mean(values)\n",
    "    std_val = np.std(values)\n",
    "    cv = std_val / mean_val if mean_val > 0 else 0  # Coefficient of variation\n",
    "    print(f\"\\n{col}:\")\n",
    "    print(f\"  Mean: {mean_val:.4f}\")\n",
    "    print(f\"  Std Dev: {std_val:.4f}\")\n",
    "    print(f\"  CV: {cv:.3f} (lower is smoother)\")\n",
    "    \n",
    "    # Check for jumps\n",
    "    if fold_rank and fold_rank <= 10:\n",
    "        before_fold = np.mean(values[:fold_rank-1])\n",
    "        after_fold = np.mean(values[fold_rank-1:])\n",
    "        jump = abs(after_fold - before_fold) / before_fold * 100\n",
    "        print(f\"  Jump at fold: {jump:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Outcome Analysis Around Cutoff\n",
    "\n",
    "Examine purchase rates and revenue around the rank discontinuity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complex join for outcome analysis with attribution window\n",
    "# Using TABLESAMPLE for efficiency with heavy data\n",
    "query_outcomes = f\"\"\"\n",
    "WITH auction_outcomes AS (\n",
    "    SELECT\n",
    "        ar.RANKING,\n",
    "        ar.IS_WINNER,\n",
    "        CASE WHEN i.INTERACTION_ID IS NOT NULL THEN 1 ELSE 0 END as got_impression,\n",
    "        CASE WHEN c.INTERACTION_ID IS NOT NULL THEN 1 ELSE 0 END as got_click,\n",
    "        CASE WHEN p.PURCHASE_ID IS NOT NULL THEN 1 ELSE 0 END as got_purchase,\n",
    "        COALESCE(p.QUANTITY * p.UNIT_PRICE, 0) as revenue\n",
    "    FROM AUCTIONS_RESULTS ar TABLESAMPLE BERNOULLI ({SAMPLE_RATE})\n",
    "    JOIN AUCTIONS_USERS au \n",
    "        ON ar.AUCTION_ID = au.AUCTION_ID\n",
    "    LEFT JOIN IMPRESSIONS i \n",
    "        ON TO_VARCHAR(ar.AUCTION_ID) = i.AUCTION_ID\n",
    "        AND ar.PRODUCT_ID = i.PRODUCT_ID\n",
    "        AND i.USER_ID = au.OPAQUE_USER_ID\n",
    "    LEFT JOIN CLICKS c \n",
    "        ON i.INTERACTION_ID = c.INTERACTION_ID\n",
    "    LEFT JOIN PURCHASES p\n",
    "        ON c.USER_ID = p.USER_ID\n",
    "        AND c.PRODUCT_ID = p.PRODUCT_ID\n",
    "        AND p.PURCHASED_AT > c.OCCURRED_AT\n",
    "        AND DATEDIFF('minute', c.OCCURRED_AT, p.PURCHASED_AT) <= {ATTRIBUTION_WINDOW_MINUTES}\n",
    "    WHERE ar.CREATED_AT >= '{ANALYSIS_START}'\n",
    "      AND ar.CREATED_AT < '{ANALYSIS_END}'\n",
    "      AND ar.RANKING <= 20\n",
    ")\n",
    "SELECT\n",
    "    RANKING,\n",
    "    COUNT(*) as sample_size,\n",
    "    ROUND(AVG(got_impression) * 100, 2) as impression_rate,\n",
    "    ROUND(AVG(got_click) * 100, 2) as click_rate,\n",
    "    ROUND(AVG(got_purchase) * 100, 2) as purchase_rate,\n",
    "    ROUND(AVG(revenue), 2) as avg_revenue,\n",
    "    ROUND(SUM(revenue), 2) as total_revenue,\n",
    "    -- CTR and CVR calculations\n",
    "    CASE WHEN SUM(got_impression) > 0 \n",
    "         THEN ROUND(100.0 * SUM(got_click) / SUM(got_impression), 2) \n",
    "         ELSE 0 END as ctr,\n",
    "    CASE WHEN SUM(got_click) > 0 \n",
    "         THEN ROUND(100.0 * SUM(got_purchase) / SUM(got_click), 2) \n",
    "         ELSE 0 END as cvr\n",
    "FROM auction_outcomes\n",
    "GROUP BY RANKING\n",
    "ORDER BY RANKING\n",
    "\"\"\"\n",
    "\n",
    "df_outcomes = run_query(query_outcomes)\n",
    "\n",
    "# Format for display\n",
    "df_out_display = df_outcomes.copy()\n",
    "df_out_display['sample_size'] = df_out_display['SAMPLE_SIZE'].apply(lambda x: f\"{x:,}\")\n",
    "df_out_display['avg_revenue'] = df_out_display['AVG_REVENUE'].apply(lambda x: f\"${x:,.2f}\")\n",
    "df_out_display['total_revenue'] = df_out_display['TOTAL_REVENUE'].apply(lambda x: f\"${x:,.2f}\")\n",
    "\n",
    "show_table(df_out_display[['RANKING', 'sample_size', 'IMPRESSION_RATE', 'CLICK_RATE', \n",
    "                           'PURCHASE_RATE', 'CTR', 'CVR', 'avg_revenue']], \n",
    "           f\"Outcome Analysis by Rank (Sample: {SAMPLE_RATE*100}%)\", max_rows=20)\n",
    "\n",
    "# Summary statistics around fold\n",
    "print(f\"\\nðŸ“Š Outcome Summary Around Fold:\")\n",
    "if fold_rank and fold_rank <= 20:\n",
    "    above = df_outcomes[df_outcomes['RANKING'] < fold_rank]\n",
    "    below = df_outcomes[df_outcomes['RANKING'] >= fold_rank]\n",
    "    \n",
    "    metrics = ['IMPRESSION_RATE', 'CLICK_RATE', 'PURCHASE_RATE', 'CTR', 'CVR']\n",
    "    \n",
    "    comparison_data = []\n",
    "    for metric in metrics:\n",
    "        above_mean = above[metric].mean()\n",
    "        below_mean = below[metric].mean()\n",
    "        diff = above_mean - below_mean\n",
    "        comparison_data.append([metric, f\"{above_mean:.2f}%\", f\"{below_mean:.2f}%\", f\"{diff:+.2f}pp\"])\n",
    "    \n",
    "    comparison_df = pd.DataFrame(comparison_data, \n",
    "                                 columns=['Metric', f'Above Fold (<{fold_rank})', \n",
    "                                         f'Below Fold (â‰¥{fold_rank})', 'Difference'])\n",
    "    show_table(comparison_df, \"Metrics Comparison Around Fold\")\n",
    "    \n",
    "    print(f\"\\nðŸ’° Estimated Treatment Effect:\")\n",
    "    print(f\"  Purchase rate difference: {above['PURCHASE_RATE'].mean() - below['PURCHASE_RATE'].mean():.2f} percentage points\")\n",
    "    print(f\"  Average revenue difference: ${above['AVG_REVENUE'].mean() - below['AVG_REVENUE'].mean():.2f}\")\n",
    "else:\n",
    "    print(\"No fold detected for comparison\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. RDD-Specific Analysis: Local Linear Regression\n",
    "\n",
    "Estimate treatment effects using local linear regression around the cutoff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Focus on narrow bandwidth around fold for RDD\n",
    "if fold_rank:\n",
    "    bandwidth = 3  # Ranks within Â±3 of fold\n",
    "    \n",
    "    query_rdd = f\"\"\"\n",
    "    WITH rdd_sample AS (\n",
    "        SELECT\n",
    "            ar.RANKING,\n",
    "            CASE WHEN ar.RANKING < {fold_rank} THEN 1 ELSE 0 END as above_fold,\n",
    "            ar.RANKING - {fold_rank} as centered_rank,\n",
    "            CASE WHEN p.PURCHASE_ID IS NOT NULL THEN 1 ELSE 0 END as purchased\n",
    "        FROM AUCTIONS_RESULTS ar\n",
    "        JOIN AUCTIONS_USERS au ON ar.AUCTION_ID = au.AUCTION_ID\n",
    "        LEFT JOIN IMPRESSIONS i \n",
    "            ON TO_VARCHAR(ar.AUCTION_ID) = i.AUCTION_ID\n",
    "            AND ar.PRODUCT_ID = i.PRODUCT_ID\n",
    "        LEFT JOIN CLICKS c ON i.INTERACTION_ID = c.INTERACTION_ID\n",
    "        LEFT JOIN PURCHASES p\n",
    "            ON c.USER_ID = p.USER_ID\n",
    "            AND c.PRODUCT_ID = p.PRODUCT_ID\n",
    "            AND p.PURCHASED_AT > c.OCCURRED_AT\n",
    "            AND DATEDIFF('minute', c.OCCURRED_AT, p.PURCHASED_AT) <= {ATTRIBUTION_WINDOW_MINUTES}\n",
    "        WHERE ar.CREATED_AT >= '{ANALYSIS_START}'\n",
    "          AND ar.CREATED_AT < '{ANALYSIS_END}'\n",
    "          AND ar.RANKING BETWEEN {fold_rank - bandwidth} AND {fold_rank + bandwidth}\n",
    "          AND ar.IS_WINNER = TRUE\n",
    "    )\n",
    "    SELECT\n",
    "        RANKING,\n",
    "        above_fold,\n",
    "        centered_rank,\n",
    "        COUNT(*) as n,\n",
    "        AVG(purchased) * 100 as purchase_rate,\n",
    "        STDDEV(purchased) * 100 as purchase_std\n",
    "    FROM rdd_sample\n",
    "    GROUP BY RANKING, above_fold, centered_rank\n",
    "    ORDER BY RANKING\n",
    "    \"\"\"\n",
    "    \n",
    "    df_rdd = run_query(query_rdd)\n",
    "    \n",
    "    # Format for display\n",
    "    df_rdd_display = df_rdd.copy()\n",
    "    df_rdd_display['n'] = df_rdd_display['N'].apply(lambda x: f\"{x:,}\")\n",
    "    df_rdd_display['purchase_rate'] = df_rdd_display['PURCHASE_RATE'].apply(lambda x: f\"{x:.2f}%\")\n",
    "    df_rdd_display['purchase_std'] = df_rdd_display['PURCHASE_STD'].apply(lambda x: f\"{x:.2f}%\" if pd.notna(x) else \"N/A\")\n",
    "    df_rdd_display['treatment'] = df_rdd_display['ABOVE_FOLD'].apply(lambda x: 'Above Fold' if x == 1 else 'Below Fold')\n",
    "    \n",
    "    show_table(df_rdd_display[['RANKING', 'treatment', 'CENTERED_RANK', 'n', 'purchase_rate', 'purchase_std']], \n",
    "               f\"RDD Analysis: Narrow Bandwidth (Â±{bandwidth} ranks around fold)\")\n",
    "    \n",
    "    # Calculate RDD estimate using simple difference in means\n",
    "    above = df_rdd[df_rdd['ABOVE_FOLD'] == 1]\n",
    "    below = df_rdd[df_rdd['ABOVE_FOLD'] == 0]\n",
    "    \n",
    "    if len(above) > 0 and len(below) > 0:\n",
    "        # Weight by sample size\n",
    "        above_rate = np.average(above['PURCHASE_RATE'], weights=above['N'])\n",
    "        below_rate = np.average(below['PURCHASE_RATE'], weights=below['N'])\n",
    "        treatment_effect = above_rate - below_rate\n",
    "        \n",
    "        print(f\"\\nðŸŽ¯ RDD Treatment Effect Estimate:\")\n",
    "        print(f\"  Above fold purchase rate: {above_rate:.2f}%\")\n",
    "        print(f\"  Below fold purchase rate: {below_rate:.2f}%\")\n",
    "        print(f\"  Discontinuity at fold: {treatment_effect:.2f} percentage points\")\n",
    "        print(f\"\\n  Interpretation: Being above the fold (rank < {fold_rank}) increases\")\n",
    "        print(f\"  purchase probability by {abs(treatment_effect):.2f} percentage points\")\n",
    "        \n",
    "        # Calculate standard error (simplified)\n",
    "        n_above = above['N'].sum()\n",
    "        n_below = below['N'].sum()\n",
    "        se_above = np.sqrt(above_rate * (100 - above_rate) / n_above)\n",
    "        se_below = np.sqrt(below_rate * (100 - below_rate) / n_below)\n",
    "        se_total = np.sqrt(se_above**2 + se_below**2)\n",
    "        \n",
    "        print(f\"\\n  Standard error: {se_total:.3f}\")\n",
    "        print(f\"  95% CI: [{treatment_effect - 1.96*se_total:.2f}, {treatment_effect + 1.96*se_total:.2f}]\")\n",
    "else:\n",
    "    print(\"âš ï¸ No fold detected for RDD analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Bandwidth Sensitivity Analysis\n",
    "\n",
    "Test robustness of results to different bandwidth choices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test multiple bandwidths\n",
    "bandwidths = [2, 3, 4, 5, 6]\n",
    "sensitivity_results = []\n",
    "\n",
    "if fold_rank:\n",
    "    for bw in bandwidths:\n",
    "        query_bw = f\"\"\"\n",
    "        WITH bw_analysis AS (\n",
    "            SELECT\n",
    "                CASE WHEN ar.RANKING < {fold_rank} THEN 1 ELSE 0 END as treatment,\n",
    "                COUNT(*) as n,\n",
    "                AVG(CASE WHEN p.PURCHASE_ID IS NOT NULL THEN 1 ELSE 0 END) as purchase_rate\n",
    "            FROM AUCTIONS_RESULTS ar\n",
    "            JOIN AUCTIONS_USERS au ON ar.AUCTION_ID = au.AUCTION_ID\n",
    "            LEFT JOIN IMPRESSIONS i \n",
    "                ON TO_VARCHAR(ar.AUCTION_ID) = i.AUCTION_ID\n",
    "                AND ar.PRODUCT_ID = i.PRODUCT_ID\n",
    "            LEFT JOIN CLICKS c ON i.INTERACTION_ID = c.INTERACTION_ID\n",
    "            LEFT JOIN PURCHASES p\n",
    "                ON c.USER_ID = p.USER_ID\n",
    "                AND c.PRODUCT_ID = p.PRODUCT_ID\n",
    "                AND p.PURCHASED_AT > c.OCCURRED_AT\n",
    "                AND DATEDIFF('minute', c.OCCURRED_AT, p.PURCHASED_AT) <= {ATTRIBUTION_WINDOW_MINUTES}\n",
    "            WHERE ar.CREATED_AT >= '{ANALYSIS_START}'\n",
    "              AND ar.CREATED_AT < '{ANALYSIS_END}'\n",
    "              AND ar.RANKING BETWEEN {fold_rank - bw} AND {fold_rank + bw}\n",
    "              AND ar.IS_WINNER = TRUE\n",
    "            GROUP BY treatment\n",
    "        )\n",
    "        SELECT\n",
    "            MAX(CASE WHEN treatment = 1 THEN purchase_rate END) * 100 as above_rate,\n",
    "            MAX(CASE WHEN treatment = 0 THEN purchase_rate END) * 100 as below_rate,\n",
    "            (MAX(CASE WHEN treatment = 1 THEN purchase_rate END) - \n",
    "             MAX(CASE WHEN treatment = 0 THEN purchase_rate END)) * 100 as effect,\n",
    "            SUM(n) as total_n,\n",
    "            MAX(CASE WHEN treatment = 1 THEN n END) as n_above,\n",
    "            MAX(CASE WHEN treatment = 0 THEN n END) as n_below\n",
    "        FROM bw_analysis\n",
    "        \"\"\"\n",
    "        \n",
    "        df_bw = run_query(query_bw)\n",
    "        if not df_bw.empty and 'EFFECT' in df_bw.columns:\n",
    "            sensitivity_results.append({\n",
    "                'Bandwidth': f\"Â±{bw}\",\n",
    "                'Rank_Range': f\"{fold_rank - bw} to {fold_rank + bw}\",\n",
    "                'N_Above': df_bw['N_ABOVE'].iloc[0],\n",
    "                'N_Below': df_bw['N_BELOW'].iloc[0],\n",
    "                'Above_Rate': f\"{df_bw['ABOVE_RATE'].iloc[0]:.2f}%\" if pd.notna(df_bw['ABOVE_RATE'].iloc[0]) else \"N/A\",\n",
    "                'Below_Rate': f\"{df_bw['BELOW_RATE'].iloc[0]:.2f}%\" if pd.notna(df_bw['BELOW_RATE'].iloc[0]) else \"N/A\",\n",
    "                'Treatment_Effect': df_bw['EFFECT'].iloc[0] if pd.notna(df_bw['EFFECT'].iloc[0]) else None\n",
    "            })\n",
    "    \n",
    "    # Display results\n",
    "    if sensitivity_results:\n",
    "        sens_df = pd.DataFrame(sensitivity_results)\n",
    "        \n",
    "        # Format for display\n",
    "        sens_df['N_Above'] = sens_df['N_Above'].apply(lambda x: f\"{x:,}\" if pd.notna(x) else \"N/A\")\n",
    "        sens_df['N_Below'] = sens_df['N_Below'].apply(lambda x: f\"{x:,}\" if pd.notna(x) else \"N/A\")\n",
    "        sens_df['Treatment_Effect_Display'] = sens_df['Treatment_Effect'].apply(\n",
    "            lambda x: f\"{x:.2f}pp\" if pd.notna(x) else \"N/A\"\n",
    "        )\n",
    "        \n",
    "        show_table(sens_df[['Bandwidth', 'Rank_Range', 'N_Above', 'N_Below', \n",
    "                           'Above_Rate', 'Below_Rate', 'Treatment_Effect_Display']], \n",
    "                  \"Bandwidth Sensitivity Analysis\")\n",
    "        \n",
    "        # Calculate summary statistics\n",
    "        valid_effects = [x['Treatment_Effect'] for x in sensitivity_results if x['Treatment_Effect'] is not None]\n",
    "        if valid_effects:\n",
    "            mean_effect = np.mean(valid_effects)\n",
    "            std_effect = np.std(valid_effects)\n",
    "            min_effect = np.min(valid_effects)\n",
    "            max_effect = np.max(valid_effects)\n",
    "            \n",
    "            print(\"\\nðŸ“ˆ Bandwidth Sensitivity Summary:\")\n",
    "            print(f\"  Mean effect across bandwidths: {mean_effect:.2f}pp\")\n",
    "            print(f\"  Standard deviation: {std_effect:.2f}pp\")\n",
    "            print(f\"  Range: {min_effect:.2f}pp to {max_effect:.2f}pp\")\n",
    "            print(f\"  Coefficient of variation: {std_effect/abs(mean_effect):.3f}\")\n",
    "else:\n",
    "    print(\"âš ï¸ No fold detected for bandwidth sensitivity analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Summary & Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"RDD ANALYSIS SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nðŸ“… Analysis Period: {ANALYSIS_START} to {ANALYSIS_END}\")\n",
    "print(f\"ðŸ” Sample Rate Used: {SAMPLE_RATE*100}% (for heavy queries)\")\n",
    "print(f\"â° Attribution Window: {ATTRIBUTION_WINDOW_MINUTES} minutes post-click\")\n",
    "\n",
    "if fold_rank:\n",
    "    print(f\"\\nðŸŽ¯ KEY FINDINGS:\")\n",
    "    print(f\"  1. Fold detected at rank {fold_rank}\")\n",
    "    print(f\"  2. Sharp discontinuity in impression probability at this threshold\")\n",
    "    \n",
    "    if 'mean_effect' in locals():\n",
    "        print(f\"  3. Estimated causal effect of being above fold: {mean_effect:.1f}pp increase in purchases\")\n",
    "        print(f\"  4. Effect is {'robust' if std_effect < abs(mean_effect) * 0.3 else 'sensitive'} across bandwidth specifications\")\n",
    "    \n",
    "    print(f\"\\nâœ… RDD VALIDITY CHECKS:\")\n",
    "    \n",
    "    # Check for manipulation\n",
    "    if 'suspicious' in locals() and len(suspicious) == 0:\n",
    "        print(f\"  âœ“ No evidence of manipulation around cutoff\")\n",
    "    else:\n",
    "        print(f\"  âš ï¸ Some discontinuities in running variable distribution\")\n",
    "    \n",
    "    # Check covariate balance\n",
    "    if 'df_covariates' in locals():\n",
    "        cv_values = [df_covariates['PRODUCTS_PER_BID'].std() / df_covariates['PRODUCTS_PER_BID'].mean(),\n",
    "                     df_covariates['VENDORS_PER_BID'].std() / df_covariates['VENDORS_PER_BID'].mean()]\n",
    "        if all(cv < 0.2 for cv in cv_values):\n",
    "            print(f\"  âœ“ Covariates vary smoothly across threshold\")\n",
    "        else:\n",
    "            print(f\"  âš ï¸ Some covariate imbalance detected\")\n",
    "    \n",
    "    print(f\"  âœ“ Clear discontinuity in treatment assignment\")\n",
    "    \n",
    "    print(f\"\\nðŸ“‹ FINAL ESTIMATES:\")\n",
    "    summary_data = [\n",
    "        ['Fold Rank', str(fold_rank)],\n",
    "        ['Impression Rate Jump', f\"{above_fold['IMPRESSION_RATE_PCT'].mean() - below_fold['IMPRESSION_RATE_PCT'].mean():.1f}pp\" if 'above_fold' in locals() else \"N/A\"],\n",
    "        ['Purchase Rate Effect', f\"{mean_effect:.2f}pp\" if 'mean_effect' in locals() else \"N/A\"],\n",
    "        ['Revenue Effect', f\"${above['AVG_REVENUE'].mean() - below['AVG_REVENUE'].mean():.2f}\" if 'above' in locals() and 'AVG_REVENUE' in above.columns else \"N/A\"]\n",
    "    ]\n",
    "    summary_df = pd.DataFrame(summary_data, columns=['Metric', 'Value'])\n",
    "    show_table(summary_df, \"\")\n",
    "    \n",
    "else:\n",
    "    print(\"\\nâš ï¸ No clear discontinuity detected for RDD analysis\")\n",
    "    print(\"Consider:\")\n",
    "    print(\"  - Expanding the analysis period\")\n",
    "    print(\"  - Examining different rank thresholds\")\n",
    "    print(\"  - Checking data quality and join conditions\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close connection\n",
    "cursor.close()\n",
    "conn.close()\n",
    "print(\"âœ… Connection closed\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
