{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ef0cc36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Connection to Snowflake successful!\n",
      "\n",
      "--- Loading master list from 'final_purchaser_universe.parquet' ---\n",
      "✅ Loaded 4,926,305 total unique purchasing users.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from tabulate import tabulate\n",
    "from dotenv import load_dotenv\n",
    "import snowflake.connector\n",
    "\n",
    "# --- Initialize Connection ---\n",
    "load_dotenv()\n",
    "conn = None\n",
    "try:\n",
    "    conn = snowflake.connector.connect(\n",
    "        user=os.getenv('SNOWFLAKE_USER'),\n",
    "        password=os.getenv('SNOWFLAKE_PASSWORD'),\n",
    "        account=os.getenv('SNOWFLAKE_ACCOUNT'),\n",
    "        warehouse=os.getenv('SNOWFLAKE_WAREHOUSE'),\n",
    "        database='INCREMENTALITY',\n",
    "        schema='INCREMENTALITY_RESEARCH'\n",
    "    )\n",
    "    print(\"✅ Connection to Snowflake successful!\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ ERROR: Could not connect to Snowflake. {e}\", file=sys.stderr)\n",
    "    exit()\n",
    "\n",
    "# --- Load the Master Purchaser Universe ---\n",
    "UNIVERSE_FILE = \"final_purchaser_universe.parquet\"\n",
    "print(f\"\\n--- Loading master list from '{UNIVERSE_FILE}' ---\")\n",
    "try:\n",
    "    df_universe = pd.read_parquet(UNIVERSE_FILE)\n",
    "    print(f\"✅ Loaded {len(df_universe):,} total unique purchasing users.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"❌ FATAL ERROR: The universe file '{UNIVERSE_FILE}' was not found.\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af54512a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Loading Datasets ---\n",
      "-> Loading main panel from '/Users/pranjal/Code/topsort-incrementality/panel/user_panel_full_history.parquet'...\n",
      "   ✅ Loaded panel with 32,060,768 rows.\n",
      "-> Loading final holdout user list from 'final_holdout_user_ids_final.parquet'...\n",
      "   ✅ Loaded 784,133 holdout user IDs.\n",
      "-> Identifying all users with at least one purchase from the panel...\n",
      "   ✅ Identified 4,926,674 unique purchasing users.\n"
     ]
    }
   ],
   "source": [
    "import polars as pl\n",
    "import os\n",
    "from tabulate import tabulate\n",
    "\n",
    "# --- Configuration ---\n",
    "# This is the main 32M row panel dataset\n",
    "PANEL_FILE = \"/Users/pranjal/Code/topsort-incrementality/panel/user_panel_full_history.parquet\"\n",
    "# This is the final output from your long-running script\n",
    "HOLDOUT_FILE = \"final_holdout_user_ids_final.parquet\"\n",
    "# This is the file we will create\n",
    "OUTPUT_FILE = \"user_panel_with_holdout_flag.parquet\"\n",
    "\n",
    "# --- Load the datasets using Polars ---\n",
    "print(\"--- Loading Datasets ---\")\n",
    "try:\n",
    "    print(f\"-> Loading main panel from '{PANEL_FILE}'...\")\n",
    "    df_panel = pl.read_parquet(PANEL_FILE)\n",
    "    print(f\"   ✅ Loaded panel with {df_panel.height:,} rows.\")\n",
    "\n",
    "    print(f\"-> Loading final holdout user list from '{HOLDOUT_FILE}'...\")\n",
    "    df_holdouts = pl.read_parquet(HOLDOUT_FILE)\n",
    "    print(f\"   ✅ Loaded {df_holdouts.height:,} holdout user IDs.\")\n",
    "\n",
    "    # --- NEW: Identify and load the \"purchaser\" list from the panel ---\n",
    "    print(f\"-> Identifying all users with at least one purchase from the panel...\")\n",
    "    df_purchasers = df_panel.filter(pl.col(\"purchases\") > 0).select(pl.col(\"user_id\").unique())\n",
    "    print(f\"   ✅ Identified {df_purchasers.height:,} unique purchasing users.\")\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"\\n❌ FATAL ERROR: A required file was not found. Please ensure the previous scripts have completed.\")\n",
    "    print(f\"   Details: {e}\")\n",
    "    df_panel = None "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2773b69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Enriching Panel with Holdout and Purchaser Flags ---\n",
      "-> Standardizing column names to lowercase...\n",
      "   ✅ Column names standardized.\n",
      "-> Creating 'is_holdout' and 'is_purchaser' columns...\n",
      "   ✅ Flag columns created successfully.\n",
      "\n",
      "--- Verifying the new flags ---\n",
      "Distribution of the 'is_holdout' flag:\n",
      "shape: (2, 2)\n",
      "┌────────────┬──────────┐\n",
      "│ is_holdout ┆ count    │\n",
      "│ ---        ┆ ---      │\n",
      "│ u8         ┆ u32      │\n",
      "╞════════════╪══════════╡\n",
      "│ 0          ┆ 31220194 │\n",
      "│ 1          ┆ 840574   │\n",
      "└────────────┴──────────┘\n",
      "\n",
      "Distribution of the 'is_purchaser' flag:\n",
      "shape: (2, 2)\n",
      "┌──────────────┬──────────┐\n",
      "│ is_purchaser ┆ count    │\n",
      "│ ---          ┆ ---      │\n",
      "│ u8           ┆ u32      │\n",
      "╞══════════════╪══════════╡\n",
      "│ 0            ┆ 7703346  │\n",
      "│ 1            ┆ 24357422 │\n",
      "└──────────────┴──────────┘\n"
     ]
    }
   ],
   "source": [
    "if df_panel is not None:\n",
    "    print(\"\\n--- Enriching Panel with Holdout and Purchaser Flags ---\")\n",
    "    \n",
    "    # --- Standardize column names to prevent errors ---\n",
    "    print(\"-> Standardizing column names to lowercase...\")\n",
    "    df_panel.columns = [col.lower() for col in df_panel.columns]\n",
    "    df_holdouts.columns = [col.lower() for col in df_holdouts.columns]\n",
    "    df_purchasers.columns = [col.lower() for col in df_purchasers.columns]\n",
    "    print(\"   ✅ Column names standardized.\")\n",
    "    \n",
    "    # Use sets for maximum performance\n",
    "    holdout_set = set(df_holdouts['user_id'])\n",
    "    purchaser_set = set(df_purchasers['user_id'])\n",
    "\n",
    "    # --- Create the flag columns using efficient expressions ---\n",
    "    print(\"-> Creating 'is_holdout' and 'is_purchaser' columns...\")\n",
    "    df_enriched = df_panel.with_columns(\n",
    "        is_holdout = pl.col('user_id').is_in(holdout_set).cast(pl.UInt8),\n",
    "        is_purchaser = pl.col('user_id').is_in(purchaser_set).cast(pl.UInt8)\n",
    "    )\n",
    "    print(\"   ✅ Flag columns created successfully.\")\n",
    "\n",
    "    # --- Verification Step ---\n",
    "    print(\"\\n--- Verifying the new flags ---\")\n",
    "    print(\"Distribution of the 'is_holdout' flag:\")\n",
    "    print(df_enriched['is_holdout'].value_counts())\n",
    "    print(\"\\nDistribution of the 'is_purchaser' flag:\")\n",
    "    print(df_enriched['is_purchaser'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df6a4faa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Performing Overall and Temporal EDA on the Enriched Panel ---\n",
      "-> Calculating overall summary stats...\n",
      "   ✅ Overall stats calculated.\n",
      "-> Calculating weekly evolution of metrics...\n",
      "   ✅ Weekly aggregates calculated.\n",
      "-> Calculating weekly fractions...\n",
      "   ✅ Weekly fractions calculated.\n",
      "\n",
      "✅ ANALYSIS COMPLETE. A detailed EDA report has been saved to 'panel_eda_summary_report.txt'\n",
      "\n",
      "--- Evolution of Key Weekly Metrics (First 10 Weeks) ---\n",
      "+---------------------+----------------+--------------------+-----------------+----------------+-------------------+-----------------+\n",
      "| week                |   active_users |   purchasing_users |   holdout_users |   total_clicks |   total_purchases |   total_revenue |\n",
      "+=====================+================+====================+=================+================+===================+=================+\n",
      "| 2025-03-10 00:00:00 |         571746 |             459848 |           14772 |        1537431 |            374695 |      17,165,085 |\n",
      "+---------------------+----------------+--------------------+-----------------+----------------+-------------------+-----------------+\n",
      "| 2025-03-17 00:00:00 |        1022367 |             797258 |           31425 |        3394233 |            819260 |      37,365,288 |\n",
      "+---------------------+----------------+--------------------+-----------------+----------------+-------------------+-----------------+\n",
      "| 2025-03-24 00:00:00 |        1061484 |             828965 |           31082 |        3693883 |            832513 |      38,063,925 |\n",
      "+---------------------+----------------+--------------------+-----------------+----------------+-------------------+-----------------+\n",
      "| 2025-03-31 00:00:00 |        1071531 |             838841 |           30510 |        3785604 |            838217 |      38,439,738 |\n",
      "+---------------------+----------------+--------------------+-----------------+----------------+-------------------+-----------------+\n",
      "| 2025-04-07 00:00:00 |        1114325 |             862786 |           28859 |        4249478 |            806173 |      36,877,335 |\n",
      "+---------------------+----------------+--------------------+-----------------+----------------+-------------------+-----------------+\n",
      "| 2025-04-14 00:00:00 |        1153113 |             885648 |           29205 |        4677307 |            783940 |      35,985,980 |\n",
      "+---------------------+----------------+--------------------+-----------------+----------------+-------------------+-----------------+\n",
      "| 2025-04-21 00:00:00 |        1190005 |             914159 |           29446 |        4930713 |            816260 |      36,984,961 |\n",
      "+---------------------+----------------+--------------------+-----------------+----------------+-------------------+-----------------+\n",
      "| 2025-04-28 00:00:00 |        1222035 |             936676 |           31857 |        5154569 |            847721 |      38,210,185 |\n",
      "+---------------------+----------------+--------------------+-----------------+----------------+-------------------+-----------------+\n",
      "| 2025-05-05 00:00:00 |        1205662 |             923543 |           30947 |        5262750 |            813689 |      36,773,342 |\n",
      "+---------------------+----------------+--------------------+-----------------+----------------+-------------------+-----------------+\n",
      "| 2025-05-12 00:00:00 |        1243056 |             946831 |           30781 |        5830321 |            829285 |      36,649,918 |\n",
      "+---------------------+----------------+--------------------+-----------------+----------------+-------------------+-----------------+\n",
      "\n",
      "--- Evolution of Key Weekly Fractions (First 10 Weeks) ---\n",
      "+---------------------+------------------+----------------+-------------------+\n",
      "| week                |   purchaser_rate |   holdout_rate |   clicks_per_user |\n",
      "+=====================+==================+================+===================+\n",
      "| 2025-03-10 00:00:00 |           0.8043 |         0.0258 |            2.6890 |\n",
      "+---------------------+------------------+----------------+-------------------+\n",
      "| 2025-03-17 00:00:00 |           0.7798 |         0.0307 |            3.3200 |\n",
      "+---------------------+------------------+----------------+-------------------+\n",
      "| 2025-03-24 00:00:00 |           0.7809 |         0.0293 |            3.4799 |\n",
      "+---------------------+------------------+----------------+-------------------+\n",
      "| 2025-03-31 00:00:00 |           0.7828 |         0.0285 |            3.5329 |\n",
      "+---------------------+------------------+----------------+-------------------+\n",
      "| 2025-04-07 00:00:00 |           0.7743 |         0.0259 |            3.8135 |\n",
      "+---------------------+------------------+----------------+-------------------+\n",
      "| 2025-04-14 00:00:00 |           0.7680 |         0.0253 |            4.0562 |\n",
      "+---------------------+------------------+----------------+-------------------+\n",
      "| 2025-04-21 00:00:00 |           0.7682 |         0.0247 |            4.1434 |\n",
      "+---------------------+------------------+----------------+-------------------+\n",
      "| 2025-04-28 00:00:00 |           0.7665 |         0.0261 |            4.2180 |\n",
      "+---------------------+------------------+----------------+-------------------+\n",
      "| 2025-05-05 00:00:00 |           0.7660 |         0.0257 |            4.3650 |\n",
      "+---------------------+------------------+----------------+-------------------+\n",
      "| 2025-05-12 00:00:00 |           0.7617 |         0.0248 |            4.6903 |\n",
      "+---------------------+------------------+----------------+-------------------+\n"
     ]
    }
   ],
   "source": [
    "if 'df_enriched' in locals():\n",
    "    print(\"\\n--- Performing Overall and Temporal EDA on the Enriched Panel ---\")\n",
    "\n",
    "    # --- 1. Calculate Overall Summary Statistics ---\n",
    "    print(\"-> Calculating overall summary stats...\")\n",
    "    summary_stats = {\n",
    "        \"Total Rows (User-Weeks)\": df_enriched.height,\n",
    "        \"Total Unique Users\": df_enriched['user_id'].n_unique(),\n",
    "        \"Total Unique Purchasers\": df_enriched.filter(pl.col('is_purchaser') == 1)['user_id'].n_unique(),\n",
    "        \"Total Unique Holdouts\": df_enriched.filter(pl.col('is_holdout') == 1)['user_id'].n_unique(),\n",
    "        \"Total Clicks\": df_enriched['clicks'].sum(),\n",
    "        \"Total Purchases\": df_enriched['purchases'].sum(),\n",
    "        \"Total Revenue\": df_enriched['revenue_dollars'].sum()\n",
    "    }\n",
    "    print(\"   ✅ Overall stats calculated.\")\n",
    "\n",
    "    # --- 2. Calculate Weekly Evolution of Metrics ---\n",
    "    print(\"-> Calculating weekly evolution of metrics...\")\n",
    "    df_weekly = df_enriched.group_by('week').agg(\n",
    "        active_users=pl.n_unique('user_id'),\n",
    "        purchasing_users=pl.col('user_id').filter(pl.col('is_purchaser') == 1).n_unique(),\n",
    "        holdout_users=pl.col('user_id').filter(pl.col('is_holdout') == 1).n_unique(),\n",
    "        total_clicks=pl.sum('clicks'),\n",
    "        total_purchases=pl.sum('purchases'),\n",
    "        total_revenue=pl.sum('revenue_dollars')\n",
    "    ).sort('week')\n",
    "    print(\"   ✅ Weekly aggregates calculated.\")\n",
    "\n",
    "    # --- 3. Calculate Key Weekly Fractions ---\n",
    "    print(\"-> Calculating weekly fractions...\")\n",
    "    df_weekly_fractions = df_weekly.with_columns(\n",
    "        # What percentage of active users in a week are purchasers?\n",
    "        purchaser_rate = pl.col('purchasing_users') / pl.col('active_users'),\n",
    "        # What percentage of active users in a week are from the holdout group?\n",
    "        holdout_rate = pl.col('holdout_users') / pl.col('active_users'),\n",
    "        # How many clicks per active user?\n",
    "        clicks_per_user = pl.col('total_clicks') / pl.col('active_users')\n",
    "    ).select(['week', 'purchaser_rate', 'holdout_rate', 'clicks_per_user'])\n",
    "    print(\"   ✅ Weekly fractions calculated.\")\n",
    "    \n",
    "    # --- 4. Generate the Final Report ---\n",
    "    report_filename = \"panel_eda_summary_report.txt\"\n",
    "    with open(report_filename, \"w\") as f:\n",
    "        f.write(\"Exploratory Data Analysis of the Enriched User-Week Panel\\n\")\n",
    "        f.write(\"=\" * 58 + \"\\n\\n\")\n",
    "\n",
    "        f.write(\"Overall Dataset Summary\\n\")\n",
    "        f.write(\"-----------------------\\n\")\n",
    "        for key, value in summary_stats.items():\n",
    "            f.write(f\"- {key}: {value:,.0f}\\n\")\n",
    "        f.write(\"\\n\\n\")\n",
    "\n",
    "        f.write(\"Evolution of Key Weekly Metrics\\n\")\n",
    "        f.write(\"-------------------------------\\n\")\n",
    "        f.write(tabulate(df_weekly.to_pandas(), headers='keys', tablefmt='grid', showindex=False, floatfmt=\",.0f\"))\n",
    "        f.write(\"\\n\\n\")\n",
    "\n",
    "        f.write(\"Evolution of Key Weekly Fractions\\n\")\n",
    "        f.write(\"---------------------------------\\n\")\n",
    "        f.write(tabulate(df_weekly_fractions.to_pandas(), headers='keys', tablefmt='grid', showindex=False, floatfmt=\".4f\"))\n",
    "\n",
    "    print(f\"\\n✅ ANALYSIS COMPLETE. A detailed EDA report has been saved to '{report_filename}'\")\n",
    "    \n",
    "    # --- 5. Print a sample of the results to the console ---\n",
    "    print(\"\\n--- Evolution of Key Weekly Metrics (First 10 Weeks) ---\")\n",
    "    print(tabulate(df_weekly.head(10).to_pandas(), headers='keys', tablefmt='grid', showindex=False, floatfmt=\",.0f\"))\n",
    "    \n",
    "    print(\"\\n--- Evolution of Key Weekly Fractions (First 10 Weeks) ---\")\n",
    "    print(tabulate(df_weekly_fractions.head(10).to_pandas(), headers='keys', tablefmt='grid', showindex=False, floatfmt=\".4f\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "353ca7e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Phase 1: Loading Data and Defining Periods ---\n",
      "✅ Successfully loaded panel with 32,060,768 rows.\n",
      "   -> Split data with cutoff: 2025-07-01\n",
      "\n",
      "--- Phase 2: Constructing DataFrame with Controls ---\n",
      "   -> Identified 1,119,128 users for the analysis base.\n",
      "   -> Final analysis DataFrame has 1,119,128 users (rows).\n",
      "\n",
      "--- Phase 3: Performing Covariate Balance Check ---\n",
      "   -> Summary statistics for controls calculated.\n",
      "\n",
      "--- Phase 4: Generating Final Report ---\n",
      "\n",
      "✅ ANALYSIS COMPLETE. Covariate balance report saved to 'covariate_balance_check_report.txt'\n",
      "\n",
      "--- Covariate Balance Check ---\n",
      "+-----------+--------------+------------------+--------------------+-----------------+---------------------+\n",
      "| cohort    |   user_count |   avg_revenue_p1 |   avg_purchases_p1 |   avg_clicks_p1 |   median_revenue_p1 |\n",
      "+===========+==============+==================+====================+=================+=====================+\n",
      "| Treatment |      1111277 |           391.34 |               9.30 |           48.92 |              173.00 |\n",
      "+-----------+--------------+------------------+--------------------+-----------------+---------------------+\n",
      "| Control   |         7851 |           238.40 |               5.91 |            3.38 |              129.00 |\n",
      "+-----------+--------------+------------------+--------------------+-----------------+---------------------+\n"
     ]
    }
   ],
   "source": [
    "import polars as pl\n",
    "from tabulate import tabulate\n",
    "\n",
    "# --- Configuration ---\n",
    "ENRICHED_PANEL_FILE = \"user_panel_with_holdout_flag.parquet\"\n",
    "FINAL_REPORT_FILE = \"covariate_balance_check_report.txt\"\n",
    "CUTOFF_DATE = \"2025-07-01\"\n",
    "\n",
    "# ==============================================================================\n",
    "# PHASE 1: LOAD DATA AND DEFINE TIME PERIODS\n",
    "# ==============================================================================\n",
    "print(\"--- Phase 1: Loading Data and Defining Periods ---\")\n",
    "try:\n",
    "    df_enriched = pl.read_parquet(ENRICHED_PANEL_FILE)\n",
    "    print(f\"✅ Successfully loaded panel with {df_enriched.height:,} rows.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"❌ FATAL ERROR: The enriched panel file '{ENRICHED_PANEL_FILE}' was not found.\")\n",
    "    df_enriched = None\n",
    "\n",
    "if df_enriched is not None:\n",
    "    cutoff_date_pl = pl.lit(CUTOFF_DATE).str.to_date()\n",
    "    df_period1 = df_enriched.filter(pl.col(\"week\") < cutoff_date_pl)\n",
    "    df_period2 = df_enriched.filter(pl.col(\"week\") >= cutoff_date_pl)\n",
    "    print(f\"   -> Split data with cutoff: {CUTOFF_DATE}\")\n",
    "\n",
    "    # ==============================================================================\n",
    "    # PHASE 2: CONSTRUCT THE ANALYSIS DATAFRAME\n",
    "    # ==============================================================================\n",
    "    print(\"\\n--- Phase 2: Constructing DataFrame with Controls ---\")\n",
    "\n",
    "    # Define Base Population (>= 3 purchases in P1)\n",
    "    p1_user_purchases = df_period1.group_by(\"user_id\").agg(total_purchases_p1=pl.sum(\"purchases\"))\n",
    "    base_users = p1_user_purchases.filter(pl.col(\"total_purchases_p1\") >= 3).select(\"user_id\")\n",
    "    print(f\"   -> Identified {base_users.height:,} users for the analysis base.\")\n",
    "\n",
    "    # Engineer Controls (X) from Period 1\n",
    "    df_controls_p1 = df_period1.group_by(\"user_id\").agg(\n",
    "        revenue_p1=pl.sum(\"revenue_dollars\"),\n",
    "        purchases_p1=pl.sum(\"purchases\"),\n",
    "        clicks_p1=pl.sum(\"clicks\")\n",
    "    )\n",
    "    \n",
    "    # Get user holdout status\n",
    "    user_holdout_status = df_enriched.select([\"user_id\", \"is_holdout\"]).unique(subset=\"user_id\")\n",
    "    \n",
    "    # Construct the final DataFrame\n",
    "    df_analysis = base_users.join(user_holdout_status, on=\"user_id\", how=\"inner\")\n",
    "    df_analysis = df_analysis.join(df_controls_p1, on=\"user_id\", how=\"left\").fill_null(0)\n",
    "    print(f\"   -> Final analysis DataFrame has {df_analysis.height:,} users (rows).\")\n",
    "    \n",
    "    # ==============================================================================\n",
    "    # PHASE 3: COVARIATE BALANCE CHECK (INSPECTING THE CONTROLS)\n",
    "    # ==============================================================================\n",
    "    print(\"\\n--- Phase 3: Performing Covariate Balance Check ---\")\n",
    "\n",
    "    # Group by cohort and calculate summary stats for Period 1 controls\n",
    "    control_summary = df_analysis.group_by(\"is_holdout\").agg(\n",
    "        user_count=pl.len(),\n",
    "        avg_revenue_p1=pl.mean(\"revenue_p1\"),\n",
    "        avg_purchases_p1=pl.mean(\"purchases_p1\"),\n",
    "        avg_clicks_p1=pl.mean(\"clicks_p1\"),\n",
    "        median_revenue_p1=pl.median(\"revenue_p1\")\n",
    "    ).with_columns(\n",
    "        cohort = pl.when(pl.col('is_holdout') == 1).then(pl.lit(\"Control\")).otherwise(pl.lit(\"Treatment\"))\n",
    "    ).select(\"cohort\", \"user_count\", \"avg_revenue_p1\", \"avg_purchases_p1\", \"avg_clicks_p1\", \"median_revenue_p1\")\n",
    "    \n",
    "    print(\"   -> Summary statistics for controls calculated.\")\n",
    "    \n",
    "    # ==============================================================================\n",
    "    # PHASE 4: FINAL REPORTING\n",
    "    # ==============================================================================\n",
    "    print(f\"\\n--- Phase 4: Generating Final Report ---\")\n",
    "\n",
    "    with open(FINAL_REPORT_FILE, \"w\") as f:\n",
    "        f.write(\"Covariate Balance Check Report (Comparison of Period 1 Controls)\\n\")\n",
    "        f.write(\"=\" * 64 + \"\\n\\n\")\n",
    "        f.write(\"Methodology:\\n\")\n",
    "        f.write(\"This table compares the average characteristics of the Treatment and Control groups\\n\")\n",
    "        f.write(\"based *only* on their activity in Period 1 (before the outcome period).\\n\")\n",
    "        f.write(\"Large differences here indicate strong selection bias that regression must control for.\\n\\n\")\n",
    "        \n",
    "        f.write(tabulate(control_summary.to_pandas(), headers='keys', tablefmt='grid', showindex=False, floatfmt=\".2f\"))\n",
    "\n",
    "    print(f\"\\n✅ ANALYSIS COMPLETE. Covariate balance report saved to '{FINAL_REPORT_FILE}'\")\n",
    "    \n",
    "    # Print the results to the console\n",
    "    print(\"\\n--- Covariate Balance Check ---\")\n",
    "    print(tabulate(control_summary.to_pandas(), headers='keys', tablefmt='grid', showindex=False, floatfmt=\".2f\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "783263dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Phase 1: Engineering Features, Outcomes, and Tenure Proxy ---\n",
      "   -> Identified 1,119,128 users for the analysis base.\n",
      "   -> Engineered 'join_week_index_p1' as a tenure proxy.\n",
      "\n",
      "--- Phase 2: Constructing Final DataFrame for Regression ---\n",
      "   -> Final DataFrame for analysis has 1,119,128 users (rows).\n",
      "\n",
      "--- Phase 3: Estimating ATE with Tenure Control ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pranjal/Code/topsort-incrementality/venv/lib/python3.13/site-packages/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/pranjal/Code/topsort-incrementality/venv/lib/python3.13/site-packages/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/pranjal/Code/topsort-incrementality/venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/pranjal/Code/topsort-incrementality/venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/pranjal/Code/topsort-incrementality/venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/pranjal/Code/topsort-incrementality/venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/pranjal/Code/topsort-incrementality/venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/pranjal/Code/topsort-incrementality/venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/pranjal/Code/topsort-incrementality/venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/pranjal/Code/topsort-incrementality/venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/pranjal/Code/topsort-incrementality/venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/pranjal/Code/topsort-incrementality/venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/pranjal/Code/topsort-incrementality/venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/pranjal/Code/topsort-incrementality/venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/pranjal/Code/topsort-incrementality/venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/pranjal/Code/topsort-incrementality/venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/pranjal/Code/topsort-incrementality/venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/pranjal/Code/topsort-incrementality/venv/lib/python3.13/site-packages/doubleml/utils/_checks.py:194: UserWarning: Propensity predictions from learner LGBMClassifier(n_jobs=-1, random_state=42, verbose=-1) for ml_m are close to zero or one (eps=1e-12).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -> DoubleML IRM model fitting complete.\n",
      "                coef   std err         t  P>|t|    2.5 %   97.5 %\n",
      "is_treated  7.837105  0.171577  45.67686    0.0  7.50082  8.17339\n",
      "\n",
      "--- Phase 4: Analyzing Heterogeneous Effects with GATEs ---\n",
      "   -> Group Average Treatment Effects (GATEs) calculated.\n",
      "================== DoubleMLBLP Object ==================\n",
      "\n",
      "------------------ Fit summary ------------------\n",
      "                      coef   std err          t         P>|t|    [0.025  \\\n",
      "Early_Joiners_P1  9.797284  0.200115  48.958217  0.000000e+00  9.405065   \n",
      "Late_Joiners_P1   4.873772  0.306875  15.881931  8.453542e-57  4.272307   \n",
      "\n",
      "                     0.975]  \n",
      "Early_Joiners_P1  10.189502  \n",
      "Late_Joiners_P1    5.475236  \n",
      "\n",
      "--- Phase 5: Generating Final Report ---\n",
      "\n",
      "✅ ANALYSIS COMPLETE. Final report saved to 'doubleml_irm_tenure_control_report.txt'\n"
     ]
    }
   ],
   "source": [
    "!pip install doubleml scikit-learn lightgbm -q\n",
    "\n",
    "import polars as pl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from tabulate import tabulate\n",
    "import doubleml as dml\n",
    "from lightgbm import LGBMRegressor, LGBMClassifier\n",
    "from sklearn.base import clone\n",
    "\n",
    "# --- Configuration ---\n",
    "ENRICHED_PANEL_FILE = \"user_panel_with_holdout_flag.parquet\"\n",
    "FINAL_REPORT_FILE = \"doubleml_irm_tenure_control_report.txt\"\n",
    "CUTOFF_DATE = \"2025-07-01\"\n",
    "\n",
    "# ==============================================================================\n",
    "# PHASE 1: FEATURE & OUTCOME ENGINEERING (WITH TENURE PROXY)\n",
    "# ==============================================================================\n",
    "print(\"--- Phase 1: Engineering Features, Outcomes, and Tenure Proxy ---\")\n",
    "\n",
    "df_enriched = pl.read_parquet(ENRICHED_PANEL_FILE)\n",
    "cutoff_date_pl = pl.lit(CUTOFF_DATE).str.to_date()\n",
    "df_period1 = df_enriched.filter(pl.col(\"week\") < cutoff_date_pl)\n",
    "df_period2 = df_enriched.filter(pl.col(\"week\") >= cutoff_date_pl)\n",
    "\n",
    "# Define Base Population (>= 3 purchases in P1)\n",
    "p1_user_purchases = df_period1.group_by(\"user_id\").agg(total_purchases_p1=pl.sum(\"purchases\"))\n",
    "base_users = p1_user_purchases.filter(pl.col(\"total_purchases_p1\") >= 3).select(\"user_id\")\n",
    "print(f\"   -> Identified {base_users.height:,} users for the analysis base.\")\n",
    "\n",
    "# --- Engineer the Tenure Proxy ---\n",
    "df_tenure = df_period1.group_by(\"user_id\").agg(\n",
    "    first_week_p1=pl.min(\"week\")\n",
    ")\n",
    "period1_start_date = df_period1.select(pl.min(\"week\"))[0, 0]\n",
    "\n",
    "# --- THE FIX IS HERE: Use .dt.total_days() instead of .dt.days() ---\n",
    "df_tenure = df_tenure.with_columns(\n",
    "    join_week_index_p1 = ((pl.col('first_week_p1') - period1_start_date).dt.total_days() // 7)\n",
    ").select([\"user_id\", \"join_week_index_p1\"])\n",
    "print(\"   -> Engineered 'join_week_index_p1' as a tenure proxy.\")\n",
    "\n",
    "# Engineer Per-Week Controls (X) from Period 1\n",
    "df_controls_p1 = df_period1.group_by(\"user_id\").agg(\n",
    "    avg_weekly_revenue_p1=pl.mean(\"revenue_dollars\"),\n",
    "    avg_weekly_purchases_p1=pl.mean(\"purchases\"),\n",
    "    avg_weekly_clicks_p1=pl.mean(\"clicks\")\n",
    ")\n",
    "\n",
    "# Engineer Per-Week Outcome (Y) from Period 2\n",
    "df_outcomes_p2 = df_period2.group_by(\"user_id\").agg(\n",
    "    avg_weekly_revenue_p2=pl.mean(\"revenue_dollars\")\n",
    ")\n",
    "\n",
    "# ==============================================================================\n",
    "# PHASE 2: CONSTRUCT FINAL REGRESSION DATAFRAME\n",
    "# ==============================================================================\n",
    "print(\"\\n--- Phase 2: Constructing Final DataFrame for Regression ---\")\n",
    "user_holdout_status = df_enriched.select([\"user_id\", \"is_holdout\"]).unique(subset=\"user_id\")\n",
    "\n",
    "df_analysis = base_users.join(user_holdout_status, on=\"user_id\", how=\"inner\")\n",
    "df_analysis = df_analysis.join(df_controls_p1, on=\"user_id\", how=\"left\").fill_null(0)\n",
    "df_analysis = df_analysis.join(df_outcomes_p2, on=\"user_id\", how=\"left\").fill_null(0)\n",
    "df_analysis = df_analysis.join(df_tenure, on=\"user_id\", how=\"left\").fill_null(0)\n",
    "df_analysis = df_analysis.with_columns(is_treated = 1 - pl.col(\"is_holdout\"))\n",
    "\n",
    "data_pd = df_analysis.to_pandas()\n",
    "float_cols = [col for col in data_pd.columns if 'revenue' in col or 'purchases' in col or 'clicks' in col or 'index' in col]\n",
    "data_pd[float_cols] = data_pd[float_cols].astype('float64')\n",
    "\n",
    "print(f\"   -> Final DataFrame for analysis has {len(data_pd):,} users (rows).\")\n",
    "\n",
    "# ==============================================================================\n",
    "# PHASE 3: DOUBLEML SETUP AND ATE ESTIMATION\n",
    "# ==============================================================================\n",
    "print(\"\\n--- Phase 3: Estimating ATE with Tenure Control ---\")\n",
    "\n",
    "x_cols = ['avg_weekly_revenue_p1', 'avg_weekly_purchases_p1', 'avg_weekly_clicks_p1', 'join_week_index_p1']\n",
    "dml_data = dml.DoubleMLData(data_pd,\n",
    "                              y_col='avg_weekly_revenue_p2',\n",
    "                              d_cols='is_treated',\n",
    "                              x_cols=x_cols)\n",
    "\n",
    "learner_g = LGBMRegressor(n_jobs=-1, random_state=42, verbose=-1)\n",
    "learner_m = LGBMClassifier(n_jobs=-1, random_state=42, verbose=-1)\n",
    "\n",
    "dml_irm_obj = dml.DoubleMLIRM(dml_data,\n",
    "                            ml_g=clone(learner_g),\n",
    "                            ml_m=clone(learner_m))\n",
    "\n",
    "dml_irm_obj.fit(store_predictions=True)\n",
    "print(\"   -> DoubleML IRM model fitting complete.\")\n",
    "print(dml_irm_obj.summary)\n",
    "\n",
    "# ==============================================================================\n",
    "# PHASE 4: HETEROGENEITY ANALYSIS (GATES)\n",
    "# ==============================================================================\n",
    "print(\"\\n--- Phase 4: Analyzing Heterogeneous Effects with GATEs ---\")\n",
    "\n",
    "tenure_median = data_pd['join_week_index_p1'].median()\n",
    "\n",
    "groups = pd.DataFrame({\n",
    "    'Early_Joiners_P1': (data_pd['join_week_index_p1'] <= tenure_median),\n",
    "    'Late_Joiners_P1': (data_pd['join_week_index_p1'] > tenure_median)\n",
    "})\n",
    "\n",
    "gate_results = dml_irm_obj.gate(groups)\n",
    "print(\"   -> Group Average Treatment Effects (GATEs) calculated.\")\n",
    "print(gate_results)\n",
    "\n",
    "# ==============================================================================\n",
    "# PHASE 5: FINAL REPORTING\n",
    "# ==============================================================================\n",
    "print(f\"\\n--- Phase 5: Generating Final Report ---\")\n",
    "\n",
    "with open(FINAL_REPORT_FILE, \"w\") as f:\n",
    "    f.write(\"DoubleML IRM Analysis with Tenure Control\\n\")\n",
    "    f.write(\"=\" * 41 + \"\\n\\n\")\n",
    "    f.write(\"Methodology:\\n\")\n",
    "    f.write(\"A DoubleML IRM model was used to estimate the causal effect of ad exposure on the\\n\")\n",
    "    f.write(\"average weekly revenue in Period 2. The model now includes a 'join_week_index_p1'\\n\")\n",
    "    f.write(\"as a tenure proxy to control for when a user first became active.\\n\\n\")\n",
    "    \n",
    "    f.write(\"Overall Average Treatment Effect (ATE) on Weekly Revenue\\n\")\n",
    "    f.write(\"--------------------------------------------------------\\n\")\n",
    "    f.write(str(dml_irm_obj.summary))\n",
    "    f.write(\"\\n\\n\")\n",
    "    \n",
    "    f.write(\"Heterogeneous Effects by User Tenure (GATEs)\\n\")\n",
    "    f.write(\"--------------------------------------------\\n\")\n",
    "    f.write(\"This table shows if the ATE on weekly revenue is different for users who joined\\n\")\n",
    "    f.write(\"early in Period 1 vs. those who joined later.\\n\\n\")\n",
    "    f.write(str(gate_results))\n",
    "\n",
    "print(f\"\\n✅ ANALYSIS COMPLETE. Final report saved to '{FINAL_REPORT_FILE}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1045f040",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Phase 1: Engineering Features, Outcomes, and Tenure Proxy ---\n",
      "   -> Identified 1,119,128 users for the analysis base.\n",
      "   -> Engineered 'join_week_index_p1' as a tenure proxy.\n",
      "\n",
      "--- Phase 2: Constructing Final DataFrame for Regression ---\n",
      "   -> Final DataFrame for analysis has 1,119,128 users (rows).\n",
      "\n",
      "--- Phase 3: Estimating ATE with Tenure Control ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pranjal/Code/topsort-incrementality/venv/lib/python3.13/site-packages/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/pranjal/Code/topsort-incrementality/venv/lib/python3.13/site-packages/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/pranjal/Code/topsort-incrementality/venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/pranjal/Code/topsort-incrementality/venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/pranjal/Code/topsort-incrementality/venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/pranjal/Code/topsort-incrementality/venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/pranjal/Code/topsort-incrementality/venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/pranjal/Code/topsort-incrementality/venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/pranjal/Code/topsort-incrementality/venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/pranjal/Code/topsort-incrementality/venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/pranjal/Code/topsort-incrementality/venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/pranjal/Code/topsort-incrementality/venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/pranjal/Code/topsort-incrementality/venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/pranjal/Code/topsort-incrementality/venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/pranjal/Code/topsort-incrementality/venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/pranjal/Code/topsort-incrementality/venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/pranjal/Code/topsort-incrementality/venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/pranjal/Code/topsort-incrementality/venv/lib/python3.13/site-packages/doubleml/utils/_checks.py:194: UserWarning: Propensity predictions from learner LGBMClassifier(n_jobs=-1, random_state=42, verbose=-1) for ml_m are close to zero or one (eps=1e-12).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -> DoubleML IRM model fitting complete.\n",
      "\n",
      "--- Final Analysis: Interpreting Heterogeneous Treatment Effects ---\n",
      "\n",
      "============================================================\n",
      "Overall Average Treatment Effect (ATE) on Weekly Revenue\n",
      "============================================================\n",
      "  - Baseline (Control Group Avg. Weekly Revenue): $27.32\n",
      "  - Absolute Incremental Lift:                    $7.58 per user per week\n",
      "  - 95% Confidence Interval (Absolute):         ($7.26, $7.91)\n",
      "  - Relative Incremental Lift:                    +27.76%\n",
      "  - 95% Confidence Interval (Relative):         (+26.56%, +28.96%)\n",
      "\n",
      "============================================================\n",
      "Heterogeneous Effects by: Avg. Weekly Revenue (P1)\n",
      "============================================================\n",
      "+--------------------------------------+----------------+----------------+--------------------+--------------------+\n",
      "| Subgroup                             | ATE ($ Lift)   | ATE (% Lift)   | 95% CI (% Lift)    | Baseline Revenue   |\n",
      "+======================================+================+================+====================+====================+\n",
      "| High (avg_weekly_revenue_p1 > 30.07) | $13.21         | +41.68%        | (+39.69%, +43.68%) | $31.69             |\n",
      "+--------------------------------------+----------------+----------------+--------------------+--------------------+\n",
      "| Low (avg_weekly_revenue_p1 <= 30.07) | $1.96          | +15.02%        | (+13.74%, +16.30%) | $13.05             |\n",
      "+--------------------------------------+----------------+----------------+--------------------+--------------------+\n",
      "\n",
      "============================================================\n",
      "Heterogeneous Effects by: Avg. Weekly Purchases (P1)\n",
      "============================================================\n",
      "+---------------------------------------+----------------+----------------+--------------------+--------------------+\n",
      "| Subgroup                              | ATE ($ Lift)   | ATE (% Lift)   | 95% CI (% Lift)    | Baseline Revenue   |\n",
      "+=======================================+================+================+====================+====================+\n",
      "| High (avg_weekly_purchases_p1 > 1.00) | $12.94         | +44.92%        | (+42.49%, +47.36%) | $28.80             |\n",
      "+---------------------------------------+----------------+----------------+--------------------+--------------------+\n",
      "| Low (avg_weekly_purchases_p1 <= 1.00) | $3.61          | +17.00%        | (+15.93%, +18.08%) | $21.22             |\n",
      "+---------------------------------------+----------------+----------------+--------------------+--------------------+\n",
      "\n",
      "============================================================\n",
      "Heterogeneous Effects by: Avg. Weekly Clicks (P1)\n",
      "============================================================\n",
      "+------------------------------------+----------------+----------------+--------------------+--------------------+\n",
      "| Subgroup                           | ATE ($ Lift)   | ATE (% Lift)   | 95% CI (% Lift)    | Baseline Revenue   |\n",
      "+====================================+================+================+====================+====================+\n",
      "| High (avg_weekly_clicks_p1 > 2.75) | $9.34          | +51.37%        | (+49.11%, +53.62%) | $18.18             |\n",
      "+------------------------------------+----------------+----------------+--------------------+--------------------+\n",
      "| Low (avg_weekly_clicks_p1 <= 2.75) | $5.85          | +21.05%        | (+19.23%, +22.88%) | $27.77             |\n",
      "+------------------------------------+----------------+----------------+--------------------+--------------------+\n",
      "\n",
      "============================================================\n",
      "Heterogeneous Effects by: Tenure (Join Week Index P1)\n",
      "============================================================\n",
      "+----------------------------------+----------------+----------------+--------------------+--------------------+\n",
      "| Subgroup                         | ATE ($ Lift)   | ATE (% Lift)   | 95% CI (% Lift)    | Baseline Revenue   |\n",
      "+==================================+================+================+====================+====================+\n",
      "| High (join_week_index_p1 > 2.00) | $4.53          | +16.99%        | (+14.82%, +19.16%) | $26.64             |\n",
      "+----------------------------------+----------------+----------------+--------------------+--------------------+\n",
      "| Low (join_week_index_p1 <= 2.00) | $9.60          | +33.59%        | (+32.24%, +34.94%) | $28.59             |\n",
      "+----------------------------------+----------------+----------------+--------------------+--------------------+\n",
      "\n",
      "✅ ANALYSIS COMPLETE.\n"
     ]
    }
   ],
   "source": [
    "!pip install doubleml scikit-learn lightgbm -q\n",
    "\n",
    "import polars as pl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from tabulate import tabulate\n",
    "import doubleml as dml\n",
    "from lightgbm import LGBMRegressor, LGBMClassifier\n",
    "from sklearn.base import clone\n",
    "\n",
    "# --- Configuration ---\n",
    "ENRICHED_PANEL_FILE = \"user_panel_with_holdout_flag.parquet\"\n",
    "CUTOFF_DATE = \"2025-07-01\"\n",
    "\n",
    "# ==============================================================================\n",
    "# PHASE 1: FEATURE & OUTCOME ENGINEERING (WITH TENURE PROXY)\n",
    "# ==============================================================================\n",
    "print(\"--- Phase 1: Engineering Features, Outcomes, and Tenure Proxy ---\")\n",
    "\n",
    "df_enriched = pl.read_parquet(ENRICHED_PANEL_FILE)\n",
    "cutoff_date_pl = pl.lit(CUTOFF_DATE).str.to_date()\n",
    "df_period1 = df_enriched.filter(pl.col(\"week\") < cutoff_date_pl)\n",
    "df_period2 = df_enriched.filter(pl.col(\"week\") >= cutoff_date_pl)\n",
    "\n",
    "# Define Base Population (>= 3 purchases in P1)\n",
    "p1_user_purchases = df_period1.group_by(\"user_id\").agg(total_purchases_p1=pl.sum(\"purchases\"))\n",
    "base_users = p1_user_purchases.filter(pl.col(\"total_purchases_p1\") >= 3).select(\"user_id\")\n",
    "print(f\"   -> Identified {base_users.height:,} users for the analysis base.\")\n",
    "\n",
    "# Engineer the Tenure Proxy\n",
    "df_tenure = df_period1.group_by(\"user_id\").agg(first_week_p1=pl.min(\"week\"))\n",
    "period1_start_date = df_period1.select(pl.min(\"week\"))[0, 0]\n",
    "df_tenure = df_tenure.with_columns(\n",
    "    join_week_index_p1 = ((pl.col('first_week_p1') - period1_start_date).dt.total_days() // 7)\n",
    ").select([\"user_id\", \"join_week_index_p1\"])\n",
    "print(\"   -> Engineered 'join_week_index_p1' as a tenure proxy.\")\n",
    "\n",
    "# Engineer Per-Week Controls (X) from Period 1\n",
    "df_controls_p1 = df_period1.group_by(\"user_id\").agg(\n",
    "    avg_weekly_revenue_p1=pl.mean(\"revenue_dollars\"),\n",
    "    avg_weekly_purchases_p1=pl.mean(\"purchases\"),\n",
    "    avg_weekly_clicks_p1=pl.mean(\"clicks\")\n",
    ")\n",
    "\n",
    "# Engineer Per-Week Outcome (Y) from Period 2\n",
    "df_outcomes_p2 = df_period2.group_by(\"user_id\").agg(\n",
    "    avg_weekly_revenue_p2=pl.mean(\"revenue_dollars\")\n",
    ")\n",
    "\n",
    "# ==============================================================================\n",
    "# PHASE 2: CONSTRUCT FINAL REGRESSION DATAFRAME\n",
    "# ==============================================================================\n",
    "print(\"\\n--- Phase 2: Constructing Final DataFrame for Regression ---\")\n",
    "user_holdout_status = df_enriched.select([\"user_id\", \"is_holdout\"]).unique(subset=\"user_id\")\n",
    "\n",
    "df_analysis = base_users.join(user_holdout_status, on=\"user_id\", how=\"inner\")\n",
    "df_analysis = df_analysis.join(df_controls_p1, on=\"user_id\", how=\"left\").fill_null(0)\n",
    "df_analysis = df_analysis.join(df_outcomes_p2, on=\"user_id\", how=\"left\").fill_null(0)\n",
    "df_analysis = df_analysis.join(df_tenure, on=\"user_id\", how=\"left\").fill_null(0)\n",
    "df_analysis = df_analysis.with_columns(is_treated = 1 - pl.col(\"is_holdout\"))\n",
    "\n",
    "data_pd = df_analysis.to_pandas()\n",
    "float_cols = [col for col in data_pd.columns if 'revenue' in col or 'purchases' in col or 'clicks' in col or 'index' in col]\n",
    "data_pd[float_cols] = data_pd[float_cols].astype('float64')\n",
    "\n",
    "print(f\"   -> Final DataFrame for analysis has {len(data_pd):,} users (rows).\")\n",
    "\n",
    "# ==============================================================================\n",
    "# PHASE 3: DOUBLEML ATE ESTIMATION WITH TENURE CONTROL\n",
    "# ==============================================================================\n",
    "print(\"\\n--- Phase 3: Estimating ATE with Tenure Control ---\")\n",
    "\n",
    "x_cols = ['avg_weekly_revenue_p1', 'avg_weekly_purchases_p1', 'avg_weekly_clicks_p1', 'join_week_index_p1']\n",
    "dml_data = dml.DoubleMLData(data_pd,\n",
    "                              y_col='avg_weekly_revenue_p2',\n",
    "                              d_cols='is_treated',\n",
    "                              x_cols=x_cols)\n",
    "\n",
    "learner_g = LGBMRegressor(n_jobs=-1, random_state=42, verbose=-1)\n",
    "learner_m = LGBMClassifier(n_jobs=-1, random_state=42, verbose=-1)\n",
    "\n",
    "dml_irm_obj = dml.DoubleMLIRM(dml_data,\n",
    "                            ml_g=clone(learner_g),\n",
    "                            ml_m=clone(learner_m))\n",
    "\n",
    "dml_irm_obj.fit(store_predictions=True)\n",
    "print(\"   -> DoubleML IRM model fitting complete.\")\n",
    "\n",
    "# ==============================================================================\n",
    "# PHASE 4: FINAL INTERPRETABLE REPORTING\n",
    "# ==============================================================================\n",
    "print(\"\\n--- Final Analysis: Interpreting Heterogeneous Treatment Effects ---\")\n",
    "\n",
    "# --- 4.1. INTERPRET THE OVERALL AVERAGE TREATMENT EFFECT (ATE) ---\n",
    "control_group_data = data_pd[data_pd['is_treated'] == 0]\n",
    "baseline_revenue = control_group_data['avg_weekly_revenue_p2'].mean()\n",
    "\n",
    "ate_summary = dml_irm_obj.summary\n",
    "ate_abs = ate_summary.loc['is_treated', 'coef']\n",
    "ate_lower = ate_summary.loc['is_treated', '2.5 %']\n",
    "ate_upper = ate_summary.loc['is_treated', '97.5 %']\n",
    "\n",
    "ate_pct = (ate_abs / baseline_revenue) * 100 if baseline_revenue > 0 else float('inf')\n",
    "ate_pct_lower = (ate_lower / baseline_revenue) * 100 if baseline_revenue > 0 else float('inf')\n",
    "ate_pct_upper = (ate_upper / baseline_revenue) * 100 if baseline_revenue > 0 else float('inf')\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Overall Average Treatment Effect (ATE) on Weekly Revenue\")\n",
    "print(\"=\"*60)\n",
    "print(f\"  - Baseline (Control Group Avg. Weekly Revenue): ${baseline_revenue:.2f}\")\n",
    "print(f\"  - Absolute Incremental Lift:                    ${ate_abs:.2f} per user per week\")\n",
    "print(f\"  - 95% Confidence Interval (Absolute):         (${ate_lower:.2f}, ${ate_upper:.2f})\")\n",
    "print(f\"  - Relative Incremental Lift:                    {ate_pct:+.2f}%\")\n",
    "print(f\"  - 95% Confidence Interval (Relative):         ({ate_pct_lower:+.2f}%, {ate_pct_upper:+.2f}%)\")\n",
    "\n",
    "# --- 4.2. COMPREHENSIVE HETEROGENEITY ANALYSIS (GATES) ---\n",
    "control_vars = {\n",
    "    'avg_weekly_revenue_p1': 'Avg. Weekly Revenue (P1)',\n",
    "    'avg_weekly_purchases_p1': 'Avg. Weekly Purchases (P1)',\n",
    "    'avg_weekly_clicks_p1': 'Avg. Weekly Clicks (P1)',\n",
    "    'join_week_index_p1': 'Tenure (Join Week Index P1)'\n",
    "}\n",
    "\n",
    "for var, name in control_vars.items():\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"Heterogeneous Effects by: {name}\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    median_val = data_pd[var].median()\n",
    "    groups = pd.DataFrame({\n",
    "        f'High ({var} > {median_val:.2f})': (data_pd[var] > median_val),\n",
    "        f'Low ({var} <= {median_val:.2f})': (data_pd[var] <= median_val)\n",
    "    })\n",
    "\n",
    "    gate_results = dml_irm_obj.gate(groups)\n",
    "    gate_summary_df = gate_results.summary\n",
    "\n",
    "    interpreted_gates = []\n",
    "    for group_name in gate_summary_df.index:\n",
    "        gate_abs = gate_summary_df.loc[group_name, 'coef']\n",
    "        # --- THE FIX IS HERE ---\n",
    "        # Use the correct, specific column names from the GATEs summary object\n",
    "        gate_lower = gate_summary_df.loc[group_name, '[0.025']\n",
    "        gate_upper = gate_summary_df.loc[group_name, '0.975]']\n",
    "        \n",
    "        group_mask = groups[group_name]\n",
    "        control_in_gate = data_pd[(data_pd['is_treated'] == 0) & (group_mask)]\n",
    "        gate_baseline = control_in_gate['avg_weekly_revenue_p2'].mean()\n",
    "        \n",
    "        if gate_baseline > 0:\n",
    "            gate_pct = (gate_abs / gate_baseline) * 100\n",
    "            gate_pct_lower = (gate_lower / gate_baseline) * 100\n",
    "            gate_pct_upper = (gate_upper / gate_baseline) * 100\n",
    "        else:\n",
    "            gate_pct, gate_pct_lower, gate_pct_upper = np.nan, np.nan, np.nan\n",
    "        \n",
    "        interpreted_gates.append({\n",
    "            'Subgroup': group_name,\n",
    "            'ATE ($ Lift)': f\"${gate_abs:.2f}\",\n",
    "            'ATE (% Lift)': f\"{gate_pct:+.2f}%\",\n",
    "            '95% CI (% Lift)': f\"({gate_pct_lower:+.2f}%, {gate_pct_upper:+.2f}%)\",\n",
    "            'Baseline Revenue': f\"${gate_baseline:.2f}\"\n",
    "        })\n",
    "    \n",
    "    gates_df = pd.DataFrame(interpreted_gates)\n",
    "    print(tabulate(gates_df, headers='keys', tablefmt='grid', showindex=False))\n",
    "\n",
    "print(\"\\n✅ ANALYSIS COMPLETE.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d88b5b",
   "metadata": {},
   "source": [
    "## User-Vendor Spend Panel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2fe2fd35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Connected to Snowflake\n",
      "\n",
      "--- Phase 1: Discovering Full Date Range from PURCHASES ---\n",
      "   -> Executing: Date Range Query...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/b7/1tvk5qmx0ds9c6gk2lrlhv380000gn/T/ipykernel_24409/2276747383.py:34: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, conn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✅ Query successful in 53.56s.\n",
      "   -> Purchases data ranges from 2025-03-14 to 2025-09-09.\n",
      "\n",
      "--- Phase 2: Building Purchaser Panel Incrementally ---\n",
      "\n",
      "Processing Week 1/26: 2025-03-17...\n",
      "   -> Executing: Week 1 Panel Query...\n",
      "   ✅ Query successful in 219.35s.\n",
      "   -> Data Shape: (51317, 5)\n",
      "   -> Weekly Stats: 45,780 users, 18,205 vendors\n",
      "\n",
      "Processing Week 2/26: 2025-03-24...\n",
      "   -> Executing: Week 2 Panel Query...\n",
      "   ✅ Query successful in 217.03s.\n",
      "   -> Data Shape: (60056, 5)\n",
      "   -> Weekly Stats: 53,472 users, 20,330 vendors\n",
      "\n",
      "Processing Week 3/26: 2025-03-31...\n",
      "   -> Executing: Week 3 Panel Query...\n",
      "   ✅ Query successful in 216.31s.\n",
      "   -> Data Shape: (65819, 5)\n",
      "   -> Weekly Stats: 58,067 users, 22,003 vendors\n",
      "\n",
      "Processing Week 4/26: 2025-04-07...\n",
      "   -> Executing: Week 4 Panel Query...\n",
      "   ✅ Query successful in 215.01s.\n",
      "   -> Data Shape: (69756, 5)\n",
      "   -> Weekly Stats: 61,582 users, 22,894 vendors\n",
      "\n",
      "Processing Week 5/26: 2025-04-14...\n",
      "   -> Executing: Week 5 Panel Query...\n",
      "   ✅ Query successful in 213.77s.\n",
      "   -> Data Shape: (73958, 5)\n",
      "   -> Weekly Stats: 64,998 users, 23,799 vendors\n",
      "\n",
      "Processing Week 6/26: 2025-04-21...\n",
      "   -> Executing: Week 6 Panel Query...\n",
      "   ✅ Query successful in 217.98s.\n",
      "   -> Data Shape: (80004, 5)\n",
      "   -> Weekly Stats: 70,041 users, 24,726 vendors\n",
      "\n",
      "Processing Week 7/26: 2025-04-28...\n",
      "   -> Executing: Week 7 Panel Query...\n",
      "   ✅ Query successful in 218.81s.\n",
      "   -> Data Shape: (85670, 5)\n",
      "   -> Weekly Stats: 74,432 users, 26,295 vendors\n",
      "\n",
      "Processing Week 8/26: 2025-05-05...\n",
      "   -> Executing: Week 8 Panel Query...\n",
      "   ✅ Query successful in 219.32s.\n",
      "   -> Data Shape: (84893, 5)\n",
      "   -> Weekly Stats: 73,696 users, 26,343 vendors\n",
      "\n",
      "Processing Week 9/26: 2025-05-12...\n",
      "   -> Executing: Week 9 Panel Query...\n",
      "   ✅ Query successful in 218.24s.\n",
      "   -> Data Shape: (94543, 5)\n",
      "   -> Weekly Stats: 81,389 users, 27,810 vendors\n",
      "\n",
      "Processing Week 10/26: 2025-05-19...\n",
      "   -> Executing: Week 10 Panel Query...\n",
      "   ✅ Query successful in 224.05s.\n",
      "   -> Data Shape: (95068, 5)\n",
      "   -> Weekly Stats: 81,527 users, 27,995 vendors\n",
      "\n",
      "Processing Week 11/26: 2025-05-26...\n",
      "   -> Executing: Week 11 Panel Query...\n",
      "   ✅ Query successful in 217.35s.\n",
      "   -> Data Shape: (100326, 5)\n",
      "   -> Weekly Stats: 85,815 users, 29,201 vendors\n",
      "\n",
      "Processing Week 12/26: 2025-06-02...\n",
      "   -> Executing: Week 12 Panel Query...\n",
      "   ✅ Query successful in 217.54s.\n",
      "   -> Data Shape: (98776, 5)\n",
      "   -> Weekly Stats: 84,568 users, 30,009 vendors\n",
      "\n",
      "Processing Week 13/26: 2025-06-09...\n",
      "   -> Executing: Week 13 Panel Query...\n",
      "   ✅ Query successful in 218.29s.\n",
      "   -> Data Shape: (96934, 5)\n",
      "   -> Weekly Stats: 83,251 users, 30,206 vendors\n",
      "\n",
      "Processing Week 14/26: 2025-06-16...\n",
      "   -> Executing: Week 14 Panel Query...\n",
      "   ✅ Query successful in 222.11s.\n",
      "   -> Data Shape: (97940, 5)\n",
      "   -> Weekly Stats: 83,826 users, 30,712 vendors\n",
      "\n",
      "Processing Week 15/26: 2025-06-23...\n",
      "   -> Executing: Week 15 Panel Query...\n",
      "   ✅ Query successful in 220.54s.\n",
      "   -> Data Shape: (98414, 5)\n",
      "   -> Weekly Stats: 84,217 users, 30,872 vendors\n",
      "\n",
      "Processing Week 16/26: 2025-06-30...\n",
      "   -> Executing: Week 16 Panel Query...\n",
      "   ✅ Query successful in 219.42s.\n",
      "   -> Data Shape: (96860, 5)\n",
      "   -> Weekly Stats: 82,242 users, 30,915 vendors\n",
      "\n",
      "Processing Week 17/26: 2025-07-07...\n",
      "   -> Executing: Week 17 Panel Query...\n",
      "   ✅ Query successful in 218.91s.\n",
      "   -> Data Shape: (98399, 5)\n",
      "   -> Weekly Stats: 83,995 users, 32,070 vendors\n",
      "\n",
      "Processing Week 18/26: 2025-07-14...\n",
      "   -> Executing: Week 18 Panel Query...\n",
      "   ✅ Query successful in 220.92s.\n",
      "   -> Data Shape: (104919, 5)\n",
      "   -> Weekly Stats: 89,194 users, 33,393 vendors\n",
      "\n",
      "Processing Week 19/26: 2025-07-21...\n",
      "   -> Executing: Week 19 Panel Query...\n",
      "   ✅ Query successful in 282.35s.\n",
      "   -> Data Shape: (105211, 5)\n",
      "   -> Weekly Stats: 89,737 users, 34,309 vendors\n",
      "\n",
      "Processing Week 20/26: 2025-07-28...\n",
      "   -> Executing: Week 20 Panel Query...\n",
      "   ✅ Query successful in 290.30s.\n",
      "   -> Data Shape: (110417, 5)\n",
      "   -> Weekly Stats: 93,572 users, 35,942 vendors\n",
      "\n",
      "Processing Week 21/26: 2025-08-04...\n",
      "   -> Executing: Week 21 Panel Query...\n",
      "   ✅ Query successful in 259.95s.\n",
      "   -> Data Shape: (111986, 5)\n",
      "   -> Weekly Stats: 95,041 users, 36,779 vendors\n",
      "\n",
      "Processing Week 22/26: 2025-08-11...\n",
      "   -> Executing: Week 22 Panel Query...\n",
      "   ✅ Query successful in 144.39s.\n",
      "   -> Data Shape: (113225, 5)\n",
      "   -> Weekly Stats: 95,979 users, 36,985 vendors\n",
      "\n",
      "Processing Week 23/26: 2025-08-18...\n",
      "   -> Executing: Week 23 Panel Query...\n",
      "   ✅ Query successful in 144.63s.\n",
      "   -> Data Shape: (113619, 5)\n",
      "   -> Weekly Stats: 96,615 users, 36,948 vendors\n",
      "\n",
      "Processing Week 24/26: 2025-08-25...\n",
      "   -> Executing: Week 24 Panel Query...\n",
      "   ✅ Query successful in 143.51s.\n",
      "   -> Data Shape: (112889, 5)\n",
      "   -> Weekly Stats: 95,701 users, 37,094 vendors\n",
      "\n",
      "Processing Week 25/26: 2025-09-01...\n",
      "   -> Executing: Week 25 Panel Query...\n",
      "   ✅ Query successful in 144.66s.\n",
      "   -> Data Shape: (117276, 5)\n",
      "   -> Weekly Stats: 99,865 users, 38,624 vendors\n",
      "\n",
      "Processing Week 26/26: 2025-09-08...\n",
      "   -> Executing: Week 26 Panel Query...\n",
      "   ✅ Query successful in 140.32s.\n",
      "   -> Data Shape: (31189, 5)\n",
      "   -> Weekly Stats: 28,701 users, 17,948 vendors\n",
      "\n",
      "--- Phase 3: Finalizing Panel and Generating Report ---\n",
      "   -> Concatenated all weekly data into a final panel.\n",
      "✅ Final purchaser-only panel successfully saved to 'full_purchaser_vendor_panel_unfiltered.parquet'\n",
      "✅ Summary report saved to 'full_purchaser_vendor_panel_build_summary.txt'\n",
      "\n",
      "✅ Snowflake connection closed.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "import snowflake.connector\n",
    "import time\n",
    "from tabulate import tabulate\n",
    "\n",
    "# --- Standard Functions ---\n",
    "load_dotenv()\n",
    "\n",
    "def connect_to_snowflake():\n",
    "    \"\"\"Establishes a connection to Snowflake.\"\"\"\n",
    "    try:\n",
    "        conn = snowflake.connector.connect(\n",
    "            user=os.getenv('SNOWFLAKE_USER'),\n",
    "            password=os.getenv('SNOWFLAKE_PASSWORD'),\n",
    "            account=os.getenv('SNOWFLAKE_ACCOUNT'),\n",
    "            warehouse=os.getenv('SNOWFLAKE_WAREHOUSE', 'COMPUTE_WH'),\n",
    "            database='INCREMENTALITY',\n",
    "            schema='INCREMENTALITY_RESEARCH'\n",
    "        )\n",
    "        print(\"✅ Connected to Snowflake\")\n",
    "        return conn\n",
    "    except snowflake.connector.Error as e:\n",
    "        print(f\"❌ Could not connect to Snowflake: {e}\", file=sys.stderr)\n",
    "        return None\n",
    "\n",
    "def fetch_data(conn, query, query_name=\"Query\"):\n",
    "    \"\"\"Executes a query, times it, and returns a pandas DataFrame.\"\"\"\n",
    "    print(f\"   -> Executing: {query_name}...\")\n",
    "    start_time = time.time()\n",
    "    try:\n",
    "        df = pd.read_sql(query, conn)\n",
    "        duration = time.time() - start_time\n",
    "        print(f\"   ✅ Query successful in {duration:.2f}s.\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"   ❌ ERROR executing {query_name}: {e}\", file=sys.stderr)\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# --- NEW Refactored Query Functions ---\n",
    "\n",
    "def get_purchases_date_range(conn):\n",
    "    \"\"\"Queries the PURCHASES table to find its min and max dates.\"\"\"\n",
    "    print(\"\\n--- Phase 1: Discovering Full Date Range from PURCHASES ---\")\n",
    "    query = \"SELECT MIN(PURCHASED_AT) AS min_date, MAX(PURCHASED_AT) AS max_date FROM PURCHASES;\"\n",
    "    df_dates = fetch_data(conn, query, \"Date Range Query\")\n",
    "    if df_dates.empty:\n",
    "        return None, None\n",
    "    \n",
    "    start_date = pd.to_datetime(df_dates['MIN_DATE'].iloc[0])\n",
    "    end_date = pd.to_datetime(df_dates['MAX_DATE'].iloc[0])\n",
    "    \n",
    "    print(f\"   -> Purchases data ranges from {start_date.strftime('%Y-%m-%d')} to {end_date.strftime('%Y-%m-%d')}.\")\n",
    "    return start_date, end_date\n",
    "\n",
    "def build_weekly_purchases_query(week_start, week_end):\n",
    "    \"\"\"Builds the SQL to get one week of purchase-based panel data for ALL users and vendors.\"\"\"\n",
    "    return f\"\"\"\n",
    "    WITH\n",
    "    -- This map is necessary to link a purchase's product_id to a vendor_id.\n",
    "    PRODUCT_VENDOR_MAP AS (\n",
    "        SELECT DISTINCT PRODUCT_ID, VENDOR_ID \n",
    "        FROM CLICKS \n",
    "        WHERE PRODUCT_ID IS NOT NULL AND VENDOR_ID IS NOT NULL\n",
    "    )\n",
    "    -- The main query only uses the PURCHASES table for activity.\n",
    "    SELECT\n",
    "        p.USER_ID,\n",
    "        pvm.VENDOR_ID,\n",
    "        DATE_TRUNC('WEEK', p.PURCHASED_AT) AS week,\n",
    "        COUNT(DISTINCT p.PURCHASE_ID) AS purchases,\n",
    "        (COALESCE(SUM(p.QUANTITY * p.UNIT_PRICE), 0) / 100)::DECIMAL(18, 2) AS revenue_dollars\n",
    "    FROM PURCHASES AS p\n",
    "    JOIN PRODUCT_VENDOR_MAP AS pvm ON p.PRODUCT_ID = pvm.PRODUCT_ID\n",
    "    WHERE p.PURCHASED_AT >= '{week_start}' AND p.PURCHASED_AT < '{week_end}'\n",
    "    GROUP BY 1, 2, 3;\n",
    "    \"\"\"\n",
    "\n",
    "def main():\n",
    "    # --- CONFIGURATION ---\n",
    "    OUTPUT_FILENAME = \"full_purchaser_vendor_panel_unfiltered.parquet\"\n",
    "    REPORT_FILENAME = \"full_purchaser_vendor_panel_build_summary.txt\"\n",
    "    \n",
    "    conn = connect_to_snowflake()\n",
    "    if not conn:\n",
    "        sys.exit(1)\n",
    "\n",
    "    try:\n",
    "        # Phase 1: Get the full date range dynamically\n",
    "        start_date, end_date = get_purchases_date_range(conn)\n",
    "        if not start_date or not end_date:\n",
    "            print(\"Could not determine date range. Halting.\")\n",
    "            return\n",
    "\n",
    "        # Phase 2: Loop through each week of the analysis period\n",
    "        print(\"\\n--- Phase 2: Building Purchaser Panel Incrementally ---\")\n",
    "        weekly_panels = []\n",
    "        weekly_logs = []\n",
    "        date_range = pd.date_range(start=start_date, end=end_date, freq='W-MON')\n",
    "\n",
    "        for i, week_start in enumerate(date_range):\n",
    "            week_end = week_start + pd.DateOffset(weeks=1)\n",
    "            print(f\"\\nProcessing Week {i+1}/{len(date_range)}: {week_start.strftime('%Y-%m-%d')}...\")\n",
    "            \n",
    "            weekly_query = build_weekly_purchases_query(week_start.strftime('%Y-%m-%d'), week_end.strftime('%Y-%m-%d'))\n",
    "            weekly_df = fetch_data(conn, weekly_query, f\"Week {i+1} Panel Query\")\n",
    "            \n",
    "            if not weekly_df.empty:\n",
    "                log_entry = {\n",
    "                    \"Week\": week_start.strftime('%Y-%m-%d'),\n",
    "                    \"Rows\": len(weekly_df),\n",
    "                    \"Unique Users\": weekly_df['USER_ID'].nunique(),\n",
    "                    \"Unique Vendors\": weekly_df['VENDOR_ID'].nunique()\n",
    "                }\n",
    "                print(f\"   -> Data Shape: {weekly_df.shape}\")\n",
    "                print(f\"   -> Weekly Stats: {log_entry['Unique Users']:,} users, {log_entry['Unique Vendors']:,} vendors\")\n",
    "                weekly_logs.append(log_entry)\n",
    "                weekly_panels.append(weekly_df)\n",
    "            else:\n",
    "                print(\"   -> No data returned for this week.\")\n",
    "\n",
    "        # Phase 3: Concatenate, save, and report\n",
    "        print(\"\\n--- Phase 3: Finalizing Panel and Generating Report ---\")\n",
    "        if weekly_panels:\n",
    "            final_panel_df = pd.concat(weekly_panels, ignore_index=True)\n",
    "            print(f\"   -> Concatenated all weekly data into a final panel.\")\n",
    "            \n",
    "            # Add a 'clicks' column with all zeros for schema consistency\n",
    "            final_panel_df['clicks'] = 0\n",
    "            final_panel_df.columns = [col.lower() for col in final_panel_df.columns]\n",
    "            \n",
    "            # Reorder columns to the desired format\n",
    "            final_panel_df = final_panel_df[['user_id', 'vendor_id', 'week', 'purchases', 'revenue_dollars', 'clicks']]\n",
    "            \n",
    "            final_panel_df.to_parquet(OUTPUT_FILENAME, index=False, engine='pyarrow')\n",
    "            print(f\"✅ Final purchaser-only panel successfully saved to '{OUTPUT_FILENAME}'\")\n",
    "\n",
    "            with open(REPORT_FILENAME, \"w\") as f:\n",
    "                f.write(\"Full Purchaser-Vendor Panel Build Summary\\n\")\n",
    "                f.write(\"=\" * 42 + \"\\n\\n\")\n",
    "                f.write(\"Overall Panel Statistics\\n------------------------\\n\")\n",
    "                f.write(f\"- Total Rows: {len(final_panel_df):,}\\n\")\n",
    "                f.write(f\"- Total Unique Users: {final_panel_df['user_id'].nunique():,}\\n\")\n",
    "                f.write(f\"- Total Unique Vendors: {final_panel_df['vendor_id'].nunique():,}\\n\\n\")\n",
    "                f.write(\"Weekly Build Log\\n----------------\\n\")\n",
    "                summary_df = pd.DataFrame(weekly_logs)\n",
    "                f.write(tabulate(summary_df, headers='keys', tablefmt='grid', showindex=False))\n",
    "            print(f\"✅ Summary report saved to '{REPORT_FILENAME}'\")\n",
    "        else:\n",
    "            print(\"   -> No data was generated. No file saved.\")\n",
    "\n",
    "    finally:\n",
    "        if conn and not conn.is_closed():\n",
    "            conn.close()\n",
    "            print(\"\\n✅ Snowflake connection closed.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1725b6c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Phase 1: Loading All Source Datasets ---\n",
      "✅ All source panels loaded successfully.\n",
      "\n",
      "--- Phase 2: Defining Time Periods and Base Population ---\n",
      "   -> Identified 1,119,128 users for the analysis base (>= 3 purchases in P1).\n",
      "\n",
      "--- Phase 3: Engineering All Features from Period 1 ---\n",
      "   -> Engineered clicks and tenure features.\n",
      "   -> Engineered vendor-based loyalty and variety features.\n",
      "\n",
      "--- Phase 4: Engineering Outcome from Period 2 ---\n",
      "   -> Calculated Period 2 revenue for 5,397,437 users.\n",
      "\n",
      "--- Phase 5: Constructing and Saving the Final Analysis Dataset ---\n",
      "   -> Final dataset constructed with 1,119,128 users and 10 columns.\n",
      "✅ Final analysis-ready dataset saved to 'final_analysis_user_level_dataset.parquet'\n",
      "✅ A summary report of the new dataset has been saved to 'final_dataset_build_summary.txt'\n"
     ]
    }
   ],
   "source": [
    "import polars as pl\n",
    "import os\n",
    "from tabulate import tabulate\n",
    "\n",
    "# --- Configuration ---\n",
    "# Input files from previous steps\n",
    "USER_WEEK_PANEL_FILE = \"user_panel_with_holdout_flag.parquet\"\n",
    "USER_VENDOR_PANEL_FILE = \"full_purchaser_vendor_panel_unfiltered.parquet\"\n",
    "\n",
    "# Final output file for the analysis-ready dataset\n",
    "OUTPUT_FILE = \"final_analysis_user_level_dataset.parquet\"\n",
    "REPORT_FILE = \"final_dataset_build_summary.txt\"\n",
    "CUTOFF_DATE = \"2025-07-01\"\n",
    "\n",
    "# ==============================================================================\n",
    "# PHASE 1: LOAD ALL SOURCE DATA\n",
    "# ==============================================================================\n",
    "print(\"--- Phase 1: Loading All Source Datasets ---\")\n",
    "try:\n",
    "    df_user_panel = pl.read_parquet(USER_WEEK_PANEL_FILE)\n",
    "    df_vendor_panel = pl.read_parquet(USER_VENDOR_PANEL_FILE)\n",
    "    print(\"✅ All source panels loaded successfully.\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"❌ FATAL ERROR: A required input file was not found. Details: {e}\")\n",
    "    df_user_panel = None\n",
    "\n",
    "if df_user_panel is not None:\n",
    "    # ==============================================================================\n",
    "    # PHASE 2: DEFINE TIME PERIODS AND BASE POPULATION\n",
    "    # ==============================================================================\n",
    "    print(\"\\n--- Phase 2: Defining Time Periods and Base Population ---\")\n",
    "    cutoff_date_pl = pl.lit(CUTOFF_DATE).str.to_date()\n",
    "\n",
    "    # Split both panels by the cutoff date\n",
    "    df_user_p1 = df_user_panel.filter(pl.col(\"week\") < cutoff_date_pl)\n",
    "    df_user_p2 = df_user_panel.filter(pl.col(\"week\") >= cutoff_date_pl)\n",
    "    df_vendor_p1 = df_vendor_panel.filter(pl.col(\"week\") < cutoff_date_pl)\n",
    "    df_vendor_p2 = df_vendor_panel.filter(pl.col(\"week\") >= cutoff_date_pl)\n",
    "\n",
    "    # Define Base Population (>= 3 purchases in P1) from the user panel\n",
    "    p1_user_purchases = df_user_p1.group_by(\"user_id\").agg(total_purchases_p1=pl.sum(\"purchases\"))\n",
    "    base_users = p1_user_purchases.filter(pl.col(\"total_purchases_p1\") >= 3).select(\"user_id\")\n",
    "    print(f\"   -> Identified {base_users.height:,} users for the analysis base (>= 3 purchases in P1).\")\n",
    "\n",
    "    # ==============================================================================\n",
    "    # PHASE 3: FEATURE ENGINEERING FROM PERIOD 1\n",
    "    # ==============================================================================\n",
    "    print(\"\\n--- Phase 3: Engineering All Features from Period 1 ---\")\n",
    "\n",
    "    # Engineer \"older\" features (clicks, tenure) from the user panel\n",
    "    df_controls_user_p1 = df_user_p1.group_by(\"user_id\").agg(\n",
    "        clicks_p1=pl.sum(\"clicks\")\n",
    "    )\n",
    "    df_tenure = df_user_p1.group_by(\"user_id\").agg(first_week_p1=pl.min(\"week\"))\n",
    "    period1_start_date = df_user_p1.select(pl.min(\"week\"))[0, 0]\n",
    "    df_tenure = df_tenure.with_columns(\n",
    "        join_week_index_p1=((pl.col('first_week_p1') - period1_start_date).dt.total_days() // 7)\n",
    "    ).select([\"user_id\", \"join_week_index_p1\"])\n",
    "    print(\"   -> Engineered clicks and tenure features.\")\n",
    "\n",
    "    # Engineer \"newer\" features (vendor loyalty/variety) from the vendor panel\n",
    "    df_controls_vendor_p1_agg1 = df_vendor_p1.group_by([\"user_id\", \"vendor_id\"]).agg(\n",
    "        revenue_per_vendor=pl.sum(\"revenue_dollars\")\n",
    "    )\n",
    "    df_controls_vendor_p1 = df_controls_vendor_p1_agg1.group_by(\"user_id\").agg(\n",
    "        revenue_p1=pl.sum(\"revenue_per_vendor\"),\n",
    "        distinct_vendors_p1=pl.n_unique(\"vendor_id\"),\n",
    "        spend_on_top_vendor_p1=pl.max(\"revenue_per_vendor\")\n",
    "    ).with_columns(\n",
    "        spend_concentration_p1 = pl.col(\"spend_on_top_vendor_p1\") / pl.col(\"revenue_p1\")\n",
    "    ).drop(\"spend_on_top_vendor_p1\")\n",
    "    print(\"   -> Engineered vendor-based loyalty and variety features.\")\n",
    "    \n",
    "    # Also get purchases_p1 from the base definition step\n",
    "    df_purchases_p1 = p1_user_purchases.select([\"user_id\", pl.col(\"total_purchases_p1\").alias(\"purchases_p1\")])\n",
    "\n",
    "    # ==============================================================================\n",
    "    # PHASE 4: OUTCOME ENGINEERING FROM PERIOD 2\n",
    "    # ==============================================================================\n",
    "    print(\"\\n--- Phase 4: Engineering Outcome from Period 2 ---\")\n",
    "    df_outcome_p2 = df_user_p2.group_by(\"user_id\").agg(\n",
    "        revenue_p2=pl.sum(\"revenue_dollars\")\n",
    "    )\n",
    "    print(f\"   -> Calculated Period 2 revenue for {df_outcome_p2.height:,} users.\")\n",
    "\n",
    "    # ==============================================================================\n",
    "    # PHASE 5: CONSTRUCT AND SAVE THE FINAL DATASET\n",
    "    # ==============================================================================\n",
    "    print(\"\\n--- Phase 5: Constructing and Saving the Final Analysis Dataset ---\")\n",
    "    \n",
    "    # Get the static holdout flag for all users\n",
    "    user_holdout_status = df_user_panel.select([\"user_id\", \"is_holdout\"]).unique(subset=\"user_id\")\n",
    "    \n",
    "    # Start with our base of qualified users and their holdout status\n",
    "    df_final = base_users.join(user_holdout_status, on=\"user_id\", how=\"inner\")\n",
    "    \n",
    "    # Sequentially join all our engineered feature sets\n",
    "    df_final = df_final.join(df_purchases_p1, on=\"user_id\", how=\"left\")\n",
    "    df_final = df_final.join(df_controls_user_p1, on=\"user_id\", how=\"left\")\n",
    "    df_final = df_final.join(df_tenure, on=\"user_id\", how=\"left\")\n",
    "    df_final = df_final.join(df_controls_vendor_p1, on=\"user_id\", how=\"left\")\n",
    "    df_final = df_final.join(df_outcome_p2, on=\"user_id\", how=\"left\")\n",
    "    \n",
    "    # Create the 'is_treated' (T) flag and fill any nulls that may have resulted from joins\n",
    "    df_final = df_final.with_columns(is_treated = 1 - pl.col(\"is_holdout\")).fill_null(0)\n",
    "    \n",
    "    print(f\"   -> Final dataset constructed with {df_final.height:,} users and {df_final.width} columns.\")\n",
    "    \n",
    "    # Save the final dataset\n",
    "    df_final.write_parquet(OUTPUT_FILE)\n",
    "    print(f\"✅ Final analysis-ready dataset saved to '{OUTPUT_FILE}'\")\n",
    "\n",
    "    # Generate a final summary report of the created dataset\n",
    "    with open(REPORT_FILE, \"w\") as f:\n",
    "        f.write(\"Summary of Final Analysis-Ready Dataset\\n\")\n",
    "        f.write(\"=\" * 40 + \"\\n\\n\")\n",
    "        f.write(\"This dataset contains a single row for each user in the 'Base' population\\n\")\n",
    "        f.write(\"(>= 3 purchases in Period 1), with features from Period 1 and the outcome from Period 2.\\n\\n\")\n",
    "        f.write(f\"Total Users (Rows): {df_final.height:,}\\n\")\n",
    "        f.write(f\"Total Columns: {df_final.width}\\n\\n\")\n",
    "        f.write(\"Schema and Sample Data:\\n\")\n",
    "        f.write(\"-----------------------\\n\")\n",
    "        # Use pandas for tabulate, as it handles formatting well\n",
    "        f.write(tabulate(df_final.head(10).to_pandas(), headers='keys', tablefmt='grid', showindex=False, floatfmt=\".2f\"))\n",
    "\n",
    "    print(f\"✅ A summary report of the new dataset has been saved to '{REPORT_FILE}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "348f19f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Phase 1 & 2: Loading and Preparing Data ---\n",
      "✅ Successfully loaded and prepared data with 1,119,128 users.\n",
      "\n",
      "--- Phase 3: Estimating Overall Average Treatment Effect (ATE) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pranjal/Code/topsort-incrementality/venv/lib/python3.13/site-packages/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/pranjal/Code/topsort-incrementality/venv/lib/python3.13/site-packages/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/pranjal/Code/topsort-incrementality/venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/pranjal/Code/topsort-incrementality/venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/pranjal/Code/topsort-incrementality/venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/pranjal/Code/topsort-incrementality/venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/pranjal/Code/topsort-incrementality/venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/pranjal/Code/topsort-incrementality/venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/pranjal/Code/topsort-incrementality/venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/pranjal/Code/topsort-incrementality/venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/pranjal/Code/topsort-incrementality/venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/pranjal/Code/topsort-incrementality/venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/pranjal/Code/topsort-incrementality/venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/pranjal/Code/topsort-incrementality/venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/pranjal/Code/topsort-incrementality/venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/pranjal/Code/topsort-incrementality/venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/pranjal/Code/topsort-incrementality/venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/pranjal/Code/topsort-incrementality/venv/lib/python3.13/site-packages/doubleml/utils/_checks.py:194: UserWarning: Propensity predictions from learner LGBMClassifier(n_jobs=-1, random_state=42, verbose=-1) for ml_m are close to zero or one (eps=1e-12).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -> DoubleML IRM model fitting complete.\n",
      "\n",
      "--- Final Analysis: Interpreting Granular Heterogeneous Treatment Effects ---\n",
      "\n",
      "================================================================================\n",
      "Overall Average Treatment Effect (ATE) on Period 2 Revenue\n",
      "================================================================================\n",
      "  - Baseline (Control Group Avg. Revenue): $84.32\n",
      "  - Absolute Incremental Lift:             $33.72 per user\n",
      "  - 95% Confidence Interval (Absolute):  ($30.00, $37.44)\n",
      "  - Relative Incremental Lift:             +39.99%\n",
      "  - 95% Confidence Interval (Relative):  (+35.58%, +44.41%)\n",
      "\n",
      "================================================================================\n",
      "Heterogeneous Effects by: Revenue (P1)\n",
      "================================================================================\n",
      "   -> Creating 5 quantiles for 'Revenue (P1)'...\n",
      "+------------+----------------+----------------+--------------------+--------------------+\n",
      "| Subgroup   | ATE ($ Lift)   | ATE (% Lift)   | 95% CI (% Lift)    | Baseline Revenue   |\n",
      "+============+================+================+====================+====================+\n",
      "| Q_0        | $28.29         | +56.92%        | (+51.85%, +61.99%) | $49.70             |\n",
      "+------------+----------------+----------------+--------------------+--------------------+\n",
      "| Q_1        | $23.77         | +35.08%        | (+26.94%, +43.22%) | $67.75             |\n",
      "+------------+----------------+----------------+--------------------+--------------------+\n",
      "| Q_2        | $18.72         | +19.51%        | (+13.92%, +25.10%) | $95.95             |\n",
      "+------------+----------------+----------------+--------------------+--------------------+\n",
      "| Q_3        | $23.25         | +11.05%        | (+1.23%, +20.87%)  | $210.44            |\n",
      "+------------+----------------+----------------+--------------------+--------------------+\n",
      "| Q_4        | $102.49        | +23.69%        | (+17.62%, +29.76%) | $432.60            |\n",
      "+------------+----------------+----------------+--------------------+--------------------+\n",
      "\n",
      "================================================================================\n",
      "Heterogeneous Effects by: Purchases (P1)\n",
      "================================================================================\n",
      "   -> Creating 7 quantiles for 'Purchases (P1)'...\n",
      "+------------+----------------+----------------+--------------------+--------------------+\n",
      "| Subgroup   | ATE ($ Lift)   | ATE (% Lift)   | 95% CI (% Lift)    | Baseline Revenue   |\n",
      "+============+================+================+====================+====================+\n",
      "| Q_0        | $21.29         | +63.06%        | (+57.41%, +68.70%) | $33.76             |\n",
      "+------------+----------------+----------------+--------------------+--------------------+\n",
      "| Q_1        | $20.37         | +35.73%        | (+29.69%, +41.78%) | $57.00             |\n",
      "+------------+----------------+----------------+--------------------+--------------------+\n",
      "| Q_2        | $15.26         | +11.21%        | (-2.62%, +25.04%)  | $136.19            |\n",
      "+------------+----------------+----------------+--------------------+--------------------+\n",
      "| Q_3        | $3.92          | +4.11%         | (-1.47%, +9.69%)   | $95.35             |\n",
      "+------------+----------------+----------------+--------------------+--------------------+\n",
      "| Q_4        | $23.63         | +18.93%        | (+15.08%, +22.78%) | $124.86            |\n",
      "+------------+----------------+----------------+--------------------+--------------------+\n",
      "| Q_5        | $0.89          | +0.27%         | (-6.46%, +7.00%)   | $330.00            |\n",
      "+------------+----------------+----------------+--------------------+--------------------+\n",
      "| Q_6        | $197.94        | +36.05%        | (+31.28%, +40.82%) | $549.09            |\n",
      "+------------+----------------+----------------+--------------------+--------------------+\n",
      "\n",
      "================================================================================\n",
      "Heterogeneous Effects by: Clicks (P1)\n",
      "================================================================================\n",
      "   -> Creating 10 quantiles for 'Clicks (P1)'...\n",
      "+------------+----------------+----------------+----------------------+--------------------+\n",
      "| Subgroup   | ATE ($ Lift)   | ATE (% Lift)   | 95% CI (% Lift)      | Baseline Revenue   |\n",
      "+============+================+================+======================+====================+\n",
      "| Q_0        | $14.49         | +17.30%        | (-3.99%, +38.60%)    | $83.75             |\n",
      "+------------+----------------+----------------+----------------------+--------------------+\n",
      "| Q_1        | $27.00         | +120.09%       | (+101.62%, +138.56%) | $22.48             |\n",
      "+------------+----------------+----------------+----------------------+--------------------+\n",
      "| Q_2        | $24.42         | +43.62%        | (+32.29%, +54.95%)   | $55.99             |\n",
      "+------------+----------------+----------------+----------------------+--------------------+\n",
      "| Q_3        | $26.79         | +40.12%        | (+34.51%, +45.74%)   | $66.77             |\n",
      "+------------+----------------+----------------+----------------------+--------------------+\n",
      "| Q_4        | $27.42         | +36.67%        | (+31.93%, +41.41%)   | $74.77             |\n",
      "+------------+----------------+----------------+----------------------+--------------------+\n",
      "| Q_5        | $41.71         | +108.02%       | (+98.32%, +117.73%)  | $38.62             |\n",
      "+------------+----------------+----------------+----------------------+--------------------+\n",
      "| Q_6        | $33.48         | +38.65%        | (+31.99%, +45.31%)   | $86.61             |\n",
      "+------------+----------------+----------------+----------------------+--------------------+\n",
      "| Q_7        | $26.18         | +17.53%        | (+10.57%, +24.48%)   | $149.34            |\n",
      "+------------+----------------+----------------+----------------------+--------------------+\n",
      "| Q_8        | $46.19         | +40.36%        | (+29.41%, +51.32%)   | $114.44            |\n",
      "+------------+----------------+----------------+----------------------+--------------------+\n",
      "| Q_9        | $74.93         | +25.12%        | (+18.39%, +31.84%)   | $298.32            |\n",
      "+------------+----------------+----------------+----------------------+--------------------+\n",
      "\n",
      "================================================================================\n",
      "Heterogeneous Effects by: Tenure (Join Week P1)\n",
      "================================================================================\n",
      "   -> Creating 6 quantiles for 'Tenure (Join Week P1)'...\n",
      "+------------+----------------+----------------+--------------------+--------------------+\n",
      "| Subgroup   | ATE ($ Lift)   | ATE (% Lift)   | 95% CI (% Lift)    | Baseline Revenue   |\n",
      "+============+================+================+====================+====================+\n",
      "| Q_0        | $52.40         | +54.70%        | (+49.66%, +59.73%) | $95.81             |\n",
      "+------------+----------------+----------------+--------------------+--------------------+\n",
      "| Q_1        | $26.91         | +37.22%        | (+26.23%, +48.21%) | $72.30             |\n",
      "+------------+----------------+----------------+--------------------+--------------------+\n",
      "| Q_2        | $22.20         | +35.73%        | (+20.40%, +51.07%) | $62.14             |\n",
      "+------------+----------------+----------------+--------------------+--------------------+\n",
      "| Q_3        | $15.67         | +29.80%        | (+20.78%, +38.83%) | $52.59             |\n",
      "+------------+----------------+----------------+--------------------+--------------------+\n",
      "| Q_4        | $18.35         | +34.09%        | (+20.14%, +48.04%) | $53.82             |\n",
      "+------------+----------------+----------------+--------------------+--------------------+\n",
      "| Q_5        | $-4.27         | -3.99%         | (-25.27%, +17.30%) | $107.06            |\n",
      "+------------+----------------+----------------+--------------------+--------------------+\n",
      "\n",
      "================================================================================\n",
      "Heterogeneous Effects by: Vendor Variety (P1)\n",
      "================================================================================\n",
      "   -> Could not create quantiles for 'Vendor Variety (P1)'. Falling back to median split.\n",
      "   -> Could not create quantiles for 'Vendor Variety (P1)'. Falling back to median split.\n",
      "+-----------------------------------+----------------+----------------+--------------------+--------------------+\n",
      "| Subgroup                          | ATE ($ Lift)   | ATE (% Lift)   | 95% CI (% Lift)    | Baseline Revenue   |\n",
      "+===================================+================+================+====================+====================+\n",
      "| High (Vendor Variety (P1) > 0.00) | $35.28         | +21.31%        | (+16.98%, +25.65%) | $165.54            |\n",
      "+-----------------------------------+----------------+----------------+--------------------+--------------------+\n",
      "| Low (Vendor Variety (P1) <= 0.00) | $32.28         | +67.23%        | (+61.65%, +72.80%) | $48.02             |\n",
      "+-----------------------------------+----------------+----------------+--------------------+--------------------+\n",
      "\n",
      "================================================================================\n",
      "Heterogeneous Effects by: Spend Concentration (P1)\n",
      "================================================================================\n",
      "   -> Could not create quantiles for 'Spend Concentration (P1)'. Falling back to median split.\n",
      "   -> Could not create quantiles for 'Spend Concentration (P1)'. Falling back to median split.\n",
      "+----------------------------------------+----------------+----------------+--------------------+--------------------+\n",
      "| Subgroup                               | ATE ($ Lift)   | ATE (% Lift)   | 95% CI (% Lift)    | Baseline Revenue   |\n",
      "+========================================+================+================+====================+====================+\n",
      "| High (Spend Concentration (P1) > 0.00) | $35.28         | +21.31%        | (+16.98%, +25.65%) | $165.54            |\n",
      "+----------------------------------------+----------------+----------------+--------------------+--------------------+\n",
      "| Low (Spend Concentration (P1) <= 0.00) | $32.28         | +67.23%        | (+61.65%, +72.80%) | $48.02             |\n",
      "+----------------------------------------+----------------+----------------+--------------------+--------------------+\n",
      "\n",
      "✅ ANALYSIS COMPLETE.\n"
     ]
    }
   ],
   "source": [
    "!pip install doubleml scikit-learn lightgbm -q\n",
    "\n",
    "import polars as pl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from tabulate import tabulate\n",
    "import doubleml as dml\n",
    "from lightgbm import LGBMRegressor, LGBMClassifier\n",
    "from sklearn.base import clone\n",
    "\n",
    "# --- Configuration ---\n",
    "ANALYSIS_DATA_FILE = \"final_analysis_user_level_dataset.parquet\"\n",
    "\n",
    "# ==============================================================================\n",
    "# PHASE 1 & 2: LOAD AND PREPARE DATA (Unchanged)\n",
    "# ==============================================================================\n",
    "print(\"--- Phase 1 & 2: Loading and Preparing Data ---\")\n",
    "\n",
    "data_pd = pd.read_parquet(ANALYSIS_DATA_FILE)\n",
    "# Ensure all key numeric columns are standard float64\n",
    "float_cols = [col for col in data_pd.columns if col not in ['user_id', 'is_holdout', 'is_treated']]\n",
    "data_pd[float_cols] = data_pd[float_cols].astype('float64')\n",
    "print(f\"✅ Successfully loaded and prepared data with {len(data_pd):,} users.\")\n",
    "\n",
    "# ==============================================================================\n",
    "# PHASE 3: DOUBLEML ATE ESTIMATION (Unchanged)\n",
    "# ==============================================================================\n",
    "print(\"\\n--- Phase 3: Estimating Overall Average Treatment Effect (ATE) ---\")\n",
    "\n",
    "x_cols = ['revenue_p1', 'purchases_p1', 'clicks_p1', 'join_week_index_p1', 'distinct_vendors_p1', 'spend_concentration_p1']\n",
    "dml_data = dml.DoubleMLData(data_pd, y_col='revenue_p2', d_cols='is_treated', x_cols=x_cols)\n",
    "learner_g = LGBMRegressor(n_jobs=-1, random_state=42, verbose=-1)\n",
    "learner_m = LGBMClassifier(n_jobs=-1, random_state=42, verbose=-1)\n",
    "dml_irm_obj = dml.DoubleMLIRM(dml_data, ml_g=clone(learner_g), ml_m=clone(learner_m))\n",
    "dml_irm_obj.fit(store_predictions=True)\n",
    "print(\"   -> DoubleML IRM model fitting complete.\")\n",
    "\n",
    "# ==============================================================================\n",
    "# PHASE 4: GRANULAR HETEROGENEITY ANALYSIS (NEW LOGIC)\n",
    "# ==============================================================================\n",
    "print(\"\\n--- Final Analysis: Interpreting Granular Heterogeneous Treatment Effects ---\")\n",
    "\n",
    "# --- Helper function to create quantile groups robustly ---\n",
    "def create_quantile_groups(data, var, n_quantiles, var_name):\n",
    "    try:\n",
    "        # Use qcut to create bins. duplicates='drop' handles non-unique bin edges.\n",
    "        quantiles = pd.qcut(data[var], q=n_quantiles, labels=False, duplicates='drop')\n",
    "        n_groups = quantiles.nunique()\n",
    "        \n",
    "        # If we get enough groups, use them\n",
    "        if n_groups >= min(n_quantiles, 4):\n",
    "            print(f\"   -> Creating {n_groups} quantiles for '{var_name}'...\")\n",
    "            labels = [f\"Quantile {i+1}\" for i in range(n_groups)]\n",
    "            if n_groups > 2:\n",
    "                labels[0] += \" (Lowest)\"\n",
    "                labels[-1] += \" (Highest)\"\n",
    "            \n",
    "            groups = pd.get_dummies(quantiles, prefix=f\"Q\").rename(\n",
    "                columns={f\"Q_{i}.0\": labels[i] for i in range(n_groups)}\n",
    "            )\n",
    "            return groups\n",
    "    except Exception:\n",
    "        pass # Fallback if qcut fails for any reason\n",
    "    \n",
    "    # Fallback to median split if quantiles are not possible\n",
    "    print(f\"   -> Could not create quantiles for '{var_name}'. Falling back to median split.\")\n",
    "    median_val = data[var].median()\n",
    "    groups = pd.DataFrame({\n",
    "        f'High ({var_name} > {median_val:.2f})': (data[var] > median_val),\n",
    "        f'Low ({var_name} <= {median_val:.2f})': (data[var] <= median_val)\n",
    "    })\n",
    "    return groups\n",
    "\n",
    "# --- Overall ATE Interpretation ---\n",
    "control_group_data = data_pd[data_pd['is_treated'] == 0]\n",
    "baseline_revenue = control_group_data['revenue_p2'].mean()\n",
    "ate_summary = dml_irm_obj.summary\n",
    "ate_abs = ate_summary.loc['is_treated', 'coef']\n",
    "ate_lower = ate_summary.loc['is_treated', '2.5 %']\n",
    "ate_upper = ate_summary.loc['is_treated', '97.5 %']\n",
    "ate_pct = (ate_abs / baseline_revenue) * 100 if baseline_revenue > 0 else float('inf')\n",
    "ate_pct_lower = (ate_lower / baseline_revenue) * 100 if baseline_revenue > 0 else float('inf')\n",
    "ate_pct_upper = (ate_upper / baseline_revenue) * 100 if baseline_revenue > 0 else float('inf')\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Overall Average Treatment Effect (ATE) on Period 2 Revenue\")\n",
    "print(\"=\"*80)\n",
    "print(f\"  - Baseline (Control Group Avg. Revenue): ${baseline_revenue:.2f}\")\n",
    "print(f\"  - Absolute Incremental Lift:             ${ate_abs:.2f} per user\")\n",
    "print(f\"  - 95% Confidence Interval (Absolute):  (${ate_lower:.2f}, ${ate_upper:.2f})\")\n",
    "print(f\"  - Relative Incremental Lift:             {ate_pct:+.2f}%\")\n",
    "print(f\"  - 95% Confidence Interval (Relative):  ({ate_pct_lower:+.2f}%, {ate_pct_upper:+.2f}%)\")\n",
    "\n",
    "# --- Comprehensive Heterogeneity Analysis Loop ---\n",
    "control_vars = {\n",
    "    'revenue_p1': 'Revenue (P1)',\n",
    "    'purchases_p1': 'Purchases (P1)',\n",
    "    'clicks_p1': 'Clicks (P1)',\n",
    "    'join_week_index_p1': 'Tenure (Join Week P1)',\n",
    "    'distinct_vendors_p1': 'Vendor Variety (P1)',\n",
    "    'spend_concentration_p1': 'Spend Concentration (P1)'\n",
    "}\n",
    "\n",
    "for var, name in control_vars.items():\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"Heterogeneous Effects by: {name}\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Use our robust helper function to create groups, trying for deciles first\n",
    "    groups = create_quantile_groups(data_pd, var, 10, name)\n",
    "    if len(groups.columns) < 4: # Fallback to quartiles if deciles failed\n",
    "        groups = create_quantile_groups(data_pd, var, 4, name)\n",
    "\n",
    "    gate_results = dml_irm_obj.gate(groups)\n",
    "    gate_summary_df = gate_results.summary\n",
    "\n",
    "    interpreted_gates = []\n",
    "    for group_name in sorted(gate_summary_df.index): # Sort to ensure quantiles are in order\n",
    "        gate_abs = gate_summary_df.loc[group_name, 'coef']\n",
    "        gate_lower = gate_summary_df.loc[group_name, '[0.025']\n",
    "        gate_upper = gate_summary_df.loc[group_name, '0.975]']\n",
    "        \n",
    "        group_mask = groups[group_name]\n",
    "        control_in_gate = data_pd[(data_pd['is_treated'] == 0) & (group_mask)]\n",
    "        gate_baseline = control_in_gate['revenue_p2'].mean()\n",
    "        \n",
    "        if gate_baseline > 0:\n",
    "            gate_pct = (gate_abs / gate_baseline) * 100\n",
    "            gate_pct_lower = (gate_lower / gate_baseline) * 100\n",
    "            gate_pct_upper = (gate_upper / gate_baseline) * 100\n",
    "        else:\n",
    "            gate_pct, gate_pct_lower, gate_pct_upper = np.nan, np.nan, np.nan\n",
    "        \n",
    "        interpreted_gates.append({\n",
    "            'Subgroup': group_name,\n",
    "            'ATE ($ Lift)': f\"${gate_abs:.2f}\",\n",
    "            'ATE (% Lift)': f\"{gate_pct:+.2f}%\",\n",
    "            '95% CI (% Lift)': f\"({gate_pct_lower:+.2f}%, {gate_pct_upper:+.2f}%)\",\n",
    "            'Baseline Revenue': f\"${gate_baseline:.2f}\"\n",
    "        })\n",
    "    \n",
    "    gates_df = pd.DataFrame(interpreted_gates)\n",
    "    print(tabulate(gates_df, headers='keys', tablefmt='grid', showindex=False))\n",
    "\n",
    "print(\"\\n✅ ANALYSIS COMPLETE.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e9529cfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Phase 1: Loading Data for Cleaning ---\n",
      "✅ Successfully loaded original holdout list and the main user panel.\n",
      "-> Standardizing column names to lowercase for consistency...\n",
      "   ✅ Column names standardized.\n",
      "\n",
      "--- Phase 2: Identifying Holdouts with Click Activity ---\n",
      "   -> Original Holdout Count: 784,133\n",
      "   -> Found 3,164 holdout users with at least one click.\n",
      "\n",
      "--- Phase 3: Creating the Cleaned Holdout List ---\n",
      "   -> Removed 3,164 users.\n",
      "   -> New, Cleaned Holdout Count: 780,969\n",
      "\n",
      "--- Phase 4: Saving Cleaned List and Generating Report ---\n",
      "✅ Cleaned holdout user list successfully saved to 'final_holdout_user_ids_CLEANED.parquet'\n",
      "✅ A summary report has been saved to 'holdout_cleaning_summary_report.txt'\n"
     ]
    }
   ],
   "source": [
    "import polars as pl\n",
    "import os\n",
    "from tabulate import tabulate\n",
    "\n",
    "# --- 1. Configuration ---\n",
    "ORIGINAL_HOLDOUT_FILE = \"final_holdout_user_ids_final.parquet\"\n",
    "USER_PANEL_FILE = \"user_panel_with_holdout_flag.parquet\"\n",
    "CLEANED_HOLDOUT_FILE = \"final_holdout_user_ids_CLEANED.parquet\"\n",
    "REPORT_FILE = \"holdout_cleaning_summary_report.txt\"\n",
    "\n",
    "# ==============================================================================\n",
    "# PHASE 1: LOAD DATA\n",
    "# ==============================================================================\n",
    "print(\"--- Phase 1: Loading Data for Cleaning ---\")\n",
    "try:\n",
    "    df_holdouts_original = pl.read_parquet(ORIGINAL_HOLDOUT_FILE)\n",
    "    df_panel = pl.read_parquet(USER_PANEL_FILE)\n",
    "    print(\"✅ Successfully loaded original holdout list and the main user panel.\")\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"❌ FATAL ERROR: A required input file was not found. Details: {e}\")\n",
    "    df_holdouts_original = None\n",
    "\n",
    "if df_holdouts_original is not None:\n",
    "    # --- FIX: Standardize column names immediately after loading ---\n",
    "    print(\"-> Standardizing column names to lowercase for consistency...\")\n",
    "    df_holdouts_original.columns = [col.lower() for col in df_holdouts_original.columns]\n",
    "    df_panel.columns = [col.lower() for col in df_panel.columns]\n",
    "    print(\"   ✅ Column names standardized.\")\n",
    "    \n",
    "    # ==============================================================================\n",
    "    # PHASE 2: IDENTIFY AND SIZE THE \"LEAKED\" HOLDOUTS\n",
    "    # ==============================================================================\n",
    "    print(\"\\n--- Phase 2: Identifying Holdouts with Click Activity ---\")\n",
    "\n",
    "    df_all_clickers = df_panel.filter(pl.col(\"clicks\") > 0).select(\"user_id\").unique()\n",
    "    \n",
    "    # Now the join will work correctly.\n",
    "    df_leaked_holdouts = df_holdouts_original.join(df_all_clickers, on=\"user_id\", how=\"inner\")\n",
    "    \n",
    "    original_holdout_count = df_holdouts_original.height\n",
    "    leaked_holdout_count = df_leaked_holdouts.height\n",
    "    \n",
    "    print(f\"   -> Original Holdout Count: {original_holdout_count:,}\")\n",
    "    print(f\"   -> Found {leaked_holdout_count:,} holdout users with at least one click.\")\n",
    "    \n",
    "    # ==============================================================================\n",
    "    # PHASE 3: REMOVE LEAKED USERS AND CREATE THE CLEANED LIST\n",
    "    # ==============================================================================\n",
    "    print(\"\\n--- Phase 3: Creating the Cleaned Holdout List ---\")\n",
    "\n",
    "    df_holdouts_cleaned = df_holdouts_original.join(df_leaked_holdouts, on=\"user_id\", how=\"anti\")\n",
    "    \n",
    "    cleaned_holdout_count = df_holdouts_cleaned.height\n",
    "    \n",
    "    print(f\"   -> Removed {leaked_holdout_count:,} users.\")\n",
    "    print(f\"   -> New, Cleaned Holdout Count: {cleaned_holdout_count:,}\")\n",
    "\n",
    "    # ==============================================================================\n",
    "    # PHASE 4: SAVE THE CLEANED LIST AND GENERATE REPORT\n",
    "    # ==============================================================================\n",
    "    print(f\"\\n--- Phase 4: Saving Cleaned List and Generating Report ---\")\n",
    "    \n",
    "    df_holdouts_cleaned.write_parquet(CLEANED_HOLDOUT_FILE)\n",
    "    print(f\"✅ Cleaned holdout user list successfully saved to '{CLEANED_HOLDOUT_FILE}'\")\n",
    "\n",
    "    with open(REPORT_FILE, \"w\") as f:\n",
    "        f.write(\"Holdout Group Cleaning Summary\\n\")\n",
    "        f.write(\"=\" * 30 + \"\\n\\n\")\n",
    "        f.write(\"Methodology:\\n\")\n",
    "        f.write(\"The original holdout list was checked against all users with click activity.\\n\")\n",
    "        f.write(\"Any holdout user found to have at least one click was removed to create a\\n\")\n",
    "        f.write(\"new, clean, and definitive holdout list for analysis.\\n\\n\")\n",
    "        \n",
    "        f.write(\"Summary of Cleaning Process\\n\")\n",
    "        f.write(\"---------------------------\\n\")\n",
    "        f.write(f\"- Original Holdout Count: {original_holdout_count:,}\\n\")\n",
    "        f.write(f\"- Users Removed (with clicks): {leaked_holdout_count:,}\\n\")\n",
    "        f.write(f\"- Final Cleaned Holdout Count: {cleaned_holdout_count:,}\\n\\n\")\n",
    "        f.write(f\"The cleaned list has been saved to:\\n'{CLEANED_HOLDOUT_FILE}'\\n\")\n",
    "\n",
    "    print(f\"✅ A summary report has been saved to '{REPORT_FILE}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c39e29ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Phase 1: Engineering All Features and Outcomes ---\n",
      "   -> Identified 1,119,128 users for the analysis base.\n",
      "   -> Engineered all Period 1 control variables.\n",
      "   -> Engineered all Period 2 outcome variables.\n",
      "   -> Final DataFrame constructed with 1,119,128 users.\n",
      "\n",
      "--- Phase 2: Running DoubleML Causal Analysis for Multiple Outcomes ---\n",
      "\n",
      "--- Analyzing Outcome: Revenue ($) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pranjal/Code/topsort-incrementality/venv/lib/python3.13/site-packages/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/pranjal/Code/topsort-incrementality/venv/lib/python3.13/site-packages/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/pranjal/Code/topsort-incrementality/venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/pranjal/Code/topsort-incrementality/venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/pranjal/Code/topsort-incrementality/venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/pranjal/Code/topsort-incrementality/venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/pranjal/Code/topsort-incrementality/venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/pranjal/Code/topsort-incrementality/venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/pranjal/Code/topsort-incrementality/venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/pranjal/Code/topsort-incrementality/venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/pranjal/Code/topsort-incrementality/venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/pranjal/Code/topsort-incrementality/venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 889654, number of negative: 5648\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001796 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1082\n",
      "[LightGBM] [Info] Number of data points in the train set: 895302, number of used features: 6\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.993692 -> initscore=5.059531\n",
      "[LightGBM] [Info] Start training from score 5.059531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pranjal/Code/topsort-incrementality/venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 889654, number of negative: 5648\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001945 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1082\n",
      "[LightGBM] [Info] Number of data points in the train set: 895302, number of used features: 6\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.993692 -> initscore=5.059531\n",
      "[LightGBM] [Info] Start training from score 5.059531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pranjal/Code/topsort-incrementality/venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 889654, number of negative: 5648\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001731 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1082\n",
      "[LightGBM] [Info] Number of data points in the train set: 895302, number of used features: 6\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.993692 -> initscore=5.059531\n",
      "[LightGBM] [Info] Start training from score 5.059531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pranjal/Code/topsort-incrementality/venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 889655, number of negative: 5648\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001704 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1079\n",
      "[LightGBM] [Info] Number of data points in the train set: 895303, number of used features: 6\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.993692 -> initscore=5.059532\n",
      "[LightGBM] [Info] Start training from score 5.059532\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pranjal/Code/topsort-incrementality/venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 889655, number of negative: 5648\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001653 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1079\n",
      "[LightGBM] [Info] Number of data points in the train set: 895303, number of used features: 6\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.993692 -> initscore=5.059532\n",
      "[LightGBM] [Info] Start training from score 5.059532\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pranjal/Code/topsort-incrementality/venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -> ATE for Revenue ($): +39.13 (+46.48%)\n",
      "\n",
      "--- Analyzing Outcome: Purchases (#) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pranjal/Code/topsort-incrementality/venv/lib/python3.13/site-packages/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/pranjal/Code/topsort-incrementality/venv/lib/python3.13/site-packages/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/pranjal/Code/topsort-incrementality/venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/pranjal/Code/topsort-incrementality/venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/pranjal/Code/topsort-incrementality/venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/pranjal/Code/topsort-incrementality/venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/pranjal/Code/topsort-incrementality/venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/pranjal/Code/topsort-incrementality/venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/pranjal/Code/topsort-incrementality/venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/pranjal/Code/topsort-incrementality/venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/pranjal/Code/topsort-incrementality/venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/pranjal/Code/topsort-incrementality/venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 889654, number of negative: 5648\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001719 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1085\n",
      "[LightGBM] [Info] Number of data points in the train set: 895302, number of used features: 6\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.993692 -> initscore=5.059531\n",
      "[LightGBM] [Info] Start training from score 5.059531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pranjal/Code/topsort-incrementality/venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 889654, number of negative: 5648\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001913 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1083\n",
      "[LightGBM] [Info] Number of data points in the train set: 895302, number of used features: 6\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.993692 -> initscore=5.059531\n",
      "[LightGBM] [Info] Start training from score 5.059531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pranjal/Code/topsort-incrementality/venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 889654, number of negative: 5648\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002012 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1085\n",
      "[LightGBM] [Info] Number of data points in the train set: 895302, number of used features: 6\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.993692 -> initscore=5.059531\n",
      "[LightGBM] [Info] Start training from score 5.059531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pranjal/Code/topsort-incrementality/venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 889655, number of negative: 5648\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.010807 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1081\n",
      "[LightGBM] [Info] Number of data points in the train set: 895303, number of used features: 6\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.993692 -> initscore=5.059532\n",
      "[LightGBM] [Info] Start training from score 5.059532\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pranjal/Code/topsort-incrementality/venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 889655, number of negative: 5648\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.030564 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1080\n",
      "[LightGBM] [Info] Number of data points in the train set: 895303, number of used features: 6\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.993692 -> initscore=5.059532\n",
      "[LightGBM] [Info] Start training from score 5.059532\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pranjal/Code/topsort-incrementality/venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -> ATE for Purchases (#): +0.75 (+39.16%)\n",
      "\n",
      "--- Analyzing Outcome: Vendor Variety (#) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pranjal/Code/topsort-incrementality/venv/lib/python3.13/site-packages/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/pranjal/Code/topsort-incrementality/venv/lib/python3.13/site-packages/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/pranjal/Code/topsort-incrementality/venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/pranjal/Code/topsort-incrementality/venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/pranjal/Code/topsort-incrementality/venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/pranjal/Code/topsort-incrementality/venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/pranjal/Code/topsort-incrementality/venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/pranjal/Code/topsort-incrementality/venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/pranjal/Code/topsort-incrementality/venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/pranjal/Code/topsort-incrementality/venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/pranjal/Code/topsort-incrementality/venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/pranjal/Code/topsort-incrementality/venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 889654, number of negative: 5648\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002097 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1084\n",
      "[LightGBM] [Info] Number of data points in the train set: 895302, number of used features: 6\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.993692 -> initscore=5.059531\n",
      "[LightGBM] [Info] Start training from score 5.059531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pranjal/Code/topsort-incrementality/venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 889654, number of negative: 5648\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001688 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1081\n",
      "[LightGBM] [Info] Number of data points in the train set: 895302, number of used features: 6\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.993692 -> initscore=5.059531\n",
      "[LightGBM] [Info] Start training from score 5.059531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pranjal/Code/topsort-incrementality/venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 889654, number of negative: 5648\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001739 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1081\n",
      "[LightGBM] [Info] Number of data points in the train set: 895302, number of used features: 6\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.993692 -> initscore=5.059531\n",
      "[LightGBM] [Info] Start training from score 5.059531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pranjal/Code/topsort-incrementality/venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 889655, number of negative: 5648\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001739 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1079\n",
      "[LightGBM] [Info] Number of data points in the train set: 895303, number of used features: 6\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.993692 -> initscore=5.059532\n",
      "[LightGBM] [Info] Start training from score 5.059532\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pranjal/Code/topsort-incrementality/venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 889655, number of negative: 5648\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001761 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1079\n",
      "[LightGBM] [Info] Number of data points in the train set: 895303, number of used features: 6\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.993692 -> initscore=5.059532\n",
      "[LightGBM] [Info] Start training from score 5.059532\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pranjal/Code/topsort-incrementality/venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -> ATE for Vendor Variety (#): +0.12 (+57.70%)\n",
      "\n",
      "--- Analyzing Outcome: Spend Concentration (%) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pranjal/Code/topsort-incrementality/venv/lib/python3.13/site-packages/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/pranjal/Code/topsort-incrementality/venv/lib/python3.13/site-packages/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/pranjal/Code/topsort-incrementality/venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/pranjal/Code/topsort-incrementality/venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/pranjal/Code/topsort-incrementality/venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/pranjal/Code/topsort-incrementality/venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/pranjal/Code/topsort-incrementality/venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/pranjal/Code/topsort-incrementality/venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/pranjal/Code/topsort-incrementality/venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/pranjal/Code/topsort-incrementality/venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/pranjal/Code/topsort-incrementality/venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/pranjal/Code/topsort-incrementality/venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 889654, number of negative: 5648\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001706 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1082\n",
      "[LightGBM] [Info] Number of data points in the train set: 895302, number of used features: 6\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.993692 -> initscore=5.059531\n",
      "[LightGBM] [Info] Start training from score 5.059531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pranjal/Code/topsort-incrementality/venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 889654, number of negative: 5648\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001713 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1080\n",
      "[LightGBM] [Info] Number of data points in the train set: 895302, number of used features: 6\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.993692 -> initscore=5.059531\n",
      "[LightGBM] [Info] Start training from score 5.059531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pranjal/Code/topsort-incrementality/venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 889654, number of negative: 5648\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001616 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1080\n",
      "[LightGBM] [Info] Number of data points in the train set: 895302, number of used features: 6\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.993692 -> initscore=5.059531\n",
      "[LightGBM] [Info] Start training from score 5.059531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pranjal/Code/topsort-incrementality/venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 889655, number of negative: 5648\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.029465 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1081\n",
      "[LightGBM] [Info] Number of data points in the train set: 895303, number of used features: 6\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.993692 -> initscore=5.059532\n",
      "[LightGBM] [Info] Start training from score 5.059532\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pranjal/Code/topsort-incrementality/venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 889655, number of negative: 5648\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001614 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1081\n",
      "[LightGBM] [Info] Number of data points in the train set: 895303, number of used features: 6\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.993692 -> initscore=5.059532\n",
      "[LightGBM] [Info] Start training from score 5.059532\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pranjal/Code/topsort-incrementality/venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -> ATE for Spend Concentration (%): +0.06 (+54.11%)\n",
      "\n",
      "--- Phase 3: Generating Final Report ---\n",
      "\n",
      "✅ ANALYSIS COMPLETE. Final multi-outcome report saved to 'final_multi_outcome_causal_report.txt'\n"
     ]
    }
   ],
   "source": [
    "import polars as pl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from tabulate import tabulate\n",
    "import doubleml as dml\n",
    "from lightgbm import LGBMRegressor, LGBMClassifier\n",
    "from sklearn.base import clone\n",
    "\n",
    "# --- Configuration ---\n",
    "USER_WEEK_PANEL_FILE = \"user_panel_with_holdout_flag.parquet\"\n",
    "USER_VENDOR_PANEL_FILE = \"full_purchaser_vendor_panel_unfiltered.parquet\"\n",
    "CLEANED_HOLDOUT_FILE = \"final_holdout_user_ids_CLEANED.parquet\"\n",
    "FINAL_REPORT_FILE = \"final_multi_outcome_causal_report.txt\"\n",
    "CUTOFF_DATE = \"2025-07-01\"\n",
    "\n",
    "# ==============================================================================\n",
    "# PHASE 1: COMPREHENSIVE FEATURE AND OUTCOME ENGINEERING\n",
    "# ==============================================================================\n",
    "print(\"--- Phase 1: Engineering All Features and Outcomes ---\")\n",
    "\n",
    "# --- Load Data ---\n",
    "df_user_panel = pl.read_parquet(USER_WEEK_PANEL_FILE)\n",
    "df_vendor_panel = pl.read_parquet(USER_VENDOR_PANEL_FILE)\n",
    "df_holdouts_cleaned = pl.read_parquet(CLEANED_HOLDOUT_FILE)\n",
    "\n",
    "# --- Define Time Periods ---\n",
    "cutoff_date_pl = pl.lit(CUTOFF_DATE).str.to_date()\n",
    "df_user_p1 = df_user_panel.filter(pl.col(\"week\") < cutoff_date_pl)\n",
    "df_user_p2 = df_user_panel.filter(pl.col(\"week\") >= cutoff_date_pl)\n",
    "df_vendor_p1 = df_vendor_panel.filter(pl.col(\"week\") < cutoff_date_pl)\n",
    "df_vendor_p2 = df_vendor_panel.filter(pl.col(\"week\") >= cutoff_date_pl)\n",
    "\n",
    "# --- Define Base Population (>= 3 purchases in P1) ---\n",
    "p1_user_purchases = df_user_p1.group_by(\"user_id\").agg(total_purchases_p1=pl.sum(\"purchases\"))\n",
    "base_users = p1_user_purchases.filter(pl.col(\"total_purchases_p1\") >= 3).select(\"user_id\")\n",
    "print(f\"   -> Identified {base_users.height:,} users for the analysis base.\")\n",
    "\n",
    "# --- Engineer Period 1 Controls (X) ---\n",
    "df_controls_user_p1 = df_user_p1.group_by(\"user_id\").agg(clicks_p1=pl.sum(\"clicks\"))\n",
    "df_tenure = df_user_p1.group_by(\"user_id\").agg(first_week_p1=pl.min(\"week\"))\n",
    "period1_start_date = df_user_p1.select(pl.min(\"week\"))[0, 0]\n",
    "df_tenure = df_tenure.with_columns(\n",
    "    join_week_index_p1=((pl.col('first_week_p1') - period1_start_date).dt.total_days() // 7)\n",
    ").select([\"user_id\", \"join_week_index_p1\"])\n",
    "df_controls_vendor_p1_agg1 = df_vendor_p1.group_by([\"user_id\", \"vendor_id\"]).agg(revenue_per_vendor=pl.sum(\"revenue_dollars\"))\n",
    "df_controls_vendor_p1 = df_controls_vendor_p1_agg1.group_by(\"user_id\").agg(\n",
    "    revenue_p1=pl.sum(\"revenue_per_vendor\"),\n",
    "    distinct_vendors_p1=pl.n_unique(\"vendor_id\"),\n",
    "    spend_on_top_vendor_p1=pl.max(\"revenue_per_vendor\")\n",
    ").with_columns(spend_concentration_p1 = pl.col(\"spend_on_top_vendor_p1\") / pl.col(\"revenue_p1\")).drop(\"spend_on_top_vendor_p1\")\n",
    "df_purchases_p1 = p1_user_purchases.select([\"user_id\", pl.col(\"total_purchases_p1\").alias(\"purchases_p1\")])\n",
    "print(\"   -> Engineered all Period 1 control variables.\")\n",
    "\n",
    "# --- Engineer Period 2 Outcomes (Y vector) ---\n",
    "df_outcomes_p2_base = df_user_p2.group_by(\"user_id\").agg(\n",
    "    revenue_p2=pl.sum(\"revenue_dollars\"),\n",
    "    purchases_p2=pl.sum(\"purchases\")\n",
    ")\n",
    "df_outcomes_p2_vendor_agg1 = df_vendor_p2.group_by([\"user_id\", \"vendor_id\"]).agg(revenue_per_vendor=pl.sum(\"revenue_dollars\"))\n",
    "df_outcomes_p2_vendor = df_outcomes_p2_vendor_agg1.group_by(\"user_id\").agg(\n",
    "    p2_total_rev=pl.sum(\"revenue_per_vendor\"), # temp column\n",
    "    distinct_vendors_p2=pl.n_unique(\"vendor_id\"),\n",
    "    p2_spend_on_top=pl.max(\"revenue_per_vendor\")\n",
    ").with_columns(spend_concentration_p2 = pl.col(\"p2_spend_on_top\") / pl.col(\"p2_total_rev\")).select([\"user_id\", \"distinct_vendors_p2\", \"spend_concentration_p2\"])\n",
    "print(\"   -> Engineered all Period 2 outcome variables.\")\n",
    "\n",
    "# --- Construct Final Analysis DataFrame ---\n",
    "holdout_set = set(df_holdouts_cleaned['user_id'])\n",
    "df_final = base_users.with_columns(is_holdout = pl.col(\"user_id\").is_in(holdout_set).cast(pl.UInt8))\n",
    "# Join all features and outcomes\n",
    "df_final = df_final.join(df_purchases_p1, on=\"user_id\", how=\"left\")\n",
    "df_final = df_final.join(df_controls_user_p1, on=\"user_id\", how=\"left\")\n",
    "df_final = df_final.join(df_tenure, on=\"user_id\", how=\"left\")\n",
    "df_final = df_final.join(df_controls_vendor_p1, on=\"user_id\", how=\"left\")\n",
    "df_final = df_final.join(df_outcomes_p2_base, on=\"user_id\", how=\"left\")\n",
    "df_final = df_final.join(df_outcomes_p2_vendor, on=\"user_id\", how=\"left\")\n",
    "df_final = df_final.with_columns(is_treated = 1 - pl.col(\"is_holdout\")).fill_null(0)\n",
    "data_pd = df_final.to_pandas().astype({col: 'float64' for col in df_final.columns if col != 'user_id'})\n",
    "print(f\"   -> Final DataFrame constructed with {len(data_pd):,} users.\")\n",
    "\n",
    "# ==============================================================================\n",
    "# PHASE 2: MULTI-OUTCOME CAUSAL ANALYSIS\n",
    "# ==============================================================================\n",
    "print(\"\\n--- Phase 2: Running DoubleML Causal Analysis for Multiple Outcomes ---\")\n",
    "\n",
    "outcome_vars = {\n",
    "    'revenue_p2': 'Revenue ($)',\n",
    "    'purchases_p2': 'Purchases (#)',\n",
    "    'distinct_vendors_p2': 'Vendor Variety (#)',\n",
    "    'spend_concentration_p2': 'Spend Concentration (%)'\n",
    "}\n",
    "x_cols = ['revenue_p1', 'purchases_p1', 'clicks_p1', 'join_week_index_p1', 'distinct_vendors_p1', 'spend_concentration_p1']\n",
    "all_results = []\n",
    "\n",
    "for y_var, y_name in outcome_vars.items():\n",
    "    print(f\"\\n--- Analyzing Outcome: {y_name} ---\")\n",
    "    \n",
    "    dml_data = dml.DoubleMLData(data_pd, y_col=y_var, d_cols='is_treated', x_cols=x_cols)\n",
    "    learner_g = LGBMRegressor(n_jobs=-1, random_state=42, verbose=-1)\n",
    "    learner_m = LGBMClassifier(n_jobs=-1, random_state=42, verbose=--1)\n",
    "    dml_irm_obj = dml.DoubleMLIRM(dml_data, ml_g=clone(learner_g), ml_m=clone(learner_m))\n",
    "    dml_irm_obj.fit()\n",
    "    \n",
    "    # --- Interpret and store results ---\n",
    "    control_baseline = data_pd[data_pd['is_treated'] == 0][y_var].mean()\n",
    "    ate_summary = dml_irm_obj.summary\n",
    "    ate_abs = ate_summary.loc['is_treated', 'coef']\n",
    "    ate_lower = ate_summary.loc['is_treated', '2.5 %']\n",
    "    ate_upper = ate_summary.loc['is_treated', '97.5 %']\n",
    "    ate_pct = (ate_abs / control_baseline) * 100 if control_baseline != 0 else float('inf')\n",
    "    \n",
    "    all_results.append({\n",
    "        \"Outcome Variable\": y_name,\n",
    "        \"Control Group Baseline\": f\"{control_baseline:.2f}\",\n",
    "        \"Incremental Lift (ATE)\": f\"{ate_abs:+.2f}\",\n",
    "        \"Relative Lift (%)\": f\"{ate_pct:+.2f}%\",\n",
    "        \"95% CI (Absolute)\": f\"({ate_lower:.2f}, {ate_upper:.2f})\"\n",
    "    })\n",
    "    print(f\"   -> ATE for {y_name}: {ate_abs:+.2f} ({ate_pct:+.2f}%)\")\n",
    "\n",
    "# ==============================================================================\n",
    "# PHASE 3: FINAL REPORTING\n",
    "# ==============================================================================\n",
    "print(\"\\n--- Phase 3: Generating Final Report ---\")\n",
    "\n",
    "results_df = pd.DataFrame(all_results)\n",
    "with open(FINAL_REPORT_FILE, \"w\") as f:\n",
    "    f.write(\"Multi-Outcome Causal Analysis of Advertising\\n\")\n",
    "    f.write(\"=\" * 44 + \"\\n\\n\")\n",
    "    f.write(\"Methodology:\\n\")\n",
    "    f.write(\"A DoubleML IRM model was run independently for four different outcome variables from Period 2.\\n\")\n",
    "    f.write(\"Each model controlled for a user's complete behavioral profile from Period 1.\\n\\n\")\n",
    "    f.write(\"Summary of Average Treatment Effects (ATE)\\n\")\n",
    "    f.write(\"----------------------------------------\\n\")\n",
    "    f.write(tabulate(results_df, headers='keys', tablefmt='grid', showindex=False))\n",
    "\n",
    "print(f\"\\n✅ ANALYSIS COMPLETE. Final multi-outcome report saved to '{FINAL_REPORT_FILE}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ecf1abd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
