{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ef0cc36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Connection to Snowflake successful!\n",
      "\n",
      "--- Loading master list from 'final_purchaser_universe.parquet' ---\n",
      "✅ Loaded 4,926,305 total unique purchasing users.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from tabulate import tabulate\n",
    "from dotenv import load_dotenv\n",
    "import snowflake.connector\n",
    "\n",
    "# --- Initialize Connection ---\n",
    "load_dotenv()\n",
    "conn = None\n",
    "try:\n",
    "    conn = snowflake.connector.connect(\n",
    "        user=os.getenv('SNOWFLAKE_USER'),\n",
    "        password=os.getenv('SNOWFLAKE_PASSWORD'),\n",
    "        account=os.getenv('SNOWFLAKE_ACCOUNT'),\n",
    "        warehouse=os.getenv('SNOWFLAKE_WAREHOUSE'),\n",
    "        database='INCREMENTALITY',\n",
    "        schema='INCREMENTALITY_RESEARCH'\n",
    "    )\n",
    "    print(\"✅ Connection to Snowflake successful!\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ ERROR: Could not connect to Snowflake. {e}\", file=sys.stderr)\n",
    "    exit()\n",
    "\n",
    "# --- Load the Master Purchaser Universe ---\n",
    "UNIVERSE_FILE = \"final_purchaser_universe.parquet\"\n",
    "print(f\"\\n--- Loading master list from '{UNIVERSE_FILE}' ---\")\n",
    "try:\n",
    "    df_universe = pd.read_parquet(UNIVERSE_FILE)\n",
    "    print(f\"✅ Loaded {len(df_universe):,} total unique purchasing users.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"❌ FATAL ERROR: The universe file '{UNIVERSE_FILE}' was not found.\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af54512a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Loading Datasets ---\n",
      "-> Loading main panel from '/Users/pranjal/Code/topsort-incrementality/panel/user_panel_full_history.parquet'...\n",
      "   ✅ Loaded panel with 32,060,768 rows.\n",
      "-> Loading final holdout user list from 'final_holdout_user_ids_final.parquet'...\n",
      "   ✅ Loaded 784,133 holdout user IDs.\n",
      "-> Identifying all users with at least one purchase from the panel...\n",
      "   ✅ Identified 4,926,674 unique purchasing users.\n"
     ]
    }
   ],
   "source": [
    "import polars as pl\n",
    "import os\n",
    "from tabulate import tabulate\n",
    "\n",
    "# --- Configuration ---\n",
    "# This is the main 32M row panel dataset\n",
    "PANEL_FILE = \"/Users/pranjal/Code/topsort-incrementality/panel/user_panel_full_history.parquet\"\n",
    "# This is the final output from your long-running script\n",
    "HOLDOUT_FILE = \"final_holdout_user_ids_final.parquet\"\n",
    "# This is the file we will create\n",
    "OUTPUT_FILE = \"user_panel_with_holdout_flag.parquet\"\n",
    "\n",
    "# --- Load the datasets using Polars ---\n",
    "print(\"--- Loading Datasets ---\")\n",
    "try:\n",
    "    print(f\"-> Loading main panel from '{PANEL_FILE}'...\")\n",
    "    df_panel = pl.read_parquet(PANEL_FILE)\n",
    "    print(f\"   ✅ Loaded panel with {df_panel.height:,} rows.\")\n",
    "\n",
    "    print(f\"-> Loading final holdout user list from '{HOLDOUT_FILE}'...\")\n",
    "    df_holdouts = pl.read_parquet(HOLDOUT_FILE)\n",
    "    print(f\"   ✅ Loaded {df_holdouts.height:,} holdout user IDs.\")\n",
    "\n",
    "    # --- NEW: Identify and load the \"purchaser\" list from the panel ---\n",
    "    print(f\"-> Identifying all users with at least one purchase from the panel...\")\n",
    "    df_purchasers = df_panel.filter(pl.col(\"purchases\") > 0).select(pl.col(\"user_id\").unique())\n",
    "    print(f\"   ✅ Identified {df_purchasers.height:,} unique purchasing users.\")\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"\\n❌ FATAL ERROR: A required file was not found. Please ensure the previous scripts have completed.\")\n",
    "    print(f\"   Details: {e}\")\n",
    "    df_panel = None "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2773b69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Enriching Panel with Holdout and Purchaser Flags ---\n",
      "-> Standardizing column names to lowercase...\n",
      "   ✅ Column names standardized.\n",
      "-> Creating 'is_holdout' and 'is_purchaser' columns...\n",
      "   ✅ Flag columns created successfully.\n",
      "\n",
      "--- Verifying the new flags ---\n",
      "Distribution of the 'is_holdout' flag:\n",
      "shape: (2, 2)\n",
      "┌────────────┬──────────┐\n",
      "│ is_holdout ┆ count    │\n",
      "│ ---        ┆ ---      │\n",
      "│ u8         ┆ u32      │\n",
      "╞════════════╪══════════╡\n",
      "│ 0          ┆ 31220194 │\n",
      "│ 1          ┆ 840574   │\n",
      "└────────────┴──────────┘\n",
      "\n",
      "Distribution of the 'is_purchaser' flag:\n",
      "shape: (2, 2)\n",
      "┌──────────────┬──────────┐\n",
      "│ is_purchaser ┆ count    │\n",
      "│ ---          ┆ ---      │\n",
      "│ u8           ┆ u32      │\n",
      "╞══════════════╪══════════╡\n",
      "│ 0            ┆ 7703346  │\n",
      "│ 1            ┆ 24357422 │\n",
      "└──────────────┴──────────┘\n"
     ]
    }
   ],
   "source": [
    "if df_panel is not None:\n",
    "    print(\"\\n--- Enriching Panel with Holdout and Purchaser Flags ---\")\n",
    "    \n",
    "    # --- Standardize column names to prevent errors ---\n",
    "    print(\"-> Standardizing column names to lowercase...\")\n",
    "    df_panel.columns = [col.lower() for col in df_panel.columns]\n",
    "    df_holdouts.columns = [col.lower() for col in df_holdouts.columns]\n",
    "    df_purchasers.columns = [col.lower() for col in df_purchasers.columns]\n",
    "    print(\"   ✅ Column names standardized.\")\n",
    "    \n",
    "    # Use sets for maximum performance\n",
    "    holdout_set = set(df_holdouts['user_id'])\n",
    "    purchaser_set = set(df_purchasers['user_id'])\n",
    "\n",
    "    # --- Create the flag columns using efficient expressions ---\n",
    "    print(\"-> Creating 'is_holdout' and 'is_purchaser' columns...\")\n",
    "    df_enriched = df_panel.with_columns(\n",
    "        is_holdout = pl.col('user_id').is_in(holdout_set).cast(pl.UInt8),\n",
    "        is_purchaser = pl.col('user_id').is_in(purchaser_set).cast(pl.UInt8)\n",
    "    )\n",
    "    print(\"   ✅ Flag columns created successfully.\")\n",
    "\n",
    "    # --- Verification Step ---\n",
    "    print(\"\\n--- Verifying the new flags ---\")\n",
    "    print(\"Distribution of the 'is_holdout' flag:\")\n",
    "    print(df_enriched['is_holdout'].value_counts())\n",
    "    print(\"\\nDistribution of the 'is_purchaser' flag:\")\n",
    "    print(df_enriched['is_purchaser'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df6a4faa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Performing Overall and Temporal EDA on the Enriched Panel ---\n",
      "-> Calculating overall summary stats...\n",
      "   ✅ Overall stats calculated.\n",
      "-> Calculating weekly evolution of metrics...\n",
      "   ✅ Weekly aggregates calculated.\n",
      "-> Calculating weekly fractions...\n",
      "   ✅ Weekly fractions calculated.\n",
      "\n",
      "✅ ANALYSIS COMPLETE. A detailed EDA report has been saved to 'panel_eda_summary_report.txt'\n",
      "\n",
      "--- Evolution of Key Weekly Metrics (First 10 Weeks) ---\n",
      "+---------------------+----------------+--------------------+-----------------+----------------+-------------------+-----------------+\n",
      "| week                |   active_users |   purchasing_users |   holdout_users |   total_clicks |   total_purchases |   total_revenue |\n",
      "+=====================+================+====================+=================+================+===================+=================+\n",
      "| 2025-03-10 00:00:00 |         571746 |             459848 |           14772 |        1537431 |            374695 |      17,165,085 |\n",
      "+---------------------+----------------+--------------------+-----------------+----------------+-------------------+-----------------+\n",
      "| 2025-03-17 00:00:00 |        1022367 |             797258 |           31425 |        3394233 |            819260 |      37,365,288 |\n",
      "+---------------------+----------------+--------------------+-----------------+----------------+-------------------+-----------------+\n",
      "| 2025-03-24 00:00:00 |        1061484 |             828965 |           31082 |        3693883 |            832513 |      38,063,925 |\n",
      "+---------------------+----------------+--------------------+-----------------+----------------+-------------------+-----------------+\n",
      "| 2025-03-31 00:00:00 |        1071531 |             838841 |           30510 |        3785604 |            838217 |      38,439,738 |\n",
      "+---------------------+----------------+--------------------+-----------------+----------------+-------------------+-----------------+\n",
      "| 2025-04-07 00:00:00 |        1114325 |             862786 |           28859 |        4249478 |            806173 |      36,877,335 |\n",
      "+---------------------+----------------+--------------------+-----------------+----------------+-------------------+-----------------+\n",
      "| 2025-04-14 00:00:00 |        1153113 |             885648 |           29205 |        4677307 |            783940 |      35,985,980 |\n",
      "+---------------------+----------------+--------------------+-----------------+----------------+-------------------+-----------------+\n",
      "| 2025-04-21 00:00:00 |        1190005 |             914159 |           29446 |        4930713 |            816260 |      36,984,961 |\n",
      "+---------------------+----------------+--------------------+-----------------+----------------+-------------------+-----------------+\n",
      "| 2025-04-28 00:00:00 |        1222035 |             936676 |           31857 |        5154569 |            847721 |      38,210,185 |\n",
      "+---------------------+----------------+--------------------+-----------------+----------------+-------------------+-----------------+\n",
      "| 2025-05-05 00:00:00 |        1205662 |             923543 |           30947 |        5262750 |            813689 |      36,773,342 |\n",
      "+---------------------+----------------+--------------------+-----------------+----------------+-------------------+-----------------+\n",
      "| 2025-05-12 00:00:00 |        1243056 |             946831 |           30781 |        5830321 |            829285 |      36,649,918 |\n",
      "+---------------------+----------------+--------------------+-----------------+----------------+-------------------+-----------------+\n",
      "\n",
      "--- Evolution of Key Weekly Fractions (First 10 Weeks) ---\n",
      "+---------------------+------------------+----------------+-------------------+\n",
      "| week                |   purchaser_rate |   holdout_rate |   clicks_per_user |\n",
      "+=====================+==================+================+===================+\n",
      "| 2025-03-10 00:00:00 |           0.8043 |         0.0258 |            2.6890 |\n",
      "+---------------------+------------------+----------------+-------------------+\n",
      "| 2025-03-17 00:00:00 |           0.7798 |         0.0307 |            3.3200 |\n",
      "+---------------------+------------------+----------------+-------------------+\n",
      "| 2025-03-24 00:00:00 |           0.7809 |         0.0293 |            3.4799 |\n",
      "+---------------------+------------------+----------------+-------------------+\n",
      "| 2025-03-31 00:00:00 |           0.7828 |         0.0285 |            3.5329 |\n",
      "+---------------------+------------------+----------------+-------------------+\n",
      "| 2025-04-07 00:00:00 |           0.7743 |         0.0259 |            3.8135 |\n",
      "+---------------------+------------------+----------------+-------------------+\n",
      "| 2025-04-14 00:00:00 |           0.7680 |         0.0253 |            4.0562 |\n",
      "+---------------------+------------------+----------------+-------------------+\n",
      "| 2025-04-21 00:00:00 |           0.7682 |         0.0247 |            4.1434 |\n",
      "+---------------------+------------------+----------------+-------------------+\n",
      "| 2025-04-28 00:00:00 |           0.7665 |         0.0261 |            4.2180 |\n",
      "+---------------------+------------------+----------------+-------------------+\n",
      "| 2025-05-05 00:00:00 |           0.7660 |         0.0257 |            4.3650 |\n",
      "+---------------------+------------------+----------------+-------------------+\n",
      "| 2025-05-12 00:00:00 |           0.7617 |         0.0248 |            4.6903 |\n",
      "+---------------------+------------------+----------------+-------------------+\n"
     ]
    }
   ],
   "source": [
    "if 'df_enriched' in locals():\n",
    "    print(\"\\n--- Performing Overall and Temporal EDA on the Enriched Panel ---\")\n",
    "\n",
    "    # --- 1. Calculate Overall Summary Statistics ---\n",
    "    print(\"-> Calculating overall summary stats...\")\n",
    "    summary_stats = {\n",
    "        \"Total Rows (User-Weeks)\": df_enriched.height,\n",
    "        \"Total Unique Users\": df_enriched['user_id'].n_unique(),\n",
    "        \"Total Unique Purchasers\": df_enriched.filter(pl.col('is_purchaser') == 1)['user_id'].n_unique(),\n",
    "        \"Total Unique Holdouts\": df_enriched.filter(pl.col('is_holdout') == 1)['user_id'].n_unique(),\n",
    "        \"Total Clicks\": df_enriched['clicks'].sum(),\n",
    "        \"Total Purchases\": df_enriched['purchases'].sum(),\n",
    "        \"Total Revenue\": df_enriched['revenue_dollars'].sum()\n",
    "    }\n",
    "    print(\"   ✅ Overall stats calculated.\")\n",
    "\n",
    "    # --- 2. Calculate Weekly Evolution of Metrics ---\n",
    "    print(\"-> Calculating weekly evolution of metrics...\")\n",
    "    df_weekly = df_enriched.group_by('week').agg(\n",
    "        active_users=pl.n_unique('user_id'),\n",
    "        purchasing_users=pl.col('user_id').filter(pl.col('is_purchaser') == 1).n_unique(),\n",
    "        holdout_users=pl.col('user_id').filter(pl.col('is_holdout') == 1).n_unique(),\n",
    "        total_clicks=pl.sum('clicks'),\n",
    "        total_purchases=pl.sum('purchases'),\n",
    "        total_revenue=pl.sum('revenue_dollars')\n",
    "    ).sort('week')\n",
    "    print(\"   ✅ Weekly aggregates calculated.\")\n",
    "\n",
    "    # --- 3. Calculate Key Weekly Fractions ---\n",
    "    print(\"-> Calculating weekly fractions...\")\n",
    "    df_weekly_fractions = df_weekly.with_columns(\n",
    "        # What percentage of active users in a week are purchasers?\n",
    "        purchaser_rate = pl.col('purchasing_users') / pl.col('active_users'),\n",
    "        # What percentage of active users in a week are from the holdout group?\n",
    "        holdout_rate = pl.col('holdout_users') / pl.col('active_users'),\n",
    "        # How many clicks per active user?\n",
    "        clicks_per_user = pl.col('total_clicks') / pl.col('active_users')\n",
    "    ).select(['week', 'purchaser_rate', 'holdout_rate', 'clicks_per_user'])\n",
    "    print(\"   ✅ Weekly fractions calculated.\")\n",
    "    \n",
    "    # --- 4. Generate the Final Report ---\n",
    "    report_filename = \"panel_eda_summary_report.txt\"\n",
    "    with open(report_filename, \"w\") as f:\n",
    "        f.write(\"Exploratory Data Analysis of the Enriched User-Week Panel\\n\")\n",
    "        f.write(\"=\" * 58 + \"\\n\\n\")\n",
    "\n",
    "        f.write(\"Overall Dataset Summary\\n\")\n",
    "        f.write(\"-----------------------\\n\")\n",
    "        for key, value in summary_stats.items():\n",
    "            f.write(f\"- {key}: {value:,.0f}\\n\")\n",
    "        f.write(\"\\n\\n\")\n",
    "\n",
    "        f.write(\"Evolution of Key Weekly Metrics\\n\")\n",
    "        f.write(\"-------------------------------\\n\")\n",
    "        f.write(tabulate(df_weekly.to_pandas(), headers='keys', tablefmt='grid', showindex=False, floatfmt=\",.0f\"))\n",
    "        f.write(\"\\n\\n\")\n",
    "\n",
    "        f.write(\"Evolution of Key Weekly Fractions\\n\")\n",
    "        f.write(\"---------------------------------\\n\")\n",
    "        f.write(tabulate(df_weekly_fractions.to_pandas(), headers='keys', tablefmt='grid', showindex=False, floatfmt=\".4f\"))\n",
    "\n",
    "    print(f\"\\n✅ ANALYSIS COMPLETE. A detailed EDA report has been saved to '{report_filename}'\")\n",
    "    \n",
    "    # --- 5. Print a sample of the results to the console ---\n",
    "    print(\"\\n--- Evolution of Key Weekly Metrics (First 10 Weeks) ---\")\n",
    "    print(tabulate(df_weekly.head(10).to_pandas(), headers='keys', tablefmt='grid', showindex=False, floatfmt=\",.0f\"))\n",
    "    \n",
    "    print(\"\\n--- Evolution of Key Weekly Fractions (First 10 Weeks) ---\")\n",
    "    print(tabulate(df_weekly_fractions.head(10).to_pandas(), headers='keys', tablefmt='grid', showindex=False, floatfmt=\".4f\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "353ca7e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Phase 1: Loading Data and Defining Periods ---\n",
      "✅ Successfully loaded panel with 32,060,768 rows.\n",
      "   -> Split data with cutoff: 2025-07-01\n",
      "\n",
      "--- Phase 2: Constructing DataFrame with Controls ---\n",
      "   -> Identified 1,119,128 users for the analysis base.\n",
      "   -> Final analysis DataFrame has 1,119,128 users (rows).\n",
      "\n",
      "--- Phase 3: Performing Covariate Balance Check ---\n",
      "   -> Summary statistics for controls calculated.\n",
      "\n",
      "--- Phase 4: Generating Final Report ---\n",
      "\n",
      "✅ ANALYSIS COMPLETE. Covariate balance report saved to 'covariate_balance_check_report.txt'\n",
      "\n",
      "--- Covariate Balance Check ---\n",
      "+-----------+--------------+------------------+--------------------+-----------------+---------------------+\n",
      "| cohort    |   user_count |   avg_revenue_p1 |   avg_purchases_p1 |   avg_clicks_p1 |   median_revenue_p1 |\n",
      "+===========+==============+==================+====================+=================+=====================+\n",
      "| Treatment |      1111277 |           391.34 |               9.30 |           48.92 |              173.00 |\n",
      "+-----------+--------------+------------------+--------------------+-----------------+---------------------+\n",
      "| Control   |         7851 |           238.40 |               5.91 |            3.38 |              129.00 |\n",
      "+-----------+--------------+------------------+--------------------+-----------------+---------------------+\n"
     ]
    }
   ],
   "source": [
    "import polars as pl\n",
    "from tabulate import tabulate\n",
    "\n",
    "# --- Configuration ---\n",
    "ENRICHED_PANEL_FILE = \"user_panel_with_holdout_flag.parquet\"\n",
    "FINAL_REPORT_FILE = \"covariate_balance_check_report.txt\"\n",
    "CUTOFF_DATE = \"2025-07-01\"\n",
    "\n",
    "# ==============================================================================\n",
    "# PHASE 1: LOAD DATA AND DEFINE TIME PERIODS\n",
    "# ==============================================================================\n",
    "print(\"--- Phase 1: Loading Data and Defining Periods ---\")\n",
    "try:\n",
    "    df_enriched = pl.read_parquet(ENRICHED_PANEL_FILE)\n",
    "    print(f\"✅ Successfully loaded panel with {df_enriched.height:,} rows.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"❌ FATAL ERROR: The enriched panel file '{ENRICHED_PANEL_FILE}' was not found.\")\n",
    "    df_enriched = None\n",
    "\n",
    "if df_enriched is not None:\n",
    "    cutoff_date_pl = pl.lit(CUTOFF_DATE).str.to_date()\n",
    "    df_period1 = df_enriched.filter(pl.col(\"week\") < cutoff_date_pl)\n",
    "    df_period2 = df_enriched.filter(pl.col(\"week\") >= cutoff_date_pl)\n",
    "    print(f\"   -> Split data with cutoff: {CUTOFF_DATE}\")\n",
    "\n",
    "    # ==============================================================================\n",
    "    # PHASE 2: CONSTRUCT THE ANALYSIS DATAFRAME\n",
    "    # ==============================================================================\n",
    "    print(\"\\n--- Phase 2: Constructing DataFrame with Controls ---\")\n",
    "\n",
    "    # Define Base Population (>= 3 purchases in P1)\n",
    "    p1_user_purchases = df_period1.group_by(\"user_id\").agg(total_purchases_p1=pl.sum(\"purchases\"))\n",
    "    base_users = p1_user_purchases.filter(pl.col(\"total_purchases_p1\") >= 3).select(\"user_id\")\n",
    "    print(f\"   -> Identified {base_users.height:,} users for the analysis base.\")\n",
    "\n",
    "    # Engineer Controls (X) from Period 1\n",
    "    df_controls_p1 = df_period1.group_by(\"user_id\").agg(\n",
    "        revenue_p1=pl.sum(\"revenue_dollars\"),\n",
    "        purchases_p1=pl.sum(\"purchases\"),\n",
    "        clicks_p1=pl.sum(\"clicks\")\n",
    "    )\n",
    "    \n",
    "    # Get user holdout status\n",
    "    user_holdout_status = df_enriched.select([\"user_id\", \"is_holdout\"]).unique(subset=\"user_id\")\n",
    "    \n",
    "    # Construct the final DataFrame\n",
    "    df_analysis = base_users.join(user_holdout_status, on=\"user_id\", how=\"inner\")\n",
    "    df_analysis = df_analysis.join(df_controls_p1, on=\"user_id\", how=\"left\").fill_null(0)\n",
    "    print(f\"   -> Final analysis DataFrame has {df_analysis.height:,} users (rows).\")\n",
    "    \n",
    "    # ==============================================================================\n",
    "    # PHASE 3: COVARIATE BALANCE CHECK (INSPECTING THE CONTROLS)\n",
    "    # ==============================================================================\n",
    "    print(\"\\n--- Phase 3: Performing Covariate Balance Check ---\")\n",
    "\n",
    "    # Group by cohort and calculate summary stats for Period 1 controls\n",
    "    control_summary = df_analysis.group_by(\"is_holdout\").agg(\n",
    "        user_count=pl.len(),\n",
    "        avg_revenue_p1=pl.mean(\"revenue_p1\"),\n",
    "        avg_purchases_p1=pl.mean(\"purchases_p1\"),\n",
    "        avg_clicks_p1=pl.mean(\"clicks_p1\"),\n",
    "        median_revenue_p1=pl.median(\"revenue_p1\")\n",
    "    ).with_columns(\n",
    "        cohort = pl.when(pl.col('is_holdout') == 1).then(pl.lit(\"Control\")).otherwise(pl.lit(\"Treatment\"))\n",
    "    ).select(\"cohort\", \"user_count\", \"avg_revenue_p1\", \"avg_purchases_p1\", \"avg_clicks_p1\", \"median_revenue_p1\")\n",
    "    \n",
    "    print(\"   -> Summary statistics for controls calculated.\")\n",
    "    \n",
    "    # ==============================================================================\n",
    "    # PHASE 4: FINAL REPORTING\n",
    "    # ==============================================================================\n",
    "    print(f\"\\n--- Phase 4: Generating Final Report ---\")\n",
    "\n",
    "    with open(FINAL_REPORT_FILE, \"w\") as f:\n",
    "        f.write(\"Covariate Balance Check Report (Comparison of Period 1 Controls)\\n\")\n",
    "        f.write(\"=\" * 64 + \"\\n\\n\")\n",
    "        f.write(\"Methodology:\\n\")\n",
    "        f.write(\"This table compares the average characteristics of the Treatment and Control groups\\n\")\n",
    "        f.write(\"based *only* on their activity in Period 1 (before the outcome period).\\n\")\n",
    "        f.write(\"Large differences here indicate strong selection bias that regression must control for.\\n\\n\")\n",
    "        \n",
    "        f.write(tabulate(control_summary.to_pandas(), headers='keys', tablefmt='grid', showindex=False, floatfmt=\".2f\"))\n",
    "\n",
    "    print(f\"\\n✅ ANALYSIS COMPLETE. Covariate balance report saved to '{FINAL_REPORT_FILE}'\")\n",
    "    \n",
    "    # Print the results to the console\n",
    "    print(\"\\n--- Covariate Balance Check ---\")\n",
    "    print(tabulate(control_summary.to_pandas(), headers='keys', tablefmt='grid', showindex=False, floatfmt=\".2f\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "783263dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Phase 1: Engineering Features, Outcomes, and Tenure Proxy ---\n",
      "   -> Identified 1,119,128 users for the analysis base.\n",
      "   -> Engineered 'join_week_index_p1' as a tenure proxy.\n",
      "\n",
      "--- Phase 2: Constructing Final DataFrame for Regression ---\n",
      "   -> Final DataFrame for analysis has 1,119,128 users (rows).\n",
      "\n",
      "--- Phase 3: Estimating ATE with Tenure Control ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pranjal/Code/topsort-incrementality/venv/lib/python3.13/site-packages/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/pranjal/Code/topsort-incrementality/venv/lib/python3.13/site-packages/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/pranjal/Code/topsort-incrementality/venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/pranjal/Code/topsort-incrementality/venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/pranjal/Code/topsort-incrementality/venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/pranjal/Code/topsort-incrementality/venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/pranjal/Code/topsort-incrementality/venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/pranjal/Code/topsort-incrementality/venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/pranjal/Code/topsort-incrementality/venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/pranjal/Code/topsort-incrementality/venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/pranjal/Code/topsort-incrementality/venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/pranjal/Code/topsort-incrementality/venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/pranjal/Code/topsort-incrementality/venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/pranjal/Code/topsort-incrementality/venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/pranjal/Code/topsort-incrementality/venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/pranjal/Code/topsort-incrementality/venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/pranjal/Code/topsort-incrementality/venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/pranjal/Code/topsort-incrementality/venv/lib/python3.13/site-packages/doubleml/utils/_checks.py:194: UserWarning: Propensity predictions from learner LGBMClassifier(n_jobs=-1, random_state=42, verbose=-1) for ml_m are close to zero or one (eps=1e-12).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -> DoubleML IRM model fitting complete.\n",
      "                coef   std err         t  P>|t|    2.5 %   97.5 %\n",
      "is_treated  7.837105  0.171577  45.67686    0.0  7.50082  8.17339\n",
      "\n",
      "--- Phase 4: Analyzing Heterogeneous Effects with GATEs ---\n",
      "   -> Group Average Treatment Effects (GATEs) calculated.\n",
      "================== DoubleMLBLP Object ==================\n",
      "\n",
      "------------------ Fit summary ------------------\n",
      "                      coef   std err          t         P>|t|    [0.025  \\\n",
      "Early_Joiners_P1  9.797284  0.200115  48.958217  0.000000e+00  9.405065   \n",
      "Late_Joiners_P1   4.873772  0.306875  15.881931  8.453542e-57  4.272307   \n",
      "\n",
      "                     0.975]  \n",
      "Early_Joiners_P1  10.189502  \n",
      "Late_Joiners_P1    5.475236  \n",
      "\n",
      "--- Phase 5: Generating Final Report ---\n",
      "\n",
      "✅ ANALYSIS COMPLETE. Final report saved to 'doubleml_irm_tenure_control_report.txt'\n"
     ]
    }
   ],
   "source": [
    "!pip install doubleml scikit-learn lightgbm -q\n",
    "\n",
    "import polars as pl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from tabulate import tabulate\n",
    "import doubleml as dml\n",
    "from lightgbm import LGBMRegressor, LGBMClassifier\n",
    "from sklearn.base import clone\n",
    "\n",
    "# --- Configuration ---\n",
    "ENRICHED_PANEL_FILE = \"user_panel_with_holdout_flag.parquet\"\n",
    "FINAL_REPORT_FILE = \"doubleml_irm_tenure_control_report.txt\"\n",
    "CUTOFF_DATE = \"2025-07-01\"\n",
    "\n",
    "# ==============================================================================\n",
    "# PHASE 1: FEATURE & OUTCOME ENGINEERING (WITH TENURE PROXY)\n",
    "# ==============================================================================\n",
    "print(\"--- Phase 1: Engineering Features, Outcomes, and Tenure Proxy ---\")\n",
    "\n",
    "df_enriched = pl.read_parquet(ENRICHED_PANEL_FILE)\n",
    "cutoff_date_pl = pl.lit(CUTOFF_DATE).str.to_date()\n",
    "df_period1 = df_enriched.filter(pl.col(\"week\") < cutoff_date_pl)\n",
    "df_period2 = df_enriched.filter(pl.col(\"week\") >= cutoff_date_pl)\n",
    "\n",
    "# Define Base Population (>= 3 purchases in P1)\n",
    "p1_user_purchases = df_period1.group_by(\"user_id\").agg(total_purchases_p1=pl.sum(\"purchases\"))\n",
    "base_users = p1_user_purchases.filter(pl.col(\"total_purchases_p1\") >= 3).select(\"user_id\")\n",
    "print(f\"   -> Identified {base_users.height:,} users for the analysis base.\")\n",
    "\n",
    "# --- Engineer the Tenure Proxy ---\n",
    "df_tenure = df_period1.group_by(\"user_id\").agg(\n",
    "    first_week_p1=pl.min(\"week\")\n",
    ")\n",
    "period1_start_date = df_period1.select(pl.min(\"week\"))[0, 0]\n",
    "\n",
    "# --- THE FIX IS HERE: Use .dt.total_days() instead of .dt.days() ---\n",
    "df_tenure = df_tenure.with_columns(\n",
    "    join_week_index_p1 = ((pl.col('first_week_p1') - period1_start_date).dt.total_days() // 7)\n",
    ").select([\"user_id\", \"join_week_index_p1\"])\n",
    "print(\"   -> Engineered 'join_week_index_p1' as a tenure proxy.\")\n",
    "\n",
    "# Engineer Per-Week Controls (X) from Period 1\n",
    "df_controls_p1 = df_period1.group_by(\"user_id\").agg(\n",
    "    avg_weekly_revenue_p1=pl.mean(\"revenue_dollars\"),\n",
    "    avg_weekly_purchases_p1=pl.mean(\"purchases\"),\n",
    "    avg_weekly_clicks_p1=pl.mean(\"clicks\")\n",
    ")\n",
    "\n",
    "# Engineer Per-Week Outcome (Y) from Period 2\n",
    "df_outcomes_p2 = df_period2.group_by(\"user_id\").agg(\n",
    "    avg_weekly_revenue_p2=pl.mean(\"revenue_dollars\")\n",
    ")\n",
    "\n",
    "# ==============================================================================\n",
    "# PHASE 2: CONSTRUCT FINAL REGRESSION DATAFRAME\n",
    "# ==============================================================================\n",
    "print(\"\\n--- Phase 2: Constructing Final DataFrame for Regression ---\")\n",
    "user_holdout_status = df_enriched.select([\"user_id\", \"is_holdout\"]).unique(subset=\"user_id\")\n",
    "\n",
    "df_analysis = base_users.join(user_holdout_status, on=\"user_id\", how=\"inner\")\n",
    "df_analysis = df_analysis.join(df_controls_p1, on=\"user_id\", how=\"left\").fill_null(0)\n",
    "df_analysis = df_analysis.join(df_outcomes_p2, on=\"user_id\", how=\"left\").fill_null(0)\n",
    "df_analysis = df_analysis.join(df_tenure, on=\"user_id\", how=\"left\").fill_null(0)\n",
    "df_analysis = df_analysis.with_columns(is_treated = 1 - pl.col(\"is_holdout\"))\n",
    "\n",
    "data_pd = df_analysis.to_pandas()\n",
    "float_cols = [col for col in data_pd.columns if 'revenue' in col or 'purchases' in col or 'clicks' in col or 'index' in col]\n",
    "data_pd[float_cols] = data_pd[float_cols].astype('float64')\n",
    "\n",
    "print(f\"   -> Final DataFrame for analysis has {len(data_pd):,} users (rows).\")\n",
    "\n",
    "# ==============================================================================\n",
    "# PHASE 3: DOUBLEML SETUP AND ATE ESTIMATION\n",
    "# ==============================================================================\n",
    "print(\"\\n--- Phase 3: Estimating ATE with Tenure Control ---\")\n",
    "\n",
    "x_cols = ['avg_weekly_revenue_p1', 'avg_weekly_purchases_p1', 'avg_weekly_clicks_p1', 'join_week_index_p1']\n",
    "dml_data = dml.DoubleMLData(data_pd,\n",
    "                              y_col='avg_weekly_revenue_p2',\n",
    "                              d_cols='is_treated',\n",
    "                              x_cols=x_cols)\n",
    "\n",
    "learner_g = LGBMRegressor(n_jobs=-1, random_state=42, verbose=-1)\n",
    "learner_m = LGBMClassifier(n_jobs=-1, random_state=42, verbose=-1)\n",
    "\n",
    "dml_irm_obj = dml.DoubleMLIRM(dml_data,\n",
    "                            ml_g=clone(learner_g),\n",
    "                            ml_m=clone(learner_m))\n",
    "\n",
    "dml_irm_obj.fit(store_predictions=True)\n",
    "print(\"   -> DoubleML IRM model fitting complete.\")\n",
    "print(dml_irm_obj.summary)\n",
    "\n",
    "# ==============================================================================\n",
    "# PHASE 4: HETEROGENEITY ANALYSIS (GATES)\n",
    "# ==============================================================================\n",
    "print(\"\\n--- Phase 4: Analyzing Heterogeneous Effects with GATEs ---\")\n",
    "\n",
    "tenure_median = data_pd['join_week_index_p1'].median()\n",
    "\n",
    "groups = pd.DataFrame({\n",
    "    'Early_Joiners_P1': (data_pd['join_week_index_p1'] <= tenure_median),\n",
    "    'Late_Joiners_P1': (data_pd['join_week_index_p1'] > tenure_median)\n",
    "})\n",
    "\n",
    "gate_results = dml_irm_obj.gate(groups)\n",
    "print(\"   -> Group Average Treatment Effects (GATEs) calculated.\")\n",
    "print(gate_results)\n",
    "\n",
    "# ==============================================================================\n",
    "# PHASE 5: FINAL REPORTING\n",
    "# ==============================================================================\n",
    "print(f\"\\n--- Phase 5: Generating Final Report ---\")\n",
    "\n",
    "with open(FINAL_REPORT_FILE, \"w\") as f:\n",
    "    f.write(\"DoubleML IRM Analysis with Tenure Control\\n\")\n",
    "    f.write(\"=\" * 41 + \"\\n\\n\")\n",
    "    f.write(\"Methodology:\\n\")\n",
    "    f.write(\"A DoubleML IRM model was used to estimate the causal effect of ad exposure on the\\n\")\n",
    "    f.write(\"average weekly revenue in Period 2. The model now includes a 'join_week_index_p1'\\n\")\n",
    "    f.write(\"as a tenure proxy to control for when a user first became active.\\n\\n\")\n",
    "    \n",
    "    f.write(\"Overall Average Treatment Effect (ATE) on Weekly Revenue\\n\")\n",
    "    f.write(\"--------------------------------------------------------\\n\")\n",
    "    f.write(str(dml_irm_obj.summary))\n",
    "    f.write(\"\\n\\n\")\n",
    "    \n",
    "    f.write(\"Heterogeneous Effects by User Tenure (GATEs)\\n\")\n",
    "    f.write(\"--------------------------------------------\\n\")\n",
    "    f.write(\"This table shows if the ATE on weekly revenue is different for users who joined\\n\")\n",
    "    f.write(\"early in Period 1 vs. those who joined later.\\n\\n\")\n",
    "    f.write(str(gate_results))\n",
    "\n",
    "print(f\"\\n✅ ANALYSIS COMPLETE. Final report saved to '{FINAL_REPORT_FILE}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1045f040",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Phase 1: Engineering Features, Outcomes, and Tenure Proxy ---\n",
      "   -> Identified 1,119,128 users for the analysis base.\n",
      "   -> Engineered 'join_week_index_p1' as a tenure proxy.\n",
      "\n",
      "--- Phase 2: Constructing Final DataFrame for Regression ---\n",
      "   -> Final DataFrame for analysis has 1,119,128 users (rows).\n",
      "\n",
      "--- Phase 3: Estimating ATE with Tenure Control ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pranjal/Code/topsort-incrementality/venv/lib/python3.13/site-packages/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/pranjal/Code/topsort-incrementality/venv/lib/python3.13/site-packages/sklearn/utils/deprecation.py:132: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/Users/pranjal/Code/topsort-incrementality/venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/pranjal/Code/topsort-incrementality/venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/pranjal/Code/topsort-incrementality/venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/pranjal/Code/topsort-incrementality/venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/pranjal/Code/topsort-incrementality/venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/pranjal/Code/topsort-incrementality/venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/pranjal/Code/topsort-incrementality/venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/pranjal/Code/topsort-incrementality/venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/pranjal/Code/topsort-incrementality/venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/pranjal/Code/topsort-incrementality/venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/pranjal/Code/topsort-incrementality/venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/pranjal/Code/topsort-incrementality/venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/pranjal/Code/topsort-incrementality/venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/pranjal/Code/topsort-incrementality/venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/pranjal/Code/topsort-incrementality/venv/lib/python3.13/site-packages/sklearn/utils/validation.py:2749: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n",
      "  warnings.warn(\n",
      "/Users/pranjal/Code/topsort-incrementality/venv/lib/python3.13/site-packages/doubleml/utils/_checks.py:194: UserWarning: Propensity predictions from learner LGBMClassifier(n_jobs=-1, random_state=42, verbose=-1) for ml_m are close to zero or one (eps=1e-12).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   -> DoubleML IRM model fitting complete.\n",
      "                coef   std err          t  P>|t|     2.5 %    97.5 %\n",
      "is_treated  7.952556  0.165686  47.997892    0.0  7.627818  8.277294\n",
      "\n",
      "--- Phase 4: Analyzing Heterogeneous Effects by Tenure ---\n",
      "   -> Group Average Treatment Effects (GATEs) calculated.\n",
      "                       coef   std err          t         P>|t|    [0.025  \\\n",
      "Early_Joiners_P1  10.074092  0.207872  48.463047  0.000000e+00  9.666671   \n",
      "Late_Joiners_P1    4.745287  0.272761  17.397251  8.655826e-68  4.210686   \n",
      "\n",
      "                     0.975]  \n",
      "Early_Joiners_P1  10.481513  \n",
      "Late_Joiners_P1    5.279888  \n",
      "\n",
      "--- Phase 5: Generating Final Interpretable Report ---\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'2.5 %'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Code/topsort-incrementality/venv/lib/python3.13/site-packages/pandas/core/indexes/base.py:3812\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3811\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3812\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3813\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:167\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:196\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7096\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyError\u001b[39m: '2.5 %'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 131\u001b[39m\n\u001b[32m    129\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m group_name \u001b[38;5;129;01min\u001b[39;00m gate_summary_df.index:\n\u001b[32m    130\u001b[39m     gate_abs = gate_summary_df.loc[group_name, \u001b[33m'\u001b[39m\u001b[33mcoef\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m--> \u001b[39m\u001b[32m131\u001b[39m     gate_lower = \u001b[43mgate_summary_df\u001b[49m\u001b[43m.\u001b[49m\u001b[43mloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mgroup_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m2.5 \u001b[39;49m\u001b[33;43m%\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m    132\u001b[39m     gate_upper = gate_summary_df.loc[group_name, \u001b[33m'\u001b[39m\u001b[33m97.5 \u001b[39m\u001b[33m%\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m    134\u001b[39m     group_mask = groups[group_name]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Code/topsort-incrementality/venv/lib/python3.13/site-packages/pandas/core/indexing.py:1183\u001b[39m, in \u001b[36m_LocationIndexer.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   1181\u001b[39m     key = \u001b[38;5;28mtuple\u001b[39m(com.apply_if_callable(x, \u001b[38;5;28mself\u001b[39m.obj) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m key)\n\u001b[32m   1182\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._is_scalar_access(key):\n\u001b[32m-> \u001b[39m\u001b[32m1183\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_get_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtakeable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_takeable\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1184\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._getitem_tuple(key)\n\u001b[32m   1185\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1186\u001b[39m     \u001b[38;5;66;03m# we by definition only have the 0th axis\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Code/topsort-incrementality/venv/lib/python3.13/site-packages/pandas/core/frame.py:4219\u001b[39m, in \u001b[36mDataFrame._get_value\u001b[39m\u001b[34m(self, index, col, takeable)\u001b[39m\n\u001b[32m   4216\u001b[39m     series = \u001b[38;5;28mself\u001b[39m._ixs(col, axis=\u001b[32m1\u001b[39m)\n\u001b[32m   4217\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m series._values[index]\n\u001b[32m-> \u001b[39m\u001b[32m4219\u001b[39m series = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_item_cache\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4220\u001b[39m engine = \u001b[38;5;28mself\u001b[39m.index._engine\n\u001b[32m   4222\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m.index, MultiIndex):\n\u001b[32m   4223\u001b[39m     \u001b[38;5;66;03m# CategoricalIndex: Trying to use the engine fastpath may give incorrect\u001b[39;00m\n\u001b[32m   4224\u001b[39m     \u001b[38;5;66;03m#  results if our categories are integers that dont match our codes\u001b[39;00m\n\u001b[32m   4225\u001b[39m     \u001b[38;5;66;03m# IntervalIndex: IntervalTree has no get_loc\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Code/topsort-incrementality/venv/lib/python3.13/site-packages/pandas/core/frame.py:4643\u001b[39m, in \u001b[36mDataFrame._get_item_cache\u001b[39m\u001b[34m(self, item)\u001b[39m\n\u001b[32m   4638\u001b[39m res = cache.get(item)\n\u001b[32m   4639\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m res \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   4640\u001b[39m     \u001b[38;5;66;03m# All places that call _get_item_cache have unique columns,\u001b[39;00m\n\u001b[32m   4641\u001b[39m     \u001b[38;5;66;03m#  pending resolution of GH#33047\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m4643\u001b[39m     loc = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4644\u001b[39m     res = \u001b[38;5;28mself\u001b[39m._ixs(loc, axis=\u001b[32m1\u001b[39m)\n\u001b[32m   4646\u001b[39m     cache[item] = res\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Code/topsort-incrementality/venv/lib/python3.13/site-packages/pandas/core/indexes/base.py:3819\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3814\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   3815\u001b[39m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc.Iterable)\n\u001b[32m   3816\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[32m   3817\u001b[39m     ):\n\u001b[32m   3818\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[32m-> \u001b[39m\u001b[32m3819\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m   3820\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   3821\u001b[39m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[32m   3822\u001b[39m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[32m   3823\u001b[39m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[32m   3824\u001b[39m     \u001b[38;5;28mself\u001b[39m._check_indexing_error(key)\n",
      "\u001b[31mKeyError\u001b[39m: '2.5 %'"
     ]
    }
   ],
   "source": [
    "!pip install doubleml scikit-learn lightgbm -q\n",
    "\n",
    "import polars as pl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from tabulate import tabulate\n",
    "import doubleml as dml\n",
    "from lightgbm import LGBMRegressor, LGBMClassifier\n",
    "from sklearn.base import clone\n",
    "\n",
    "# --- Configuration ---\n",
    "ENRICHED_PANEL_FILE = \"user_panel_with_holdout_flag.parquet\"\n",
    "FINAL_REPORT_FILE = \"final_interpreted_causal_analysis_report.txt\"\n",
    "CUTOFF_DATE = \"2025-07-01\"\n",
    "\n",
    "# ==============================================================================\n",
    "# PHASE 1: FEATURE & OUTCOME ENGINEERING (WITH TENURE PROXY)\n",
    "# ==============================================================================\n",
    "print(\"--- Phase 1: Engineering Features, Outcomes, and Tenure Proxy ---\")\n",
    "\n",
    "df_enriched = pl.read_parquet(ENRICHED_PANEL_FILE)\n",
    "cutoff_date_pl = pl.lit(CUTOFF_DATE).str.to_date()\n",
    "df_period1 = df_enriched.filter(pl.col(\"week\") < cutoff_date_pl)\n",
    "df_period2 = df_enriched.filter(pl.col(\"week\") >= cutoff_date_pl)\n",
    "\n",
    "# Define Base Population (>= 3 purchases in P1)\n",
    "p1_user_purchases = df_period1.group_by(\"user_id\").agg(total_purchases_p1=pl.sum(\"purchases\"))\n",
    "base_users = p1_user_purchases.filter(pl.col(\"total_purchases_p1\") >= 3).select(\"user_id\")\n",
    "print(f\"   -> Identified {base_users.height:,} users for the analysis base.\")\n",
    "\n",
    "# Engineer the Tenure Proxy\n",
    "df_tenure = df_period1.group_by(\"user_id\").agg(first_week_p1=pl.min(\"week\"))\n",
    "period1_start_date = df_period1.select(pl.min(\"week\"))[0, 0]\n",
    "df_tenure = df_tenure.with_columns(\n",
    "    join_week_index_p1 = ((pl.col('first_week_p1') - period1_start_date).dt.total_days() // 7)\n",
    ").select([\"user_id\", \"join_week_index_p1\"])\n",
    "print(\"   -> Engineered 'join_week_index_p1' as a tenure proxy.\")\n",
    "\n",
    "# Engineer Per-Week Controls (X) from Period 1\n",
    "df_controls_p1 = df_period1.group_by(\"user_id\").agg(\n",
    "    avg_weekly_revenue_p1=pl.mean(\"revenue_dollars\"),\n",
    "    avg_weekly_purchases_p1=pl.mean(\"purchases\"),\n",
    "    avg_weekly_clicks_p1=pl.mean(\"clicks\")\n",
    ")\n",
    "\n",
    "# Engineer Per-Week Outcome (Y) from Period 2\n",
    "df_outcomes_p2 = df_period2.group_by(\"user_id\").agg(\n",
    "    avg_weekly_revenue_p2=pl.mean(\"revenue_dollars\")\n",
    ")\n",
    "\n",
    "# ==============================================================================\n",
    "# PHASE 2: CONSTRUCT FINAL REGRESSION DATAFRAME\n",
    "# ==============================================================================\n",
    "print(\"\\n--- Phase 2: Constructing Final DataFrame for Regression ---\")\n",
    "user_holdout_status = df_enriched.select([\"user_id\", \"is_holdout\"]).unique(subset=\"user_id\")\n",
    "\n",
    "df_analysis = base_users.join(user_holdout_status, on=\"user_id\", how=\"inner\")\n",
    "df_analysis = df_analysis.join(df_controls_p1, on=\"user_id\", how=\"left\").fill_null(0)\n",
    "df_analysis = df_analysis.join(df_outcomes_p2, on=\"user_id\", how=\"left\").fill_null(0)\n",
    "df_analysis = df_analysis.join(df_tenure, on=\"user_id\", how=\"left\").fill_null(0)\n",
    "df_analysis = df_analysis.with_columns(is_treated = 1 - pl.col(\"is_holdout\"))\n",
    "\n",
    "data_pd = df_analysis.to_pandas()\n",
    "# Fix for TypeError: Convert all key numeric columns to standard float64\n",
    "float_cols = [col for col in data_pd.columns if 'revenue' in col or 'purchases' in col or 'clicks' in col or 'index' in col]\n",
    "data_pd[float_cols] = data_pd[float_cols].astype('float64')\n",
    "\n",
    "print(f\"   -> Final DataFrame for analysis has {len(data_pd):,} users (rows).\")\n",
    "\n",
    "# ==============================================================================\n",
    "# PHASE 3: DOUBLEML ATE ESTIMATION WITH TENURE CONTROL\n",
    "# ==============================================================================\n",
    "print(\"\\n--- Phase 3: Estimating ATE with Tenure Control ---\")\n",
    "\n",
    "x_cols = ['avg_weekly_revenue_p1', 'avg_weekly_purchases_p1', 'avg_weekly_clicks_p1', 'join_week_index_p1']\n",
    "dml_data = dml.DoubleMLData(data_pd,\n",
    "                              y_col='avg_weekly_revenue_p2',\n",
    "                              d_cols='is_treated',\n",
    "                              x_cols=x_cols)\n",
    "\n",
    "learner_g = LGBMRegressor(n_jobs=-1, random_state=42, verbose=-1)\n",
    "learner_m = LGBMClassifier(n_jobs=-1, random_state=42, verbose=-1)\n",
    "\n",
    "dml_irm_obj = dml.DoubleMLIRM(dml_data,\n",
    "                            ml_g=clone(learner_g),\n",
    "                            ml_m=clone(learner_m))\n",
    "\n",
    "dml_irm_obj.fit(store_predictions=True)\n",
    "print(\"   -> DoubleML IRM model fitting complete.\")\n",
    "print(dml_irm_obj.summary)\n",
    "\n",
    "# ==============================================================================\n",
    "# PHASE 4: HETEROGENEITY ANALYSIS (GATES)\n",
    "# ==============================================================================\n",
    "print(\"\\n--- Phase 4: Analyzing Heterogeneous Effects by Tenure ---\")\n",
    "\n",
    "tenure_median = data_pd['join_week_index_p1'].median()\n",
    "groups = pd.DataFrame({\n",
    "    'Early_Joiners_P1': (data_pd['join_week_index_p1'] <= tenure_median),\n",
    "    'Late_Joiners_P1': (data_pd['join_week_index_p1'] > tenure_median)\n",
    "})\n",
    "\n",
    "gate_results = dml_irm_obj.gate(groups)\n",
    "print(\"   -> Group Average Treatment Effects (GATEs) calculated.\")\n",
    "print(gate_results.summary)\n",
    "\n",
    "# ==============================================================================\n",
    "# PHASE 5: FINAL INTERPRETABLE REPORTING\n",
    "# ==============================================================================\n",
    "print(f\"\\n--- Phase 5: Generating Final Interpretable Report ---\")\n",
    "\n",
    "# Calculate baseline revenue for the control group\n",
    "control_group_data = data_pd[data_pd['is_treated'] == 0]\n",
    "baseline_revenue = control_group_data['avg_weekly_revenue_p2'].mean()\n",
    "\n",
    "# Interpret Overall ATE\n",
    "ate_summary = dml_irm_obj.summary\n",
    "ate_abs = ate_summary.loc['is_treated', 'coef']\n",
    "ate_lower = ate_summary.loc['is_treated', '2.5 %']\n",
    "ate_upper = ate_summary.loc['is_treated', '97.5 %']\n",
    "ate_pct = (ate_abs / baseline_revenue) * 100 if baseline_revenue > 0 else float('inf')\n",
    "ate_pct_lower = (ate_lower / baseline_revenue) * 100 if baseline_revenue > 0 else float('inf')\n",
    "ate_pct_upper = (ate_upper / baseline_revenue) * 100 if baseline_revenue > 0 else float('inf')\n",
    "\n",
    "# Interpret GATEs\n",
    "gate_summary_df = gate_results.summary\n",
    "interpreted_gates = []\n",
    "for group_name in gate_summary_df.index:\n",
    "    gate_abs = gate_summary_df.loc[group_name, 'coef']\n",
    "    gate_lower = gate_summary_df.loc[group_name, '2.5 %']\n",
    "    gate_upper = gate_summary_df.loc[group_name, '97.5 %']\n",
    "    \n",
    "    group_mask = groups[group_name]\n",
    "    control_in_gate = data_pd[(data_pd['is_treated'] == 0) & (group_mask)]\n",
    "    gate_baseline = control_in_gate['avg_weekly_revenue_p2'].mean()\n",
    "    \n",
    "    if gate_baseline > 0:\n",
    "        gate_pct = (gate_abs / gate_baseline) * 100\n",
    "        gate_pct_lower = (gate_lower / gate_baseline) * 100\n",
    "        gate_pct_upper = (gate_upper / gate_baseline) * 100\n",
    "    else:\n",
    "        gate_pct, gate_pct_lower, gate_pct_upper = np.nan, np.nan, np.nan\n",
    "        \n",
    "    interpreted_gates.append({\n",
    "        'Subgroup': group_name,\n",
    "        'ATE ($ Lift)': f\"${gate_abs:.2f}\",\n",
    "        '95% CI Range ($)': f\"(${gate_lower:.2f}, ${gate_upper:.2f})\",\n",
    "        'ATE (% Lift)': f\"{gate_pct:+.2f}%\",\n",
    "        'Baseline Revenue': f\"${gate_baseline:.2f}\"\n",
    "    })\n",
    "gates_df = pd.DataFrame(interpreted_gates)\n",
    "\n",
    "# Write the final report file\n",
    "with open(FINAL_REPORT_FILE, \"w\") as f:\n",
    "    f.write(\"Final Interpreted Report: Causal Lift of Advertising\\n\")\n",
    "    f.write(\"=\" * 51 + \"\\n\\n\")\n",
    "    \n",
    "    f.write(\"Overall Average Treatment Effect (ATE)\\n\")\n",
    "    f.write(\"--------------------------------------\\n\")\n",
    "    f.write(f\"This is the average causal impact of advertising on a core customer's weekly revenue,\\n\")\n",
    "    f.write(f\"after controlling for their prior behavior and tenure.\\n\\n\")\n",
    "    f.write(f\"  - Baseline (Control Group Avg. Weekly Revenue): ${baseline_revenue:.2f}\\n\")\n",
    "    f.write(f\"  - Absolute Incremental Lift:                    ${ate_abs:.2f} per user per week\\n\")\n",
    "    f.write(f\"  - 95% Confidence Interval (Absolute):         (${ate_lower:.2f}, ${ate_upper:.2f})\\n\\n\")\n",
    "    f.write(f\"  - Relative Incremental Lift:                    {ate_pct:+.2f}%\\n\")\n",
    "    f.write(f\"  - 95% Confidence Interval (Relative):         ({ate_pct_lower:+.2f}%, {ate_pct_upper:+.2f}%)\\n\\n\\n\")\n",
    "    \n",
    "    f.write(\"Heterogeneous Effects by User Tenure (GATEs)\\n\")\n",
    "    f.write(\"--------------------------------------------\\n\")\n",
    "    f.write(\"This table shows how the treatment effect varies based on a user's tenure in Period 1.\\n\\n\")\n",
    "    f.write(tabulate(gates_df, headers='keys', tablefmt='grid', showindex=False))\n",
    "\n",
    "print(f\"\\n✅ ANALYSIS COMPLETE. Final, interpreted report saved to '{FINAL_REPORT_FILE}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c45afd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
