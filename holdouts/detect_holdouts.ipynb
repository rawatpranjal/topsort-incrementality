{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d3c1412",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Connection to Snowflake successful!\n",
      "   Snowflake version: 9.27.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "import snowflake.connector\n",
    "\n",
    "# Load environment variables from the .env file\n",
    "# This command looks for the .env file in the same directory as your notebook\n",
    "load_dotenv()\n",
    "\n",
    "# --- Snowflake Connection Block ---\n",
    "conn = None  # Initialize conn to None\n",
    "try:\n",
    "    # Establish the connection using credentials from the .env file\n",
    "    conn = snowflake.connector.connect(\n",
    "        user=os.getenv('SNOWFLAKE_USER'),\n",
    "        password=os.getenv('SNOWFLAKE_PASSWORD'),\n",
    "        account=os.getenv('SNOWFLAKE_ACCOUNT'),\n",
    "        warehouse=os.getenv('SNOWFLAKE_WAREHOUSE'),\n",
    "        database='INCREMENTALITY',\n",
    "        schema='INCREMENTALITY_RESEARCH'\n",
    "    )\n",
    "    print(\"✅ Connection to Snowflake successful!\")\n",
    "\n",
    "    # Optional: Verify the connection with a simple query\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(\"SELECT CURRENT_VERSION()\")\n",
    "    one_row = cursor.fetchone()\n",
    "    print(f\"   Snowflake version: {one_row[0]}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ ERROR: Could not connect to Snowflake.\", file=sys.stderr)\n",
    "    print(f\"   Please check your credentials in the .env file and network connection.\", file=sys.stderr)\n",
    "    print(f\"   Details: {e}\", file=sys.stderr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2105242b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Counting Distinct Impressed Users for 2025-07-08 ---\n",
      "✅ Query successful.\n",
      "✅ Analysis complete. Report saved to 'daily_impressed_users_sample_count.txt'\n",
      "\n",
      "--- Report Content for 'daily_impressed_users_sample_count.txt' ---\n",
      "Daily Impressed User Count (One-Day Sample)\n",
      "===========================================\n",
      "\n",
      "Methodology: A fast approximation query was run on the IMPRESSIONS table\n",
      "             to estimate the number of unique users shown at least one ad\n",
      "             within a single, representative day.\n",
      "\n",
      "Date Analyzed: 2025-07-08\n",
      "-------------------------------------------\n",
      "Approximate Distinct Users Shown Ads: 673,017\n",
      "-------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tabulate import tabulate\n",
    "import pandas as pd\n",
    "\n",
    "# We assume 'conn' is an active Snowflake connection.\n",
    "\n",
    "# --- 1. Define the Query and Day for Analysis ---\n",
    "# We'll analyze a single, representative day. A weekday is often a good choice.\n",
    "TARGET_DATE = '2025-07-08'\n",
    "\n",
    "# This query is designed to be extremely fast and low-cost.\n",
    "# It uses ::DATE to cast the timestamp to a date for a clean filter.\n",
    "one_day_count_query = f\"\"\"\n",
    "SELECT\n",
    "    APPROX_COUNT_DISTINCT(USER_ID) AS approximate_distinct_users\n",
    "FROM\n",
    "    IMPRESSIONS\n",
    "WHERE\n",
    "    OCCURRED_AT::DATE = '{TARGET_DATE}';\n",
    "\"\"\"\n",
    "\n",
    "# --- 2. Execute the Query ---\n",
    "print(f\"--- Counting Distinct Impressed Users for {TARGET_DATE} ---\")\n",
    "try:\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(one_day_count_query)\n",
    "    # Fetch the single result from the first row, first column\n",
    "    result = cursor.fetchone()\n",
    "    approx_count = result[0] if result else 0\n",
    "    print(f\"✅ Query successful.\")\n",
    "\n",
    "    # --- 3. Generate the Report ---\n",
    "    report_filename = \"daily_impressed_users_sample_count.txt\"\n",
    "    with open(report_filename, \"w\") as f:\n",
    "        f.write(\"Daily Impressed User Count (One-Day Sample)\\n\")\n",
    "        f.write(\"=\" * 43 + \"\\n\\n\")\n",
    "        f.write(\"Methodology: A fast approximation query was run on the IMPRESSIONS table\\n\")\n",
    "        f.write(\"             to estimate the number of unique users shown at least one ad\\n\")\n",
    "        f.write(\"             within a single, representative day.\\n\\n\")\n",
    "        \n",
    "        f.write(f\"Date Analyzed: {TARGET_DATE}\\n\")\n",
    "        f.write(\"-\" * 43 + \"\\n\")\n",
    "        f.write(f\"Approximate Distinct Users Shown Ads: {approx_count:,.0f}\\n\")\n",
    "        f.write(\"-\" * 43 + \"\\n\")\n",
    "        \n",
    "    print(f\"✅ Analysis complete. Report saved to '{report_filename}'\")\n",
    "\n",
    "    print(f\"\\n--- Report Content for '{report_filename}' ---\")\n",
    "    with open(report_filename, 'r') as f:\n",
    "        print(f.read())\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ ERROR executing query: {e}\", file=sys.stderr)\n",
    "finally:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "484790a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Counting Distinct Purchasing Users for 2025-07-01 to 2025-08-01 ---\n",
      "✅ Query successful.\n",
      "✅ Analysis complete. Report saved to 'monthly_purchasing_users_count.txt'\n",
      "\n",
      "--- Report Content for 'monthly_purchasing_users_count.txt' ---\n",
      "Monthly Purchasing User Count (One-Month Sample)\n",
      "================================================\n",
      "\n",
      "Methodology: A fast approximation query was run on the PURCHASES table\n",
      "             to estimate the number of unique users who made at least one\n",
      "             purchase within a single, representative month.\n",
      "\n",
      "Time Period Analyzed: 2025-07-01 to 2025-08-01 (exclusive)\n",
      "----------------------------------------------------------\n",
      "Approximate Distinct Purchasing Users: 1,475,138\n",
      "----------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tabulate import tabulate\n",
    "import pandas as pd\n",
    "\n",
    "# We assume 'conn' is an active Snowflake connection.\n",
    "\n",
    "# --- 1. Define the Query and Time Period ---\n",
    "# We'll analyze a single, representative month.\n",
    "START_DATE = '2025-07-01'\n",
    "END_DATE = '2025-08-01'  # The end date is exclusive, capturing all of July.\n",
    "\n",
    "# This query is designed to be very efficient.\n",
    "one_month_purchasers_query = f\"\"\"\n",
    "SELECT\n",
    "    APPROX_COUNT_DISTINCT(USER_ID) AS approximate_distinct_users\n",
    "FROM\n",
    "    PURCHASES\n",
    "WHERE\n",
    "    PURCHASED_AT >= '{START_DATE}' AND PURCHASED_AT < '{END_DATE}';\n",
    "\"\"\"\n",
    "\n",
    "# --- 2. Execute the Query ---\n",
    "print(f\"--- Counting Distinct Purchasing Users for {START_DATE} to {END_DATE} ---\")\n",
    "try:\n",
    "    cursor = conn.cursor()\n",
    "    cursor.execute(one_month_purchasers_query)\n",
    "    # Fetch the single result from the first row, first column\n",
    "    result = cursor.fetchone()\n",
    "    approx_count = result[0] if result else 0\n",
    "    print(f\"✅ Query successful.\")\n",
    "\n",
    "    # --- 3. Generate the Report ---\n",
    "    report_filename = \"monthly_purchasing_users_count.txt\"\n",
    "    with open(report_filename, \"w\") as f:\n",
    "        f.write(\"Monthly Purchasing User Count (One-Month Sample)\\n\")\n",
    "        f.write(\"=\" * 48 + \"\\n\\n\")\n",
    "        f.write(\"Methodology: A fast approximation query was run on the PURCHASES table\\n\")\n",
    "        f.write(\"             to estimate the number of unique users who made at least one\\n\")\n",
    "        f.write(\"             purchase within a single, representative month.\\n\\n\")\n",
    "        \n",
    "        f.write(f\"Time Period Analyzed: {START_DATE} to {END_DATE} (exclusive)\\n\")\n",
    "        f.write(\"-\" * 58 + \"\\n\")\n",
    "        f.write(f\"Approximate Distinct Purchasing Users: {approx_count:,.0f}\\n\")\n",
    "        f.write(\"-\" * 58 + \"\\n\")\n",
    "        \n",
    "    print(f\"✅ Analysis complete. Report saved to '{report_filename}'\")\n",
    "\n",
    "    print(f\"\\n--- Report Content for '{report_filename}' ---\")\n",
    "    with open(report_filename, 'r') as f:\n",
    "        print(f.read())\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ ERROR executing query: {e}\", file=sys.stderr)\n",
    "finally:\n",
    "    if 'cursor' in locals() and cursor:\n",
    "        cursor.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27856ed9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output will be saved to the 'monthly_purchaser_lists' directory.\n",
      "\n",
      "--- Starting Monthly Extraction Process ---\n",
      "\n",
      "Processing Month: 2025-03...\n",
      "Executing query on Snowflake...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/b7/1tvk5qmx0ds9c6gk2lrlhv380000gn/T/ipykernel_18886/175062477.py:14: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(query, connection)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Query successful. Fetched 976,617 rows.\n",
      "   -> Found 976,617 unique users. Saved to 'monthly_purchaser_lists/purchasers_2025-03.parquet'\n",
      "\n",
      "Processing Month: 2025-04...\n",
      "Executing query on Snowflake...\n",
      "✅ Query successful. Fetched 1,399,898 rows.\n",
      "   -> Found 1,399,898 unique users. Saved to 'monthly_purchaser_lists/purchasers_2025-04.parquet'\n",
      "\n",
      "Processing Month: 2025-05...\n",
      "Executing query on Snowflake...\n",
      "✅ Query successful. Fetched 1,451,601 rows.\n",
      "   -> Found 1,451,601 unique users. Saved to 'monthly_purchaser_lists/purchasers_2025-05.parquet'\n",
      "\n",
      "Processing Month: 2025-06...\n",
      "Executing query on Snowflake...\n",
      "✅ Query successful. Fetched 1,396,621 rows.\n",
      "   -> Found 1,396,621 unique users. Saved to 'monthly_purchaser_lists/purchasers_2025-06.parquet'\n",
      "\n",
      "Processing Month: 2025-07...\n",
      "Executing query on Snowflake...\n",
      "✅ Query successful. Fetched 1,436,993 rows.\n",
      "   -> Found 1,436,993 unique users. Saved to 'monthly_purchaser_lists/purchasers_2025-07.parquet'\n",
      "\n",
      "Processing Month: 2025-08...\n",
      "Executing query on Snowflake...\n",
      "✅ Query successful. Fetched 1,561,961 rows.\n",
      "   -> Found 1,561,961 unique users. Saved to 'monthly_purchaser_lists/purchasers_2025-08.parquet'\n",
      "\n",
      "Processing Month: 2025-09...\n",
      "Executing query on Snowflake...\n",
      "✅ Query successful. Fetched 600,029 rows.\n",
      "   -> Found 600,029 unique users. Saved to 'monthly_purchaser_lists/purchasers_2025-09.parquet'\n",
      "\n",
      "\n",
      "--- Extraction Process Complete ---\n",
      "\n",
      "✅ All months processed. A summary report has been saved to 'monthly_purchaser_extraction_summary.txt'\n",
      "\n",
      "--- Summary Report ---\n",
      "+---------+---------------------+----------------------------------------------------+\n",
      "| Month   |   Unique Purchasers | File Path                                          |\n",
      "+=========+=====================+====================================================+\n",
      "| 2025-03 |              976617 | monthly_purchaser_lists/purchasers_2025-03.parquet |\n",
      "+---------+---------------------+----------------------------------------------------+\n",
      "| 2025-04 |             1399898 | monthly_purchaser_lists/purchasers_2025-04.parquet |\n",
      "+---------+---------------------+----------------------------------------------------+\n",
      "| 2025-05 |             1451601 | monthly_purchaser_lists/purchasers_2025-05.parquet |\n",
      "+---------+---------------------+----------------------------------------------------+\n",
      "| 2025-06 |             1396621 | monthly_purchaser_lists/purchasers_2025-06.parquet |\n",
      "+---------+---------------------+----------------------------------------------------+\n",
      "| 2025-07 |             1436993 | monthly_purchaser_lists/purchasers_2025-07.parquet |\n",
      "+---------+---------------------+----------------------------------------------------+\n",
      "| 2025-08 |             1561961 | monthly_purchaser_lists/purchasers_2025-08.parquet |\n",
      "+---------+---------------------+----------------------------------------------------+\n",
      "| 2025-09 |              600029 | monthly_purchaser_lists/purchasers_2025-09.parquet |\n",
      "+---------+---------------------+----------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "from tabulate import tabulate\n",
    "\n",
    "# We assume 'conn' is an active Snowflake connection from a previous cell.\n",
    "\n",
    "# --- Helper Function to Execute Queries ---\n",
    "def run_query(connection, query):\n",
    "    \"\"\"Executes a query and returns the results as a Pandas DataFrame.\"\"\"\n",
    "    print(\"Executing query on Snowflake...\")\n",
    "    try:\n",
    "        # The UserWarning is expected and can be ignored.\n",
    "        df = pd.read_sql(query, connection)\n",
    "        print(f\"✅ Query successful. Fetched {len(df):,} rows.\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"❌ ERROR executing query: {e}\", file=sys.stderr)\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# --- 1. Configuration ---\n",
    "# Define the overall date range for the extraction.\n",
    "START_MONTH = '2025-03-01'\n",
    "END_MONTH = '2025-09-01'\n",
    "\n",
    "# Create a dedicated directory to store the output files.\n",
    "OUTPUT_DIR = \"monthly_purchaser_lists\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "print(f\"Output will be saved to the '{OUTPUT_DIR}' directory.\")\n",
    "\n",
    "# --- 2. Generate Monthly Date Ranges ---\n",
    "# 'MS' frequency gives the start of each month.\n",
    "monthly_start_dates = pd.date_range(start=START_MONTH, end=END_MONTH, freq='MS')\n",
    "\n",
    "# --- 3. Loop Through Each Month and Extract Data ---\n",
    "summary_data = []\n",
    "print(\"\\n--- Starting Monthly Extraction Process ---\")\n",
    "\n",
    "for start_date in monthly_start_dates:\n",
    "    # Calculate the end of the month (which is the start of the next month)\n",
    "    end_date = start_date + pd.DateOffset(months=1)\n",
    "    \n",
    "    # Format dates for the SQL query\n",
    "    start_date_str = start_date.strftime('%Y-%m-%d')\n",
    "    end_date_str = end_date.strftime('%Y-%m-%d')\n",
    "    year_month_str = start_date.strftime('%Y-%m')\n",
    "\n",
    "    print(f\"\\nProcessing Month: {year_month_str}...\")\n",
    "\n",
    "    # Define the query for the current month\n",
    "    monthly_purchasers_query = f\"\"\"\n",
    "    SELECT DISTINCT USER_ID\n",
    "    FROM PURCHASES\n",
    "    WHERE PURCHASED_AT >= '{start_date_str}' AND PURCHASED_AT < '{end_date_str}';\n",
    "    \"\"\"\n",
    "    \n",
    "    # Execute the query\n",
    "    df_monthly_users = run_query(conn, monthly_purchasers_query)\n",
    "\n",
    "    # Save the result to a Parquet file\n",
    "    if not df_monthly_users.empty:\n",
    "        file_path = os.path.join(OUTPUT_DIR, f\"purchasers_{year_month_str}.parquet\")\n",
    "        df_monthly_users.to_parquet(file_path, index=False)\n",
    "        \n",
    "        user_count = len(df_monthly_users)\n",
    "        print(f\"   -> Found {user_count:,.0f} unique users. Saved to '{file_path}'\")\n",
    "        summary_data.append([year_month_str, user_count, file_path])\n",
    "    else:\n",
    "        print(f\"   -> No purchasing users found for this month.\")\n",
    "        summary_data.append([year_month_str, 0, \"N/A\"])\n",
    "\n",
    "# --- 4. Generate a Final Summary Report ---\n",
    "print(\"\\n\\n--- Extraction Process Complete ---\")\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data, columns=['Month', 'Unique Purchasers', 'File Path'])\n",
    "\n",
    "report_filename = \"monthly_purchaser_extraction_summary.txt\"\n",
    "with open(report_filename, \"w\") as f:\n",
    "    f.write(\"Summary of Monthly Purchaser Extractions\\n\")\n",
    "    f.write(\"=\" * 40 + \"\\n\\n\")\n",
    "    f.write(\"Methodology: The PURCHASES table was queried for each month in the specified\\n\")\n",
    "    f.write(\"             range. A list of unique USER_IDs for each month was saved to a\\n\")\n",
    "    f.write(\"             separate Parquet file for further analysis.\\n\\n\")\n",
    "    \n",
    "    f.write(tabulate(summary_df, headers='keys', tablefmt='grid', showindex=False))\n",
    "\n",
    "print(f\"\\n✅ All months processed. A summary report has been saved to '{report_filename}'\")\n",
    "\n",
    "print(f\"\\n--- Summary Report ---\")\n",
    "print(tabulate(summary_df, headers='keys', tablefmt='grid', showindex=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00cc0eed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7 monthly files to process in 'monthly_purchaser_lists'.\n",
      "\n",
      "--- Consolidating Monthly User Lists ---\n",
      "Processing purchasers_2025-08.parquet...\n",
      "   -> Read 1,561,961 users. Total unique users so far: 1,561,961\n",
      "Processing purchasers_2025-09.parquet...\n",
      "   -> Read 600,029 users. Total unique users so far: 1,870,438\n",
      "Processing purchasers_2025-03.parquet...\n",
      "   -> Read 976,617 users. Total unique users so far: 2,466,845\n",
      "Processing purchasers_2025-06.parquet...\n",
      "   -> Read 1,396,621 users. Total unique users so far: 3,215,571\n",
      "Processing purchasers_2025-07.parquet...\n",
      "   -> Read 1,436,993 users. Total unique users so far: 3,836,549\n",
      "Processing purchasers_2025-05.parquet...\n",
      "   -> Read 1,451,601 users. Total unique users so far: 4,425,325\n",
      "Processing purchasers_2025-04.parquet...\n",
      "   -> Read 1,399,898 users. Total unique users so far: 4,926,305\n",
      "\n",
      "--- Consolidation Complete ---\n",
      "The final, de-duplicated list contains 4,926,305 unique purchasing users.\n",
      "Master list saved to 'final_purchaser_universe.parquet'\n",
      "\n",
      "✅ A summary report has been saved to 'purchaser_universe_consolidation_report.txt'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from tabulate import tabulate\n",
    "\n",
    "# --- 1. Configuration ---\n",
    "INPUT_DIR = \"monthly_purchaser_lists\"\n",
    "FINAL_OUTPUT_FILE = \"final_purchaser_universe.parquet\"\n",
    "\n",
    "# --- 2. Discover and Process Monthly Files ---\n",
    "# Find all the Parquet files in the input directory\n",
    "try:\n",
    "    monthly_files = [f for f in os.listdir(INPUT_DIR) if f.endswith('.parquet')]\n",
    "    print(f\"Found {len(monthly_files)} monthly files to process in '{INPUT_DIR}'.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"❌ ERROR: Directory not found: '{INPUT_DIR}'. Please run the previous extraction script first.\")\n",
    "    # Exit the cell if the directory doesn't exist\n",
    "    exit()\n",
    "\n",
    "# Use a set for efficient, memory-safe de-duplication\n",
    "all_unique_users = set()\n",
    "files_processed = []\n",
    "\n",
    "print(\"\\n--- Consolidating Monthly User Lists ---\")\n",
    "for filename in monthly_files:\n",
    "    file_path = os.path.join(INPUT_DIR, filename)\n",
    "    print(f\"Processing {filename}...\")\n",
    "    \n",
    "    df_month = pd.read_parquet(file_path)\n",
    "    \n",
    "    # The .update() method efficiently adds all items from the list to the set\n",
    "    all_unique_users.update(df_month['USER_ID'])\n",
    "    \n",
    "    print(f\"   -> Read {len(df_month):,} users. Total unique users so far: {len(all_unique_users):,}\")\n",
    "    files_processed.append(filename)\n",
    "\n",
    "\n",
    "# --- 3. Finalize and Save the Master List ---\n",
    "print(\"\\n--- Consolidation Complete ---\")\n",
    "final_df = pd.DataFrame(list(all_unique_users), columns=['USER_ID'])\n",
    "final_df.to_parquet(FINAL_OUTPUT_FILE, index=False)\n",
    "\n",
    "final_user_count = len(final_df)\n",
    "print(f\"The final, de-duplicated list contains {final_user_count:,.0f} unique purchasing users.\")\n",
    "print(f\"Master list saved to '{FINAL_OUTPUT_FILE}'\")\n",
    "\n",
    "\n",
    "# --- 4. Generate a Final Summary Report ---\n",
    "report_filename = \"purchaser_universe_consolidation_report.txt\"\n",
    "with open(report_filename, \"w\") as f:\n",
    "    f.write(\"Purchaser Universe Consolidation Report\\n\")\n",
    "    f.write(\"=\" * 39 + \"\\n\\n\")\n",
    "    f.write(\"Methodology: All monthly Parquet files containing unique purchaser IDs\\n\")\n",
    "    f.write(\"             were read and consolidated. A de-duplicated master list of all\\n\")\n",
    "    f.write(\"             users who made at least one purchase was generated.\\n\\n\")\n",
    "\n",
    "    f.write(\"Final Result\\n\")\n",
    "    f.write(\"-\" * 12 + \"\\n\")\n",
    "    f.write(f\"Total Unique Purchasing Users: {final_user_count:,.0f}\\n\\n\")\n",
    "\n",
    "    f.write(\"Source Files Processed\\n\")\n",
    "    f.write(\"-\" * 22 + \"\\n\")\n",
    "    for fname in sorted(files_processed):\n",
    "        f.write(f\"- {fname}\\n\")\n",
    "\n",
    "print(f\"\\n✅ A summary report has been saved to '{report_filename}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60bdf094",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- SCRIPT INITIATED: Holdout Identification (Max Verbosity) ---\n",
      "Loading master universe from 'final_purchaser_universe.parquet'\n",
      "Loaded 4,926,305 potential holdout users to start.\n",
      "\n",
      "================================================================================\n",
      "--- Processing Week 1/28 (2025-03-10 to 2025-03-11) ---\n",
      "================================================================================\n",
      "  [Step 1] Holdouts at start of week: 4,926,305\n",
      "  [Step 2] Querying Snowflake for users with impressions this week...\n",
      "     -> Query complete in 2.72s. Found 0 distinct impressed users.\n",
      "  [Step 3] Calculating overlap between 4,926,305 holdouts and 0 impressed users...\n",
      "     -> Overlap found: 0 users. These are the dissolved holdouts.\n",
      "  [Step 4] Removing dissolved users from the main set...\n",
      "     -> Holdouts remaining: 4,926,305 (4,926,305 -> 4,926,305)\n",
      "  [Step 5] Saving weekly checkpoint with 4,926,305 users...\n",
      "     -> Checkpoint saved successfully.\n",
      "\n",
      "================================================================================\n",
      "--- Processing Week 2/28 (2025-03-11 to 2025-03-18) ---\n",
      "================================================================================\n",
      "  [Step 1] Holdouts at start of week: 4,926,305\n",
      "  [Step 2] Querying Snowflake for users with impressions this week...\n",
      "     -> Query complete in 74.27s. Found 1,255,787 distinct impressed users.\n",
      "  [Step 3] Calculating overlap between 4,926,305 holdouts and 1,255,787 impressed users...\n",
      "     -> Overlap found: 777,060 users. These are the dissolved holdouts.\n",
      "     -> Logged these 777,060 user IDs to 'dissolved_holdout_logs/dissolved_2025-03-11.parquet'\n",
      "  [Step 4] Removing dissolved users from the main set...\n",
      "     -> Holdouts remaining: 4,149,245 (4,926,305 -> 4,149,245)\n",
      "  [Step 5] Saving weekly checkpoint with 4,149,245 users...\n",
      "     -> Checkpoint saved successfully.\n",
      "\n",
      "================================================================================\n",
      "--- Processing Week 3/28 (2025-03-18 to 2025-03-25) ---\n",
      "================================================================================\n",
      "  [Step 1] Holdouts at start of week: 4,149,245\n",
      "  [Step 2] Querying Snowflake for users with impressions this week...\n",
      "     -> Query complete in 208.12s. Found 2,245,629 distinct impressed users.\n",
      "  [Step 3] Calculating overlap between 4,149,245 holdouts and 2,245,629 impressed users...\n",
      "     -> Overlap found: 706,630 users. These are the dissolved holdouts.\n",
      "     -> Logged these 706,630 user IDs to 'dissolved_holdout_logs/dissolved_2025-03-18.parquet'\n",
      "  [Step 4] Removing dissolved users from the main set...\n",
      "     -> Holdouts remaining: 3,442,615 (4,149,245 -> 3,442,615)\n",
      "  [Step 5] Saving weekly checkpoint with 3,442,615 users...\n",
      "     -> Checkpoint saved successfully.\n",
      "\n",
      "================================================================================\n",
      "--- Processing Week 4/28 (2025-03-25 to 2025-04-01) ---\n",
      "================================================================================\n",
      "  [Step 1] Holdouts at start of week: 3,442,615\n",
      "  [Step 2] Querying Snowflake for users with impressions this week...\n",
      "     -> Query complete in 234.49s. Found 2,382,963 distinct impressed users.\n",
      "  [Step 3] Calculating overlap between 3,442,615 holdouts and 2,382,963 impressed users...\n",
      "     -> Overlap found: 403,823 users. These are the dissolved holdouts.\n",
      "     -> Logged these 403,823 user IDs to 'dissolved_holdout_logs/dissolved_2025-03-25.parquet'\n",
      "  [Step 4] Removing dissolved users from the main set...\n",
      "     -> Holdouts remaining: 3,038,792 (3,442,615 -> 3,038,792)\n",
      "  [Step 5] Saving weekly checkpoint with 3,038,792 users...\n",
      "     -> Checkpoint saved successfully.\n",
      "\n",
      "================================================================================\n",
      "--- Processing Week 5/28 (2025-04-01 to 2025-04-08) ---\n",
      "================================================================================\n",
      "  [Step 1] Holdouts at start of week: 3,038,792\n",
      "  [Step 2] Querying Snowflake for users with impressions this week...\n",
      "     -> Query complete in 213.15s. Found 2,391,030 distinct impressed users.\n",
      "  [Step 3] Calculating overlap between 3,038,792 holdouts and 2,391,030 impressed users...\n",
      "     -> Overlap found: 274,529 users. These are the dissolved holdouts.\n",
      "     -> Logged these 274,529 user IDs to 'dissolved_holdout_logs/dissolved_2025-04-01.parquet'\n",
      "  [Step 4] Removing dissolved users from the main set...\n",
      "     -> Holdouts remaining: 2,764,263 (3,038,792 -> 2,764,263)\n",
      "  [Step 5] Saving weekly checkpoint with 2,764,263 users...\n",
      "     -> Checkpoint saved successfully.\n",
      "\n",
      "================================================================================\n",
      "--- Processing Week 6/28 (2025-04-08 to 2025-04-15) ---\n",
      "================================================================================\n",
      "  [Step 1] Holdouts at start of week: 2,764,263\n",
      "  [Step 2] Querying Snowflake for users with impressions this week...\n",
      "     -> Query complete in 219.52s. Found 2,390,215 distinct impressed users.\n",
      "  [Step 3] Calculating overlap between 2,764,263 holdouts and 2,390,215 impressed users...\n",
      "     -> Overlap found: 219,992 users. These are the dissolved holdouts.\n",
      "     -> Logged these 219,992 user IDs to 'dissolved_holdout_logs/dissolved_2025-04-08.parquet'\n",
      "  [Step 4] Removing dissolved users from the main set...\n",
      "     -> Holdouts remaining: 2,544,271 (2,764,263 -> 2,544,271)\n",
      "  [Step 5] Saving weekly checkpoint with 2,544,271 users...\n",
      "     -> Checkpoint saved successfully.\n",
      "\n",
      "================================================================================\n",
      "--- Processing Week 7/28 (2025-04-15 to 2025-04-22) ---\n",
      "================================================================================\n",
      "  [Step 1] Holdouts at start of week: 2,544,271\n",
      "  [Step 2] Querying Snowflake for users with impressions this week...\n",
      "     -> Query complete in 236.16s. Found 2,368,773 distinct impressed users.\n",
      "  [Step 3] Calculating overlap between 2,544,271 holdouts and 2,368,773 impressed users...\n",
      "     -> Overlap found: 177,593 users. These are the dissolved holdouts.\n",
      "     -> Logged these 177,593 user IDs to 'dissolved_holdout_logs/dissolved_2025-04-15.parquet'\n",
      "  [Step 4] Removing dissolved users from the main set...\n",
      "     -> Holdouts remaining: 2,366,678 (2,544,271 -> 2,366,678)\n",
      "  [Step 5] Saving weekly checkpoint with 2,366,678 users...\n",
      "     -> Checkpoint saved successfully.\n",
      "\n",
      "================================================================================\n",
      "--- Processing Week 8/28 (2025-04-22 to 2025-04-29) ---\n",
      "================================================================================\n",
      "  [Step 1] Holdouts at start of week: 2,366,678\n",
      "  [Step 2] Querying Snowflake for users with impressions this week...\n",
      "     -> Query complete in 235.11s. Found 2,416,857 distinct impressed users.\n",
      "  [Step 3] Calculating overlap between 2,366,678 holdouts and 2,416,857 impressed users...\n",
      "     -> Overlap found: 156,007 users. These are the dissolved holdouts.\n",
      "     -> Logged these 156,007 user IDs to 'dissolved_holdout_logs/dissolved_2025-04-22.parquet'\n",
      "  [Step 4] Removing dissolved users from the main set...\n",
      "     -> Holdouts remaining: 2,210,671 (2,366,678 -> 2,210,671)\n",
      "  [Step 5] Saving weekly checkpoint with 2,210,671 users...\n",
      "     -> Checkpoint saved successfully.\n",
      "\n",
      "================================================================================\n",
      "--- Processing Week 9/28 (2025-04-29 to 2025-05-06) ---\n",
      "================================================================================\n",
      "  [Step 1] Holdouts at start of week: 2,210,671\n",
      "  [Step 2] Querying Snowflake for users with impressions this week...\n",
      "     -> Query complete in 259.16s. Found 2,453,539 distinct impressed users.\n",
      "  [Step 3] Calculating overlap between 2,210,671 holdouts and 2,453,539 impressed users...\n",
      "     -> Overlap found: 139,329 users. These are the dissolved holdouts.\n",
      "     -> Logged these 139,329 user IDs to 'dissolved_holdout_logs/dissolved_2025-04-29.parquet'\n",
      "  [Step 4] Removing dissolved users from the main set...\n",
      "     -> Holdouts remaining: 2,071,342 (2,210,671 -> 2,071,342)\n",
      "  [Step 5] Saving weekly checkpoint with 2,071,342 users...\n",
      "     -> Checkpoint saved successfully.\n",
      "\n",
      "================================================================================\n",
      "--- Processing Week 10/28 (2025-05-06 to 2025-05-13) ---\n",
      "================================================================================\n",
      "  [Step 1] Holdouts at start of week: 2,071,342\n",
      "  [Step 2] Querying Snowflake for users with impressions this week...\n",
      "     -> Query complete in 423.32s. Found 2,389,262 distinct impressed users.\n",
      "  [Step 3] Calculating overlap between 2,071,342 holdouts and 2,389,262 impressed users...\n",
      "     -> Overlap found: 113,123 users. These are the dissolved holdouts.\n",
      "     -> Logged these 113,123 user IDs to 'dissolved_holdout_logs/dissolved_2025-05-06.parquet'\n",
      "  [Step 4] Removing dissolved users from the main set...\n",
      "     -> Holdouts remaining: 1,958,219 (2,071,342 -> 1,958,219)\n",
      "  [Step 5] Saving weekly checkpoint with 1,958,219 users...\n",
      "     -> Checkpoint saved successfully.\n",
      "\n",
      "================================================================================\n",
      "--- Processing Week 11/28 (2025-05-13 to 2025-05-20) ---\n",
      "================================================================================\n",
      "  [Step 1] Holdouts at start of week: 1,958,219\n",
      "  [Step 2] Querying Snowflake for users with impressions this week...\n",
      "     -> Query complete in 384.20s. Found 2,415,925 distinct impressed users.\n",
      "  [Step 3] Calculating overlap between 1,958,219 holdouts and 2,415,925 impressed users...\n",
      "     -> Overlap found: 103,030 users. These are the dissolved holdouts.\n",
      "     -> Logged these 103,030 user IDs to 'dissolved_holdout_logs/dissolved_2025-05-13.parquet'\n",
      "  [Step 4] Removing dissolved users from the main set...\n",
      "     -> Holdouts remaining: 1,855,189 (1,958,219 -> 1,855,189)\n",
      "  [Step 5] Saving weekly checkpoint with 1,855,189 users...\n",
      "     -> Checkpoint saved successfully.\n",
      "\n",
      "================================================================================\n",
      "--- Processing Week 12/28 (2025-05-20 to 2025-05-27) ---\n",
      "================================================================================\n",
      "  [Step 1] Holdouts at start of week: 1,855,189\n",
      "  [Step 2] Querying Snowflake for users with impressions this week...\n",
      "     -> Query complete in 303.21s. Found 2,452,582 distinct impressed users.\n",
      "  [Step 3] Calculating overlap between 1,855,189 holdouts and 2,452,582 impressed users...\n",
      "     -> Overlap found: 109,844 users. These are the dissolved holdouts.\n",
      "     -> Logged these 109,844 user IDs to 'dissolved_holdout_logs/dissolved_2025-05-20.parquet'\n",
      "  [Step 4] Removing dissolved users from the main set...\n",
      "     -> Holdouts remaining: 1,745,345 (1,855,189 -> 1,745,345)\n",
      "  [Step 5] Saving weekly checkpoint with 1,745,345 users...\n",
      "     -> Checkpoint saved successfully.\n",
      "\n",
      "================================================================================\n",
      "--- Processing Week 13/28 (2025-05-27 to 2025-06-03) ---\n",
      "================================================================================\n",
      "  [Step 1] Holdouts at start of week: 1,745,345\n",
      "  [Step 2] Querying Snowflake for users with impressions this week...\n",
      "     -> Query complete in 291.59s. Found 2,494,809 distinct impressed users.\n",
      "  [Step 3] Calculating overlap between 1,745,345 holdouts and 2,494,809 impressed users...\n",
      "     -> Overlap found: 96,877 users. These are the dissolved holdouts.\n",
      "     -> Logged these 96,877 user IDs to 'dissolved_holdout_logs/dissolved_2025-05-27.parquet'\n",
      "  [Step 4] Removing dissolved users from the main set...\n",
      "     -> Holdouts remaining: 1,648,468 (1,745,345 -> 1,648,468)\n",
      "  [Step 5] Saving weekly checkpoint with 1,648,468 users...\n",
      "     -> Checkpoint saved successfully.\n",
      "\n",
      "================================================================================\n",
      "--- Processing Week 14/28 (2025-06-03 to 2025-06-10) ---\n",
      "================================================================================\n",
      "  [Step 1] Holdouts at start of week: 1,648,468\n",
      "  [Step 2] Querying Snowflake for users with impressions this week...\n",
      "     -> Query complete in 332.28s. Found 2,441,864 distinct impressed users.\n",
      "  [Step 3] Calculating overlap between 1,648,468 holdouts and 2,441,864 impressed users...\n",
      "     -> Overlap found: 84,701 users. These are the dissolved holdouts.\n",
      "     -> Logged these 84,701 user IDs to 'dissolved_holdout_logs/dissolved_2025-06-03.parquet'\n",
      "  [Step 4] Removing dissolved users from the main set...\n",
      "     -> Holdouts remaining: 1,563,767 (1,648,468 -> 1,563,767)\n",
      "  [Step 5] Saving weekly checkpoint with 1,563,767 users...\n",
      "     -> Checkpoint saved successfully.\n",
      "\n",
      "================================================================================\n",
      "--- Processing Week 15/28 (2025-06-10 to 2025-06-17) ---\n",
      "================================================================================\n",
      "  [Step 1] Holdouts at start of week: 1,563,767\n",
      "  [Step 2] Querying Snowflake for users with impressions this week...\n",
      "     -> Query complete in 355.98s. Found 2,406,172 distinct impressed users.\n",
      "  [Step 3] Calculating overlap between 1,563,767 holdouts and 2,406,172 impressed users...\n",
      "     -> Overlap found: 76,590 users. These are the dissolved holdouts.\n",
      "     -> Logged these 76,590 user IDs to 'dissolved_holdout_logs/dissolved_2025-06-10.parquet'\n",
      "  [Step 4] Removing dissolved users from the main set...\n",
      "     -> Holdouts remaining: 1,487,177 (1,563,767 -> 1,487,177)\n",
      "  [Step 5] Saving weekly checkpoint with 1,487,177 users...\n",
      "     -> Checkpoint saved successfully.\n",
      "\n",
      "================================================================================\n",
      "--- Processing Week 16/28 (2025-06-17 to 2025-06-24) ---\n",
      "================================================================================\n",
      "  [Step 1] Holdouts at start of week: 1,487,177\n",
      "  [Step 2] Querying Snowflake for users with impressions this week...\n",
      "     -> Query complete in 320.79s. Found 2,398,269 distinct impressed users.\n",
      "  [Step 3] Calculating overlap between 1,487,177 holdouts and 2,398,269 impressed users...\n",
      "     -> Overlap found: 71,218 users. These are the dissolved holdouts.\n",
      "     -> Logged these 71,218 user IDs to 'dissolved_holdout_logs/dissolved_2025-06-17.parquet'\n",
      "  [Step 4] Removing dissolved users from the main set...\n",
      "     -> Holdouts remaining: 1,415,959 (1,487,177 -> 1,415,959)\n",
      "  [Step 5] Saving weekly checkpoint with 1,415,959 users...\n",
      "     -> Checkpoint saved successfully.\n",
      "\n",
      "================================================================================\n",
      "--- Processing Week 17/28 (2025-06-24 to 2025-07-01) ---\n",
      "================================================================================\n",
      "  [Step 1] Holdouts at start of week: 1,415,959\n",
      "  [Step 2] Querying Snowflake for users with impressions this week...\n",
      "     -> Query complete in 340.04s. Found 2,321,704 distinct impressed users.\n",
      "  [Step 3] Calculating overlap between 1,415,959 holdouts and 2,321,704 impressed users...\n",
      "     -> Overlap found: 64,546 users. These are the dissolved holdouts.\n",
      "     -> Logged these 64,546 user IDs to 'dissolved_holdout_logs/dissolved_2025-06-24.parquet'\n",
      "  [Step 4] Removing dissolved users from the main set...\n",
      "     -> Holdouts remaining: 1,351,413 (1,415,959 -> 1,351,413)\n",
      "  [Step 5] Saving weekly checkpoint with 1,351,413 users...\n",
      "     -> Checkpoint saved successfully.\n",
      "\n",
      "================================================================================\n",
      "--- Processing Week 18/28 (2025-07-01 to 2025-07-08) ---\n",
      "================================================================================\n",
      "  [Step 1] Holdouts at start of week: 1,351,413\n",
      "  [Step 2] Querying Snowflake for users with impressions this week...\n",
      "     -> Query complete in 312.61s. Found 2,340,564 distinct impressed users.\n",
      "  [Step 3] Calculating overlap between 1,351,413 holdouts and 2,340,564 impressed users...\n",
      "     -> Overlap found: 60,848 users. These are the dissolved holdouts.\n",
      "     -> Logged these 60,848 user IDs to 'dissolved_holdout_logs/dissolved_2025-07-01.parquet'\n",
      "  [Step 4] Removing dissolved users from the main set...\n",
      "     -> Holdouts remaining: 1,290,565 (1,351,413 -> 1,290,565)\n",
      "  [Step 5] Saving weekly checkpoint with 1,290,565 users...\n",
      "     -> Checkpoint saved successfully.\n",
      "\n",
      "================================================================================\n",
      "--- Processing Week 19/28 (2025-07-08 to 2025-07-15) ---\n",
      "================================================================================\n",
      "  [Step 1] Holdouts at start of week: 1,290,565\n",
      "  [Step 2] Querying Snowflake for users with impressions this week...\n",
      "     -> Query complete in 317.98s. Found 2,445,312 distinct impressed users.\n",
      "  [Step 3] Calculating overlap between 1,290,565 holdouts and 2,445,312 impressed users...\n",
      "     -> Overlap found: 63,402 users. These are the dissolved holdouts.\n",
      "     -> Logged these 63,402 user IDs to 'dissolved_holdout_logs/dissolved_2025-07-08.parquet'\n",
      "  [Step 4] Removing dissolved users from the main set...\n",
      "     -> Holdouts remaining: 1,227,163 (1,290,565 -> 1,227,163)\n",
      "  [Step 5] Saving weekly checkpoint with 1,227,163 users...\n",
      "     -> Checkpoint saved successfully.\n",
      "\n",
      "================================================================================\n",
      "--- Processing Week 20/28 (2025-07-15 to 2025-07-22) ---\n",
      "================================================================================\n",
      "  [Step 1] Holdouts at start of week: 1,227,163\n",
      "  [Step 2] Querying Snowflake for users with impressions this week...\n",
      "     -> Query complete in 392.69s. Found 2,565,846 distinct impressed users.\n",
      "  [Step 3] Calculating overlap between 1,227,163 holdouts and 2,565,846 impressed users...\n",
      "     -> Overlap found: 63,558 users. These are the dissolved holdouts.\n",
      "     -> Logged these 63,558 user IDs to 'dissolved_holdout_logs/dissolved_2025-07-15.parquet'\n",
      "  [Step 4] Removing dissolved users from the main set...\n",
      "     -> Holdouts remaining: 1,163,605 (1,227,163 -> 1,163,605)\n",
      "  [Step 5] Saving weekly checkpoint with 1,163,605 users...\n",
      "     -> Checkpoint saved successfully.\n",
      "\n",
      "================================================================================\n",
      "--- Processing Week 21/28 (2025-07-22 to 2025-07-29) ---\n",
      "================================================================================\n",
      "  [Step 1] Holdouts at start of week: 1,163,605\n",
      "  [Step 2] Querying Snowflake for users with impressions this week...\n",
      "     -> Query complete in 406.27s. Found 2,621,642 distinct impressed users.\n",
      "  [Step 3] Calculating overlap between 1,163,605 holdouts and 2,621,642 impressed users...\n",
      "     -> Overlap found: 62,436 users. These are the dissolved holdouts.\n",
      "     -> Logged these 62,436 user IDs to 'dissolved_holdout_logs/dissolved_2025-07-22.parquet'\n",
      "  [Step 4] Removing dissolved users from the main set...\n",
      "     -> Holdouts remaining: 1,101,169 (1,163,605 -> 1,101,169)\n",
      "  [Step 5] Saving weekly checkpoint with 1,101,169 users...\n",
      "     -> Checkpoint saved successfully.\n",
      "\n",
      "================================================================================\n",
      "--- Processing Week 22/28 (2025-07-29 to 2025-08-05) ---\n",
      "================================================================================\n",
      "  [Step 1] Holdouts at start of week: 1,101,169\n",
      "  [Step 2] Querying Snowflake for users with impressions this week...\n",
      "     -> Query complete in 422.99s. Found 2,655,815 distinct impressed users.\n",
      "  [Step 3] Calculating overlap between 1,101,169 holdouts and 2,655,815 impressed users...\n",
      "     -> Overlap found: 61,277 users. These are the dissolved holdouts.\n",
      "     -> Logged these 61,277 user IDs to 'dissolved_holdout_logs/dissolved_2025-07-29.parquet'\n",
      "  [Step 4] Removing dissolved users from the main set...\n",
      "     -> Holdouts remaining: 1,039,892 (1,101,169 -> 1,039,892)\n",
      "  [Step 5] Saving weekly checkpoint with 1,039,892 users...\n",
      "     -> Checkpoint saved successfully.\n",
      "\n",
      "================================================================================\n",
      "--- Processing Week 23/28 (2025-08-05 to 2025-08-12) ---\n",
      "================================================================================\n",
      "  [Step 1] Holdouts at start of week: 1,039,892\n",
      "  [Step 2] Querying Snowflake for users with impressions this week...\n",
      "     -> Query complete in 417.38s. Found 2,673,481 distinct impressed users.\n",
      "  [Step 3] Calculating overlap between 1,039,892 holdouts and 2,673,481 impressed users...\n",
      "     -> Overlap found: 57,751 users. These are the dissolved holdouts.\n",
      "     -> Logged these 57,751 user IDs to 'dissolved_holdout_logs/dissolved_2025-08-05.parquet'\n",
      "  [Step 4] Removing dissolved users from the main set...\n",
      "     -> Holdouts remaining: 982,141 (1,039,892 -> 982,141)\n",
      "  [Step 5] Saving weekly checkpoint with 982,141 users...\n",
      "     -> Checkpoint saved successfully.\n",
      "\n",
      "================================================================================\n",
      "--- Processing Week 24/28 (2025-08-12 to 2025-08-19) ---\n",
      "================================================================================\n",
      "  [Step 1] Holdouts at start of week: 982,141\n",
      "  [Step 2] Querying Snowflake for users with impressions this week...\n",
      "     -> Query complete in 401.72s. Found 2,667,942 distinct impressed users.\n",
      "  [Step 3] Calculating overlap between 982,141 holdouts and 2,667,942 impressed users...\n",
      "     -> Overlap found: 54,832 users. These are the dissolved holdouts.\n",
      "     -> Logged these 54,832 user IDs to 'dissolved_holdout_logs/dissolved_2025-08-12.parquet'\n",
      "  [Step 4] Removing dissolved users from the main set...\n",
      "     -> Holdouts remaining: 927,309 (982,141 -> 927,309)\n",
      "  [Step 5] Saving weekly checkpoint with 927,309 users...\n",
      "     -> Checkpoint saved successfully.\n",
      "\n",
      "================================================================================\n",
      "--- Processing Week 25/28 (2025-08-19 to 2025-08-26) ---\n",
      "================================================================================\n",
      "  [Step 1] Holdouts at start of week: 927,309\n",
      "  [Step 2] Querying Snowflake for users with impressions this week...\n",
      "     -> Query complete in 455.48s. Found 2,661,552 distinct impressed users.\n",
      "  [Step 3] Calculating overlap between 927,309 holdouts and 2,661,552 impressed users...\n",
      "     -> Overlap found: 50,651 users. These are the dissolved holdouts.\n",
      "     -> Logged these 50,651 user IDs to 'dissolved_holdout_logs/dissolved_2025-08-19.parquet'\n",
      "  [Step 4] Removing dissolved users from the main set...\n",
      "     -> Holdouts remaining: 876,658 (927,309 -> 876,658)\n",
      "  [Step 5] Saving weekly checkpoint with 876,658 users...\n",
      "     -> Checkpoint saved successfully.\n",
      "\n",
      "================================================================================\n",
      "--- Processing Week 26/28 (2025-08-26 to 2025-09-02) ---\n",
      "================================================================================\n",
      "  [Step 1] Holdouts at start of week: 876,658\n",
      "  [Step 2] Querying Snowflake for users with impressions this week...\n",
      "     -> Query complete in 444.24s. Found 2,679,107 distinct impressed users.\n",
      "  [Step 3] Calculating overlap between 876,658 holdouts and 2,679,107 impressed users...\n",
      "     -> Overlap found: 47,780 users. These are the dissolved holdouts.\n",
      "     -> Logged these 47,780 user IDs to 'dissolved_holdout_logs/dissolved_2025-08-26.parquet'\n",
      "  [Step 4] Removing dissolved users from the main set...\n",
      "     -> Holdouts remaining: 828,878 (876,658 -> 828,878)\n",
      "  [Step 5] Saving weekly checkpoint with 828,878 users...\n",
      "     -> Checkpoint saved successfully.\n",
      "\n",
      "================================================================================\n",
      "--- Processing Week 27/28 (2025-09-02 to 2025-09-09) ---\n",
      "================================================================================\n",
      "  [Step 1] Holdouts at start of week: 828,878\n",
      "  [Step 2] Querying Snowflake for users with impressions this week...\n",
      "     -> Query complete in 492.23s. Found 2,711,493 distinct impressed users.\n",
      "  [Step 3] Calculating overlap between 828,878 holdouts and 2,711,493 impressed users...\n",
      "     -> Overlap found: 43,462 users. These are the dissolved holdouts.\n",
      "     -> Logged these 43,462 user IDs to 'dissolved_holdout_logs/dissolved_2025-09-02.parquet'\n",
      "  [Step 4] Removing dissolved users from the main set...\n",
      "     -> Holdouts remaining: 785,416 (828,878 -> 785,416)\n",
      "  [Step 5] Saving weekly checkpoint with 785,416 users...\n",
      "     -> Checkpoint saved successfully.\n",
      "\n",
      "================================================================================\n",
      "--- Processing Week 28/28 (2025-09-09 to 2025-09-16) ---\n",
      "================================================================================\n",
      "  [Step 1] Holdouts at start of week: 785,416\n",
      "  [Step 2] Querying Snowflake for users with impressions this week...\n",
      "     -> Query complete in 12.93s. Found 329,926 distinct impressed users.\n",
      "  [Step 3] Calculating overlap between 785,416 holdouts and 329,926 impressed users...\n",
      "     -> Overlap found: 1,283 users. These are the dissolved holdouts.\n",
      "     -> Logged these 1,283 user IDs to 'dissolved_holdout_logs/dissolved_2025-09-09.parquet'\n",
      "  [Step 4] Removing dissolved users from the main set...\n",
      "     -> Holdouts remaining: 784,133 (785,416 -> 784,133)\n",
      "  [Step 5] Saving weekly checkpoint with 784,133 users...\n",
      "     -> Checkpoint saved successfully.\n",
      "\n",
      "================================================================================\n",
      "--- PROCESS COMPLETE OR INTERRUPTED: FINALIZING ---\n",
      "================================================================================\n",
      "  -> Saving final list of 784,133 holdout users to 'final_holdout_user_ids_final.parquet'...\n",
      "  -> Saving detailed run log to 'final_holdout_run_log_with_dissolution.txt'...\n",
      "  -> Cleaning up checkpoint file...\n",
      "\n",
      "✅ All operations complete.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "from tabulate import tabulate\n",
    "\n",
    "# --- 1. Configuration ---\n",
    "UNIVERSE_FILE = \"final_purchaser_universe.parquet\"\n",
    "CHECKPOINT_FILE = \"potential_holdouts_checkpoint.parquet\"\n",
    "FINAL_HOLDOUT_FILE = \"final_holdout_user_ids_final.parquet\"\n",
    "LOG_REPORT_FILE = \"final_holdout_run_log_with_dissolution.txt\"\n",
    "DISSOLVED_LOGS_DIR = \"dissolved_holdout_logs\"\n",
    "\n",
    "START_DATE = \"2025-03-10\"\n",
    "END_DATE = \"2025-09-15\"\n",
    "\n",
    "# --- 2. Setup: Load Universe or Resume from Checkpoint ---\n",
    "print(\"--- SCRIPT INITIATED: Holdout Identification (Max Verbosity) ---\")\n",
    "os.makedirs(DISSOLVED_LOGS_DIR, exist_ok=True)\n",
    "\n",
    "if os.path.exists(CHECKPOINT_FILE):\n",
    "    print(f\"Resuming from checkpoint: '{CHECKPOINT_FILE}'\")\n",
    "    df_checkpoint = pd.read_parquet(CHECKPOINT_FILE)\n",
    "    potential_holdouts_set = set(df_checkpoint['USER_ID'])\n",
    "    last_date_processed = pd.to_datetime(df_checkpoint.attrs['last_date_processed'])\n",
    "    start_date_obj = last_date_processed + pd.DateOffset(days=1)\n",
    "    print(f\"Resuming process from {start_date_obj.strftime('%Y-%m-%d')}\")\n",
    "else:\n",
    "    print(f\"Loading master universe from '{UNIVERSE_FILE}'\")\n",
    "    df_universe = pd.read_parquet(UNIVERSE_FILE)\n",
    "    potential_holdouts_set = set(df_universe['USER_ID'])\n",
    "    start_date_obj = pd.to_datetime(START_DATE)\n",
    "\n",
    "initial_universe_size = len(potential_holdouts_set)\n",
    "print(f\"Loaded {initial_universe_size:,} potential holdout users to start.\")\n",
    "\n",
    "# --- 3. Main Processing Loop ---\n",
    "weekly_date_ranges = pd.date_range(start=start_date_obj, end=END_DATE, freq='W-MON')\n",
    "weekly_logs = []\n",
    "total_start_time = time.time()\n",
    "cursor = None\n",
    "\n",
    "try:\n",
    "    current_start = start_date_obj\n",
    "    for i, week_end in enumerate(weekly_date_ranges):\n",
    "        week_start_str = current_start.strftime('%Y-%m-%d')\n",
    "        week_end_str = (week_end + pd.DateOffset(days=1)).strftime('%Y-%m-%d')\n",
    "        \n",
    "        print(f\"\\n{'='*80}\\n--- Processing Week {i+1}/{len(weekly_date_ranges)} ({week_start_str} to {week_end_str}) ---\\n{'='*80}\")\n",
    "        \n",
    "        holdouts_before_week = len(potential_holdouts_set)\n",
    "        print(f\"  [Step 1] Holdouts at start of week: {holdouts_before_week:,}\")\n",
    "\n",
    "        # --- Query Snowflake ---\n",
    "        query_start_time = time.time()\n",
    "        weekly_impressed_query = f\"SELECT DISTINCT USER_ID FROM IMPRESSIONS WHERE OCCURRED_AT >= '{week_start_str}' AND OCCURRED_AT < '{week_end_str}';\"\n",
    "        \n",
    "        print(f\"  [Step 2] Querying Snowflake for users with impressions this week...\")\n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute(weekly_impressed_query)\n",
    "        \n",
    "        impressed_this_week_set = set()\n",
    "        for batch in cursor.fetch_pandas_batches():\n",
    "            impressed_this_week_set.update(batch['USER_ID'])\n",
    "        \n",
    "        query_duration = time.time() - query_start_time\n",
    "        impressed_count_this_week = len(impressed_this_week_set)\n",
    "        print(f\"     -> Query complete in {query_duration:.2f}s. Found {impressed_count_this_week:,} distinct impressed users.\")\n",
    "\n",
    "        # --- Calculate and Log Overlap ---\n",
    "        print(f\"  [Step 3] Calculating overlap between {holdouts_before_week:,} holdouts and {impressed_count_this_week:,} impressed users...\")\n",
    "        dissolved_this_week_set = potential_holdouts_set.intersection(impressed_this_week_set)\n",
    "        dissolved_count = len(dissolved_this_week_set)\n",
    "        print(f\"     -> Overlap found: {dissolved_count:,} users. These are the dissolved holdouts.\")\n",
    "\n",
    "        if dissolved_count > 0:\n",
    "            dissolved_df = pd.DataFrame(list(dissolved_this_week_set), columns=['USER_ID'])\n",
    "            log_path = os.path.join(DISSOLVED_LOGS_DIR, f\"dissolved_{week_start_str}.parquet\")\n",
    "            dissolved_df.to_parquet(log_path, index=False)\n",
    "            print(f\"     -> Logged these {dissolved_count:,} user IDs to '{log_path}'\")\n",
    "\n",
    "        # --- Update Main Set and Report ---\n",
    "        print(f\"  [Step 4] Removing dissolved users from the main set...\")\n",
    "        potential_holdouts_set.difference_update(dissolved_this_week_set)\n",
    "        holdouts_after_week = len(potential_holdouts_set)\n",
    "        print(f\"     -> Holdouts remaining: {holdouts_after_week:,} ({holdouts_before_week:,} -> {holdouts_after_week:,})\")\n",
    "        \n",
    "        weekly_logs.append({\n",
    "            \"Week Start\": week_start_str,\n",
    "            \"Query Time (s)\": f\"{query_duration:.2f}\",\n",
    "            \"Holdouts Before\": f\"{holdouts_before_week:,}\",\n",
    "            \"Impressed This Week\": f\"{impressed_count_this_week:,}\",\n",
    "            \"Holdouts Dissolved\": f\"{dissolved_count:,}\",\n",
    "            \"Holdouts After\": f\"{holdouts_after_week:,}\"\n",
    "        })\n",
    "\n",
    "        # --- Checkpoint ---\n",
    "        print(f\"  [Step 5] Saving weekly checkpoint with {holdouts_after_week:,} users...\")\n",
    "        checkpoint_df = pd.DataFrame(list(potential_holdouts_set), columns=['USER_ID'])\n",
    "        checkpoint_df.attrs['last_date_processed'] = week_end.strftime('%Y-%m-%d')\n",
    "        checkpoint_df.to_parquet(CHECKPOINT_FILE, index=False)\n",
    "        print(\"     -> Checkpoint saved successfully.\")\n",
    "        \n",
    "        current_start = week_end + pd.DateOffset(days=1)\n",
    "\n",
    "finally:\n",
    "    total_duration_minutes = (time.time() - total_start_time) / 60\n",
    "    \n",
    "    print(f\"\\n{'='*80}\\n--- PROCESS COMPLETE OR INTERRUPTED: FINALIZING ---\\n{'='*80}\")\n",
    "    \n",
    "    final_holdouts_df = pd.DataFrame(list(potential_holdouts_set), columns=['USER_ID'])\n",
    "    final_count = len(final_holdouts_df)\n",
    "    \n",
    "    print(f\"  -> Saving final list of {final_count:,} holdout users to '{FINAL_HOLDOUT_FILE}'...\")\n",
    "    final_holdouts_df.to_parquet(FINAL_HOLDOUT_FILE, index=False)\n",
    "    \n",
    "    log_df = pd.DataFrame(weekly_logs)\n",
    "    \n",
    "    print(f\"  -> Saving detailed run log to '{LOG_REPORT_FILE}'...\")\n",
    "    with open(LOG_REPORT_FILE, \"w\") as f:\n",
    "        f.write(\"Optimized Holdout Identification - Run Log & Summary\\n\")\n",
    "        f.write(\"=\" * 51 + \"\\n\\n\")\n",
    "        f.write(f\"Total Run Time: {total_duration_minutes:.2f} minutes\\n\")\n",
    "        f.write(f\"Weeks Processed: {len(weekly_logs)}\\n\")\n",
    "        if 'df_universe' in locals():\n",
    "            f.write(f\"Initial User Universe: {len(df_universe):,}\\n\")\n",
    "        f.write(f\"Final Holdout Users Found: {final_count:,}\\n\\n\")\n",
    "        f.write(f\"NOTE: Detailed lists of dissolved holdouts are in '{DISSOLVED_LOGS_DIR}/'\\n\\n\")\n",
    "        f.write(\"Weekly Processing Log\\n---------------------\\n\")\n",
    "        f.write(tabulate(log_df, headers='keys', tablefmt='grid', showindex=False))\n",
    "    \n",
    "    if os.path.exists(CHECKPOINT_FILE):\n",
    "        print(f\"  -> Cleaning up checkpoint file...\")\n",
    "        os.remove(CHECKPOINT_FILE)\n",
    "    \n",
    "    print(\"\\n✅ All operations complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f76919c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
