{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Connected to Snowflake\n",
      "\n",
      "--- Generating User-Vendor-Week Panel for Pilot Week ---\n",
      "   Using data from 2025-07-01 00:00:00 to 2025-07-08 00:00:00 (exclusive)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tabulate import tabulate\n",
    "from dotenv import load_dotenv\n",
    "import snowflake.connector\n",
    "import sys\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# --- Your provided Snowflake connection details ---\n",
    "# This connection is for *reading* from INCREMENTALITY.\n",
    "# We no longer need to switch contexts, as we are not creating objects.\n",
    "conn = snowflake.connector.connect(\n",
    "    user=os.getenv('SNOWFLAKE_USER'),\n",
    "    password=os.getenv('SNOWFLAKE_PASSWORD'),\n",
    "    account=os.getenv('SNOWFLAKE_ACCOUNT'),\n",
    "    warehouse=os.getenv('SNOWFLAKE_WAREHOUSE', 'COMPUTE_WH'),\n",
    "    database='INCREMENTALITY', # Source database for Clicks and Purchases\n",
    "    schema='INCREMENTALITY_RESEARCH' # Schema for Clicks and Purchases\n",
    ")\n",
    "cursor = conn.cursor()\n",
    "\n",
    "def run_query(query):\n",
    "    try:\n",
    "        cursor.execute(query)\n",
    "        # For SELECT queries, fetch results\n",
    "        if cursor.description:\n",
    "            results = cursor.fetchall()\n",
    "            columns = [desc[0] for desc in cursor.description]\n",
    "            return pd.DataFrame(results, columns=columns)\n",
    "        # For DDL/DML, no results to fetch (though we won't be doing DDL now)\n",
    "        return pd.DataFrame()\n",
    "    except snowflake.connector.ProgrammingError as e:\n",
    "        print(f\"\\nERROR executing query:\\n{query}\\nDetails: {e}\")\n",
    "        raise # Re-raise the exception to stop execution on error\n",
    "\n",
    "def show_table(df, title=\"\"):\n",
    "    if title:\n",
    "        print(f\"\\n{title}\")\n",
    "        print(\"=\"*len(title))\n",
    "    print(tabulate(df, headers='keys', tablefmt='grid', showindex=False))\n",
    "\n",
    "print(\"✅ Connected to Snowflake\")\n",
    "\n",
    "# Define the pilot week for consistency\n",
    "PILOT_WEEK_START = '2025-07-01 00:00:00'\n",
    "PILOT_WEEK_END = '2025-07-08 00:00:00' # End is exclusive\n",
    "\n",
    "print(\"\\n--- Generating User-Vendor-Week Panel for Pilot Week ---\")\n",
    "print(f\"   Using data from {PILOT_WEEK_START} to {PILOT_WEEK_END} (exclusive)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Daily Funnel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "494ece4b2b3a498cbea8dc15b6047f2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching metrics for 2025-07-07:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Daily Metrics Summary for 2025-07-07\n",
      "====================================\n",
      "+-----------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------+---------------+\n",
      "| Metric                                                    | Description                                                                                                                             | Value         |\n",
      "+===========================================================+=========================================================================================================================================+===============+\n",
      "| Auctions: Total Search Events                             | The number of distinct user actions (e.g., a search query) that triggered an ad auction.                                                | 7,420,764     |\n",
      "+-----------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------+---------------+\n",
      "| Auctions: Total Bids Submitted by Vendors                 | The total number of ad bids submitted by all vendors across all auctions.                                                               | 268,105,571   |\n",
      "+-----------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------+---------------+\n",
      "| Auctions: Total Winning Bids (Impression Slots Won)       | The number of bids that won a slot and were eligible to be shown to the user.                                                           | 198,367,623   |\n",
      "+-----------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------+---------------+\n",
      "| Auctions: Average Ad Slots Filled per Search Event        | The average number of sponsored listings shown for a single user search event.                                                          | 26.73         |\n",
      "+-----------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------+---------------+\n",
      "| -----                                                     | -----                                                                                                                                   | -----         |\n",
      "+-----------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------+---------------+\n",
      "| Ad Delivery: Impression-to-Win Ratio (Show Rate)          | The ratio of actual impressions to winning bids. A value < 100% can indicate ads that won but were not rendered (e.g., below the fold). | 13.96%        |\n",
      "+-----------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------+---------------+\n",
      "| -----                                                     | -----                                                                                                                                   | -----         |\n",
      "+-----------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------+---------------+\n",
      "| Conversion Funnel: Total Impressions Delivered            | The total number of times an ad was actually displayed to a user.                                                                       | 27,692,988    |\n",
      "+-----------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------+---------------+\n",
      "| Conversion Funnel: Total Clicks on Ads                    | The total number of user clicks on the displayed ads.                                                                                   | 891,817       |\n",
      "+-----------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------+---------------+\n",
      "| Conversion Funnel: Total Unique Purchase Transactions     | The number of distinct purchase events (e.g., shopping carts) that occurred.                                                            | 111,910       |\n",
      "+-----------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------+---------------+\n",
      "| Conversion Funnel: Total Revenue (in dollars)             | The total gross merchandise value (GMV) generated from purchases, converted from cents to dollars.                                      | $4,925,990.00 |\n",
      "+-----------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------+---------------+\n",
      "| -----                                                     | -----                                                                                                                                   | -----         |\n",
      "+-----------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------+---------------+\n",
      "| Key Ratios: Click-Through Rate (CTR)                      | Impressions / Clicks. The percentage of displayed ads that were clicked on.                                                             | 3.22%         |\n",
      "+-----------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------+---------------+\n",
      "| Key Ratios: Conversion Rate (from Click to Purchase)      | Purchases / Clicks. The percentage of clicks that resulted in a purchase transaction.                                                   | 12.55%        |\n",
      "+-----------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------+---------------+\n",
      "| Key Ratios: Conversion Rate (from Impression to Purchase) | Purchases / Impressions. The overall efficiency of the ad funnel from view to purchase.                                                 | 0.404%        |\n",
      "+-----------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------+---------------+\n",
      "| Key Ratios: Average Purchase Value (Cart Size)            | Total Revenue / Total Purchases. The average dollar value of a single purchase transaction.                                             | $44.02        |\n",
      "+-----------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------+---------------+\n",
      "| -----                                                     | -----                                                                                                                                   | -----         |\n",
      "+-----------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------+---------------+\n",
      "| User Engagement: Approx. Unique Users with Impressions    | The approximate number of distinct users who were shown at least one ad.                                                                | ~ 691,948     |\n",
      "+-----------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------+---------------+\n",
      "| User Engagement: Approx. Unique Users with Clicks         | The approximate number of distinct users who clicked on at least one ad.                                                                | ~ 257,222     |\n",
      "+-----------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------+---------------+\n",
      "| User Engagement: Approx. Unique Users with Purchases      | The approximate number of distinct users who made a purchase.                                                                           | ~ 88,459      |\n",
      "+-----------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------+---------------+\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# --- Configuration ---\n",
    "TARGET_DATE = '2025-07-07'\n",
    "NEXT_DAY = (datetime.strptime(TARGET_DATE, '%Y-%m-%d') + timedelta(days=1)).strftime('%Y-%m-%d')\n",
    "DB_SCHEMA = \"INCREMENTALITY.INCREMENTALITY_RESEARCH\"\n",
    "\n",
    "# --- Define All Queries ---\n",
    "queries = {\n",
    "    \"auctions\": f\"SELECT COUNT(DISTINCT AUCTION_ID) AS val FROM {DB_SCHEMA}.AUCTIONS_USERS WHERE CREATED_AT >= '{TARGET_DATE}' AND CREATED_AT < '{NEXT_DAY}'\",\n",
    "    \"auction_results\": f\"SELECT COUNT(*) AS val FROM {DB_SCHEMA}.AUCTIONS_RESULTS WHERE CREATED_AT >= '{TARGET_DATE}' AND CREATED_AT < '{NEXT_DAY}'\",\n",
    "    \"winning_bids\": f\"SELECT COUNT(*) AS val FROM {DB_SCHEMA}.AUCTIONS_RESULTS WHERE IS_WINNER = TRUE AND CREATED_AT >= '{TARGET_DATE}' AND CREATED_AT < '{NEXT_DAY}'\",\n",
    "    \"impressions\": f\"SELECT COUNT(*) AS val FROM {DB_SCHEMA}.IMPRESSIONS WHERE OCCURRED_AT >= '{TARGET_DATE}' AND OCCURRED_AT < '{NEXT_DAY}'\",\n",
    "    \"clicks\": f\"SELECT COUNT(*) AS val FROM {DB_SCHEMA}.CLICKS WHERE OCCURRED_AT >= '{TARGET_DATE}' AND OCCURRED_AT < '{NEXT_DAY}'\",\n",
    "    \"purchases\": f\"SELECT COUNT(DISTINCT PURCHASE_ID) AS val FROM {DB_SCHEMA}.PURCHASES WHERE PURCHASED_AT >= '{TARGET_DATE}' AND PURCHASED_AT < '{NEXT_DAY}'\",\n",
    "    \"revenue_cents\": f\"SELECT COALESCE(SUM(QUANTITY * UNIT_PRICE), 0) AS val FROM {DB_SCHEMA}.PURCHASES WHERE PURCHASED_AT >= '{TARGET_DATE}' AND PURCHASED_AT < '{NEXT_DAY}'\",\n",
    "    \"distinct_users_impression\": f\"SELECT APPROX_COUNT_DISTINCT(USER_ID) AS val FROM {DB_SCHEMA}.IMPRESSIONS WHERE OCCURRED_AT >= '{TARGET_DATE}' AND OCCURRED_AT < '{NEXT_DAY}'\",\n",
    "    \"distinct_users_click\": f\"SELECT APPROX_COUNT_DISTINCT(USER_ID) AS val FROM {DB_SCHEMA}.CLICKS WHERE OCCURRED_AT >= '{TARGET_DATE}' AND OCCURRED_AT < '{NEXT_DAY}'\",\n",
    "    \"distinct_users_purchase\": f\"SELECT APPROX_COUNT_DISTINCT(USER_ID) AS val FROM {DB_SCHEMA}.PURCHASES WHERE PURCHASED_AT >= '{TARGET_DATE}' AND PURCHASED_AT < '{NEXT_DAY}'\"\n",
    "}\n",
    "\n",
    "# --- Execute Queries with TQDM Progress Bar ---\n",
    "try:\n",
    "    results = {}\n",
    "    for name, query in tqdm(queries.items(), desc=f\"Fetching metrics for {TARGET_DATE}\"):\n",
    "        # Assumes run_query is defined and returns a pandas DataFrame\n",
    "        results[name] = run_query(query).iloc[0, 0]\n",
    "\n",
    "    # --- Calculations for Summary Table ---\n",
    "    r = results\n",
    "    r['revenue_dollars'] = r.get('revenue_cents', 0) / 100\n",
    "    \n",
    "    slots_per_auction = (r['winning_bids'] / r['auctions']) if r['auctions'] > 0 else 0\n",
    "    show_rate = (r['impressions'] / r['winning_bids']) if r['winning_bids'] > 0 else 0\n",
    "    ctr = (r['clicks'] / r['impressions']) if r['impressions'] > 0 else 0\n",
    "    cvr_click = (r['purchases'] / r['clicks']) if r['clicks'] > 0 else 0\n",
    "    cvr_impression = (r['purchases'] / r['impressions']) if r['impressions'] > 0 else 0\n",
    "    avg_purchase_val = (r['revenue_dollars'] / r['purchases']) if r['purchases'] > 0 else 0\n",
    "    \n",
    "    # --- Create and Display Final Summary Table ---\n",
    "    summary_data = {\n",
    "        \"Metric\": [\n",
    "            \"Auctions: Total Search Events\", \"Auctions: Total Bids Submitted by Vendors\", \"Auctions: Total Winning Bids (Impression Slots Won)\", \"Auctions: Average Ad Slots Filled per Search Event\",\n",
    "            \"-----\", \"Ad Delivery: Impression-to-Win Ratio (Show Rate)\", \"-----\",\n",
    "            \"Conversion Funnel: Total Impressions Delivered\", \"Conversion Funnel: Total Clicks on Ads\", \"Conversion Funnel: Total Unique Purchase Transactions\", \"Conversion Funnel: Total Revenue (in dollars)\",\n",
    "            \"-----\", \"Key Ratios: Click-Through Rate (CTR)\", \"Key Ratios: Conversion Rate (from Click to Purchase)\", \"Key Ratios: Conversion Rate (from Impression to Purchase)\", \"Key Ratios: Average Purchase Value (Cart Size)\",\n",
    "            \"-----\", \"User Engagement: Approx. Unique Users with Impressions\", \"User Engagement: Approx. Unique Users with Clicks\", \"User Engagement: Approx. Unique Users with Purchases\"\n",
    "        ],\n",
    "        \"Description\": [\n",
    "            \"The number of distinct user actions (e.g., a search query) that triggered an ad auction.\", \"The total number of ad bids submitted by all vendors across all auctions.\",\n",
    "            \"The number of bids that won a slot and were eligible to be shown to the user.\", \"The average number of sponsored listings shown for a single user search event.\",\n",
    "            \"-----\", \"The ratio of actual impressions to winning bids. A value < 100% can indicate ads that won but were not rendered (e.g., below the fold).\", \"-----\",\n",
    "            \"The total number of times an ad was actually displayed to a user.\", \"The total number of user clicks on the displayed ads.\",\n",
    "            \"The number of distinct purchase events (e.g., shopping carts) that occurred.\", \"The total gross merchandise value (GMV) generated from purchases, converted from cents to dollars.\",\n",
    "            \"-----\", \"Impressions / Clicks. The percentage of displayed ads that were clicked on.\", \"Purchases / Clicks. The percentage of clicks that resulted in a purchase transaction.\",\n",
    "            \"Purchases / Impressions. The overall efficiency of the ad funnel from view to purchase.\", \"Total Revenue / Total Purchases. The average dollar value of a single purchase transaction.\",\n",
    "            \"-----\", \"The approximate number of distinct users who were shown at least one ad.\", \"The approximate number of distinct users who clicked on at least one ad.\", \"The approximate number of distinct users who made a purchase.\"\n",
    "        ],\n",
    "        \"Value\": [\n",
    "            f\"{r.get('auctions', 0):,}\", f\"{r.get('auction_results', 0):,}\", f\"{r.get('winning_bids', 0):,}\", f\"{slots_per_auction:.2f}\",\n",
    "            \"-----\", f\"{show_rate:.2%}\", \"-----\",\n",
    "            f\"{r.get('impressions', 0):,}\", f\"{r.get('clicks', 0):,}\", f\"{r.get('purchases', 0):,}\", f\"${r['revenue_dollars']:,.2f}\",\n",
    "            \"-----\", f\"{ctr:.2%}\", f\"{cvr_click:.2%}\", f\"{cvr_impression:.3%}\", f\"${avg_purchase_val:,.2f}\",\n",
    "            \"-----\", f\"~ {r.get('distinct_users_impression', 0):,}\", f\"~ {r.get('distinct_users_click', 0):,}\", f\"~ {r.get('distinct_users_purchase', 0):,}\"\n",
    "        ]\n",
    "    }\n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    \n",
    "    # Assumes show_table is defined for clean, untruncated output\n",
    "    show_table(summary_df, f\"Daily Metrics Summary for {TARGET_DATE}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nAn error occurred while running metrics queries: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vendor-Product Mapping Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Validating the Improved Product-Vendor Map ###\n",
      "\n",
      "--- Map Configuration ---\n",
      "  Mapping Source:      AUCTIONS_RESULTS table\n",
      "  Lookback Window:     30 days from 2025-07-07\n",
      "-------------------------\n",
      "\n",
      "--- Running query to calculate business coverage of the new map... ---\n",
      "\n",
      "Validation Facts for Improved Product-Vendor Map\n",
      "================================================\n",
      "+-------------------------------------------------+----------------+\n",
      "| Metric                                          | Value          |\n",
      "+=================================================+================+\n",
      "| 1. Grand Total Revenue (Entire Platform)        | $34,515,572.49 |\n",
      "+-------------------------------------------------+----------------+\n",
      "| 2. Revenue from Mapped Products (using new map) | $5,064,090.00  |\n",
      "+-------------------------------------------------+----------------+\n",
      "| 3. Revenue from Unmapped Products               | $29,451,482.49 |\n",
      "+-------------------------------------------------+----------------+\n",
      "| 4. % of Grand Total Revenue Covered by New Map  | 14.67%         |\n",
      "+-------------------------------------------------+----------------+\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# ==============================================================================\n",
    "# VALIDATING THE IMPROVED PRODUCT-VENDOR MAP (FACTS ONLY)\n",
    "# ==============================================================================\n",
    "print(\"### Validating the Improved Product-Vendor Map ###\")\n",
    "\n",
    "# --- Configuration ---\n",
    "PILOT_WEEK_START = '2025-07-07'\n",
    "PILOT_WEEK_END = '2025-07-14'\n",
    "LOOKBACK_DAYS = 30\n",
    "DB_SCHEMA = \"INCREMENTALITY.INCREMENTALITY_RESEARCH\"\n",
    "\n",
    "# --- Print a clear header with the exact filters being used ---\n",
    "print(\"\\n--- Map Configuration ---\")\n",
    "print(f\"  Mapping Source:      AUCTIONS_RESULTS table\")\n",
    "print(f\"  Lookback Window:     {LOOKBACK_DAYS} days from {PILOT_WEEK_START}\")\n",
    "print(\"-------------------------\")\n",
    "\n",
    "# --- SQL Query to Calculate Revenue Coverage with the New, Improved Map ---\n",
    "coverage_query_new_map = f\"\"\"\n",
    "WITH\n",
    "-- STEP 1: Build the new, more comprehensive product-vendor map\n",
    "improved_product_vendor_map AS (\n",
    "    SELECT PRODUCT_ID, VENDOR_ID\n",
    "    FROM (\n",
    "        SELECT\n",
    "            PRODUCT_ID, VENDOR_ID,\n",
    "            ROW_NUMBER() OVER (PARTITION BY PRODUCT_ID ORDER BY COUNT(*) DESC) as rn\n",
    "        FROM {DB_SCHEMA}.AUCTIONS_RESULTS\n",
    "        WHERE CREATED_AT >= DATEADD(day, -{LOOKBACK_DAYS}, '{PILOT_WEEK_START}')\n",
    "          AND CREATED_AT < '{PILOT_WEEK_END}'\n",
    "        GROUP BY 1, 2\n",
    "    )\n",
    "    WHERE rn = 1\n",
    "),\n",
    "-- STEP 2: Pre-aggregate all purchases to measure coverage\n",
    "all_purchases_in_period AS (\n",
    "    SELECT\n",
    "        PRODUCT_ID,\n",
    "        (QUANTITY * UNIT_PRICE) AS revenue_cents\n",
    "    FROM {DB_SCHEMA}.PURCHASES\n",
    "    WHERE PURCHASED_AT >= '{PILOT_WEEK_START}' AND PURCHASED_AT < '{PILOT_WEEK_END}'\n",
    ")\n",
    "-- STEP 3: Calculate the final coverage metrics\n",
    "SELECT\n",
    "    SUM(revenue_cents) / 100.0 AS grand_total_revenue,\n",
    "    SUM(IFF(\n",
    "        PRODUCT_ID IN (SELECT PRODUCT_ID FROM improved_product_vendor_map),\n",
    "        revenue_cents,\n",
    "        0\n",
    "    )) / 100.0 AS revenue_from_mapped_products\n",
    "FROM all_purchases_in_period;\n",
    "\"\"\"\n",
    "\n",
    "print(f\"\\n--- Running query to calculate business coverage of the new map... ---\")\n",
    "try:\n",
    "    # Assuming `run_query` and `show_table` are defined\n",
    "    df_coverage = run_query(coverage_query_new_map)\n",
    "    \n",
    "    if not df_coverage.empty:\n",
    "        # Extract facts\n",
    "        r = df_coverage.iloc[0]\n",
    "        grand_total = r['GRAND_TOTAL_REVENUE']\n",
    "        mapped_revenue = r['REVENUE_FROM_MAPPED_PRODUCTS']\n",
    "        unmapped_revenue = grand_total - mapped_revenue\n",
    "        coverage_percent = (mapped_revenue / grand_total * 100) if grand_total > 0 else 0\n",
    "        \n",
    "        # --- Create and Display Summary Table of Facts ---\n",
    "        summary_data = {\n",
    "            \"Metric\": [\n",
    "                \"1. Grand Total Revenue (Entire Platform)\",\n",
    "                \"2. Revenue from Mapped Products (using new map)\",\n",
    "                \"3. Revenue from Unmapped Products\",\n",
    "                \"4. % of Grand Total Revenue Covered by New Map\"\n",
    "            ],\n",
    "            \"Value\": [\n",
    "                f\"${grand_total:,.2f}\",\n",
    "                f\"${mapped_revenue:,.2f}\",\n",
    "                f\"${unmapped_revenue:,.2f}\",\n",
    "                f\"{coverage_percent:.2f}%\"\n",
    "            ]\n",
    "        }\n",
    "        summary_df = pd.DataFrame(summary_data)\n",
    "        \n",
    "        show_table(summary_df, \"Validation Facts for Improved Product-Vendor Map\")\n",
    "\n",
    "    else:\n",
    "        print(\"Warning: Could not retrieve coverage facts.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nAn error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vendor-Revenue CDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Analyzing Vendor Revenue CDF (Using Improved Map) ###\n",
      "\n",
      "--- Filters and Mapping Applied ---\n",
      "  Analysis Period: 2025-07-07 to 2025-07-14\n",
      "  Mapping Source:  AUCTIONS_RESULTS table\n",
      "  Lookback Window: 30 days\n",
      "-----------------------------------\n",
      "\n",
      "--- Running query to generate the vendor revenue CDF for MAPPED REVENUE... ---\n",
      "\n",
      "Distribution of Mapped Vendor Revenue\n",
      "=====================================\n",
      "+--------------------------------------+------------------------+---------------------------+----------------------------+--------------------------+---------------------------------------+\n",
      "|   Revenue Tier (% of Mapped Revenue) | Vendors in this Tier   | Cumulative Vendor Count   | Revenue in this Tier ($)   | Cumulative Revenue ($)   | Cumulative % of All Selling Vendors   |\n",
      "+======================================+========================+===========================+============================+==========================+=======================================+\n",
      "|                                   10 | 39                     | 39                        | $504,297.00                | $504,297.00              | 0.13%                                 |\n",
      "+--------------------------------------+------------------------+---------------------------+----------------------------+--------------------------+---------------------------------------+\n",
      "|                                   20 | 175                    | 214                       | $507,714.00                | $1,012,011.00            | 0.73%                                 |\n",
      "+--------------------------------------+------------------------+---------------------------+----------------------------+--------------------------+---------------------------------------+\n",
      "|                                   30 | 350                    | 564                       | $507,163.40                | $1,519,174.40            | 1.91%                                 |\n",
      "+--------------------------------------+------------------------+---------------------------+----------------------------+--------------------------+---------------------------------------+\n",
      "|                                   40 | 575                    | 1,139                     | $504,796.00                | $2,023,970.40            | 3.87%                                 |\n",
      "+--------------------------------------+------------------------+---------------------------+----------------------------+--------------------------+---------------------------------------+\n",
      "|                                   50 | 881                    | 2,020                     | $506,296.00                | $2,530,266.40            | 6.86%                                 |\n",
      "+--------------------------------------+------------------------+---------------------------+----------------------------+--------------------------+---------------------------------------+\n",
      "|                                   60 | 1,297                  | 3,317                     | $503,799.00                | $3,034,065.40            | 11.26%                                |\n",
      "+--------------------------------------+------------------------+---------------------------+----------------------------+--------------------------+---------------------------------------+\n",
      "|                                   70 | 1,953                  | 5,270                     | $506,671.60                | $3,540,737.00            | 17.89%                                |\n",
      "+--------------------------------------+------------------------+---------------------------+----------------------------+--------------------------+---------------------------------------+\n",
      "|                                   80 | 2,953                  | 8,223                     | $501,639.00                | $4,042,376.00            | 27.91%                                |\n",
      "+--------------------------------------+------------------------+---------------------------+----------------------------+--------------------------+---------------------------------------+\n",
      "|                                   90 | 5,189                  | 13,412                    | $510,802.00                | $4,553,178.00            | 45.53%                                |\n",
      "+--------------------------------------+------------------------+---------------------------+----------------------------+--------------------------+---------------------------------------+\n",
      "|                                  100 | 16,048                 | 29,460                    | $510,912.00                | $5,064,090.00            | 100.00%                               |\n",
      "+--------------------------------------+------------------------+---------------------------+----------------------------+--------------------------+---------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# ==============================================================================\n",
    "# VENDOR REVENUE CDF (Using the Improved, Robust Map)\n",
    "# ==============================================================================\n",
    "print(\"### Analyzing Vendor Revenue CDF (Using Improved Map) ###\")\n",
    "\n",
    "# --- Configuration ---\n",
    "PILOT_WEEK_START = '2025-07-07'\n",
    "PILOT_WEEK_END = '2025-07-14'\n",
    "LOOKBACK_DAYS = 30\n",
    "DB_SCHEMA = \"INCREMENTALITY.INCREMENTALITY_RESEARCH\"\n",
    "\n",
    "# --- Print a clear header with the exact filters being used ---\n",
    "print(\"\\n--- Filters and Mapping Applied ---\")\n",
    "print(f\"  Analysis Period: {PILOT_WEEK_START} to {PILOT_WEEK_END}\")\n",
    "print(f\"  Mapping Source:  AUCTIONS_RESULTS table\")\n",
    "print(f\"  Lookback Window: {LOOKBACK_DAYS} days\")\n",
    "print(\"-----------------------------------\")\n",
    "\n",
    "# --- CORRECTED SQL Query to Generate the Full CDF Table ---\n",
    "vendor_cdf_query = f\"\"\"\n",
    "WITH\n",
    "-- STEP 1: Build the new, more comprehensive product-vendor map (our best available)\n",
    "improved_product_vendor_map AS (\n",
    "    SELECT PRODUCT_ID, VENDOR_ID\n",
    "    FROM (\n",
    "        SELECT PRODUCT_ID, VENDOR_ID, ROW_NUMBER() OVER (PARTITION BY PRODUCT_ID ORDER BY COUNT(*) DESC) as rn\n",
    "        FROM {DB_SCHEMA}.AUCTIONS_RESULTS\n",
    "        WHERE CREATED_AT >= DATEADD(day, -{LOOKBACK_DAYS}, '{PILOT_WEEK_START}')\n",
    "          AND CREATED_AT < '{PILOT_WEEK_END}'\n",
    "        GROUP BY 1, 2\n",
    "    )\n",
    "    WHERE rn = 1\n",
    "),\n",
    "-- STEP 2: Calculate total revenue per vendor ONLY for the revenue we can map\n",
    "vendor_sales AS (\n",
    "    SELECT\n",
    "        pv.VENDOR_ID,\n",
    "        COALESCE(SUM(p.QUANTITY * p.UNIT_PRICE), 0) AS total_revenue_cents\n",
    "    FROM {DB_SCHEMA}.PURCHASES p\n",
    "    -- Use an INNER JOIN to consider only the sales we can successfully map\n",
    "    INNER JOIN improved_product_vendor_map pv ON p.PRODUCT_ID = pv.PRODUCT_ID\n",
    "    WHERE p.PURCHASED_AT >= '{PILOT_WEEK_START}' AND p.PURCHASED_AT < '{PILOT_WEEK_END}'\n",
    "    GROUP BY 1\n",
    "),\n",
    "-- 3. Calculate cumulative distribution and assign percentile buckets\n",
    "cumulative_sales AS (\n",
    "    SELECT\n",
    "        VENDOR_ID, total_revenue_cents,\n",
    "        SUM(total_revenue_cents) OVER (ORDER BY total_revenue_cents DESC) as cumulative_revenue_cents,\n",
    "        CEIL(SUM(total_revenue_cents) OVER (ORDER BY total_revenue_cents DESC) * 10.0 / SUM(total_revenue_cents) OVER ()) as percentile_bucket\n",
    "    FROM vendor_sales WHERE VENDOR_ID IS NOT NULL AND total_revenue_cents > 0\n",
    "),\n",
    "-- 4. Group by the buckets\n",
    "bucketed_summary AS (\n",
    "    SELECT\n",
    "        percentile_bucket,\n",
    "        COUNT(VENDOR_ID) AS vendors_in_bucket,\n",
    "        SUM(total_revenue_cents) as revenue_in_bucket_cents\n",
    "    FROM cumulative_sales GROUP BY 1\n",
    ")\n",
    "-- 5. Final aggregation to create a true cumulative view\n",
    "SELECT\n",
    "    bs.percentile_bucket * 10 AS cumulative_percent_revenue,\n",
    "    bs.vendors_in_bucket,\n",
    "    SUM(bs.vendors_in_bucket) OVER (ORDER BY bs.percentile_bucket) as cumulative_vendor_count,\n",
    "    bs.revenue_in_bucket_cents,\n",
    "    SUM(bs.revenue_in_bucket_cents) OVER (ORDER BY bs.percentile_bucket) as cumulative_revenue_cents,\n",
    "    SUM(bs.vendors_in_bucket) OVER (ORDER BY bs.percentile_bucket) * 100.0 / (SELECT COUNT(*) FROM vendor_sales WHERE VENDOR_ID IS NOT NULL AND total_revenue_cents > 0) as cumulative_percent_of_vendors\n",
    "FROM bucketed_summary bs\n",
    "ORDER BY 1;\n",
    "\"\"\"\n",
    "\n",
    "print(f\"\\n--- Running query to generate the vendor revenue CDF for MAPPED REVENUE... ---\")\n",
    "try:\n",
    "    df_cdf = run_query(vendor_cdf_query)\n",
    "    \n",
    "    if not df_cdf.empty:\n",
    "        # Define descriptive column names for the final, corrected table\n",
    "        df_cdf.columns = [\n",
    "            'Revenue Tier (% of Mapped Revenue)',\n",
    "            'Vendors in this Tier',\n",
    "            'Cumulative Vendor Count',\n",
    "            'Revenue in this Tier ($)',\n",
    "            'Cumulative Revenue ($)',\n",
    "            'Cumulative % of All Selling Vendors'\n",
    "        ]\n",
    "        \n",
    "        # Format all columns for clear, readable output\n",
    "        df_cdf['Vendors in this Tier'] = df_cdf['Vendors in this Tier'].apply(lambda x: f\"{x:,}\")\n",
    "        df_cdf['Cumulative Vendor Count'] = df_cdf['Cumulative Vendor Count'].apply(lambda x: f\"{x:,}\")\n",
    "        df_cdf['Revenue in this Tier ($)'] = (df_cdf['Revenue in this Tier ($)'] / 100.0).apply(lambda x: f\"${x:,.2f}\")\n",
    "        df_cdf['Cumulative Revenue ($)'] = (df_cdf['Cumulative Revenue ($)'] / 100.0).apply(lambda x: f\"${x:,.2f}\")\n",
    "        df_cdf['Cumulative % of All Selling Vendors'] = df_cdf['Cumulative % of All Selling Vendors'].apply(lambda x: f\"{x:.2f}%\")\n",
    "        \n",
    "        show_table(df_cdf, \"Distribution of Mapped Vendor Revenue\")\n",
    "    else:\n",
    "        print(\"Warning: Could not generate the CDF table.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nAn error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User-Revenue CDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Analyzing User Revenue CDF for Mappable Purchases ###\n",
      "\n",
      "--- Filters and Mapping Applied ---\n",
      "  Analysis Period: 2025-07-07 to 2025-07-14\n",
      "  Revenue Base:    Only revenue from purchases that can be mapped to a vendor.\n",
      "  Mapping Source:  AUCTIONS_RESULTS table (30-day lookback)\n",
      "-----------------------------------------------------\n",
      "\n",
      "--- Running query to generate the full user revenue CDF... ---\n",
      "\n",
      "User Revenue Distribution for Mappable Purchases\n",
      "================================================\n",
      "+--------------------------------------+----------------------+-------------------------+----------------------------+--------------------------+----------------------------------------+\n",
      "|   Revenue Tier (% of Mapped Revenue) | Users in this Tier   | Cumulative User Count   | Revenue in this Tier ($)   | Cumulative Revenue ($)   | Cumulative % of All Purchasing Users   |\n",
      "+======================================+======================+=========================+============================+==========================+========================================+\n",
      "|                                   10 | 312                  | 312                     | $505,808.00                | $505,808.00              | 0.33%                                  |\n",
      "+--------------------------------------+----------------------+-------------------------+----------------------------+--------------------------+----------------------------------------+\n",
      "|                                   20 | 1,005                | 1,317                   | $505,433.00                | $1,011,241.00            | 1.38%                                  |\n",
      "+--------------------------------------+----------------------+-------------------------+----------------------------+--------------------------+----------------------------------------+\n",
      "|                                   30 | 1,888                | 3,205                   | $506,032.00                | $1,517,273.00            | 3.35%                                  |\n",
      "+--------------------------------------+----------------------+-------------------------+----------------------------+--------------------------+----------------------------------------+\n",
      "|                                   40 | 2,984                | 6,189                   | $495,805.00                | $2,013,078.00            | 6.48%                                  |\n",
      "+--------------------------------------+----------------------+-------------------------+----------------------------+--------------------------+----------------------------------------+\n",
      "|                                   50 | 4,637                | 10,826                  | $515,084.00                | $2,528,162.00            | 11.33%                                 |\n",
      "+--------------------------------------+----------------------+-------------------------+----------------------------+--------------------------+----------------------------------------+\n",
      "|                                   60 | 6,447                | 17,273                  | $501,248.00                | $3,029,410.00            | 18.07%                                 |\n",
      "+--------------------------------------+----------------------+-------------------------+----------------------------+--------------------------+----------------------------------------+\n",
      "|                                   70 | 8,786                | 26,059                  | $490,583.00                | $3,519,993.00            | 27.27%                                 |\n",
      "+--------------------------------------+----------------------+-------------------------+----------------------------+--------------------------+----------------------------------------+\n",
      "|                                   80 | 13,085               | 39,144                  | $521,199.00                | $4,041,192.00            | 40.96%                                 |\n",
      "+--------------------------------------+----------------------+-------------------------+----------------------------+--------------------------+----------------------------------------+\n",
      "|                                   90 | 18,799               | 57,943                  | $505,526.40                | $4,546,718.40            | 60.63%                                 |\n",
      "+--------------------------------------+----------------------+-------------------------+----------------------------+--------------------------+----------------------------------------+\n",
      "|                                  100 | 37,621               | 95,564                  | $517,371.60                | $5,064,090.00            | 100.00%                                |\n",
      "+--------------------------------------+----------------------+-------------------------+----------------------------+--------------------------+----------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# ==============================================================================\n",
    "# USER REVENUE CDF (Focused on Mappable Revenue)\n",
    "# ==============================================================================\n",
    "print(\"### Analyzing User Revenue CDF for Mappable Purchases ###\")\n",
    "\n",
    "# --- Configuration ---\n",
    "PILOT_WEEK_START = '2025-07-07'\n",
    "PILOT_WEEK_END = '2025-07-14'\n",
    "LOOKBACK_DAYS = 30\n",
    "DB_SCHEMA = \"INCREMENTALITY.INCREMENTALITY_RESEARCH\"\n",
    "\n",
    "# --- Print a clear header with the exact filters being used ---\n",
    "print(\"\\n--- Filters and Mapping Applied ---\")\n",
    "print(f\"  Analysis Period: {PILOT_WEEK_START} to {PILOT_WEEK_END}\")\n",
    "print(f\"  Revenue Base:    Only revenue from purchases that can be mapped to a vendor.\")\n",
    "print(f\"  Mapping Source:  AUCTIONS_RESULTS table ({LOOKBACK_DAYS}-day lookback)\")\n",
    "print(\"-----------------------------------------------------\")\n",
    "\n",
    "# --- CORRECTED SQL Query to Generate the Full User Revenue CDF Table ---\n",
    "user_cdf_query = f\"\"\"\n",
    "WITH\n",
    "-- 1. Build the best possible product-vendor map\n",
    "improved_product_vendor_map AS (\n",
    "    SELECT PRODUCT_ID, VENDOR_ID FROM (\n",
    "        SELECT PRODUCT_ID, VENDOR_ID, ROW_NUMBER() OVER (PARTITION BY PRODUCT_ID ORDER BY COUNT(*) DESC) as rn\n",
    "        FROM {DB_SCHEMA}.AUCTIONS_RESULTS\n",
    "        WHERE CREATED_AT >= DATEADD(day, -{LOOKBACK_DAYS}, '{PILOT_WEEK_START}')\n",
    "          AND CREATED_AT < '{PILOT_WEEK_END}'\n",
    "        GROUP BY 1, 2\n",
    "    ) WHERE rn = 1\n",
    "),\n",
    "-- 2. Calculate total MAPPABLE revenue per user\n",
    "user_sales AS (\n",
    "    SELECT\n",
    "        p.USER_ID,\n",
    "        COALESCE(SUM(p.QUANTITY * p.UNIT_PRICE), 0) AS total_revenue_cents\n",
    "    FROM {DB_SCHEMA}.PURCHASES p\n",
    "    -- INNER JOIN ensures we only count revenue from products we can map\n",
    "    INNER JOIN improved_product_vendor_map pv ON p.PRODUCT_ID = pv.PRODUCT_ID\n",
    "    WHERE p.PURCHASED_AT >= '{PILOT_WEEK_START}' AND p.PURCHASED_AT < '{PILOT_WEEK_END}'\n",
    "    GROUP BY 1\n",
    "),\n",
    "-- 3. Calculate cumulative distribution and assign percentile buckets\n",
    "cumulative_sales AS (\n",
    "    SELECT\n",
    "        USER_ID, total_revenue_cents,\n",
    "        SUM(total_revenue_cents) OVER (ORDER BY total_revenue_cents DESC) as cumulative_revenue_cents,\n",
    "        CEIL(SUM(total_revenue_cents) OVER (ORDER BY total_revenue_cents DESC) * 10.0 / SUM(total_revenue_cents) OVER ()) as percentile_bucket\n",
    "    FROM user_sales WHERE total_revenue_cents > 0\n",
    "),\n",
    "-- 4. Group by the buckets\n",
    "bucketed_summary AS (\n",
    "    SELECT\n",
    "        percentile_bucket,\n",
    "        COUNT(USER_ID) AS users_in_bucket,\n",
    "        SUM(total_revenue_cents) as revenue_in_bucket_cents\n",
    "    FROM cumulative_sales GROUP BY 1\n",
    ")\n",
    "-- 5. Final aggregation to create a true cumulative view\n",
    "SELECT\n",
    "    bs.percentile_bucket * 10 AS cumulative_percent_revenue,\n",
    "    bs.users_in_bucket,\n",
    "    SUM(bs.users_in_bucket) OVER (ORDER BY bs.percentile_bucket) as cumulative_user_count,\n",
    "    bs.revenue_in_bucket_cents,\n",
    "    SUM(bs.revenue_in_bucket_cents) OVER (ORDER BY bs.percentile_bucket) as cumulative_revenue_cents,\n",
    "    SUM(bs.users_in_bucket) OVER (ORDER BY bs.percentile_bucket) * 100.0 / (SELECT COUNT(*) FROM user_sales WHERE total_revenue_cents > 0) as cumulative_percent_of_users\n",
    "FROM bucketed_summary bs\n",
    "ORDER BY 1;\n",
    "\"\"\"\n",
    "\n",
    "print(f\"\\n--- Running query to generate the full user revenue CDF... ---\")\n",
    "try:\n",
    "    df_cdf_user = run_query(user_cdf_query)\n",
    "    \n",
    "    if not df_cdf_user.empty:\n",
    "        # Define descriptive column names for the final table\n",
    "        df_cdf_user.columns = [\n",
    "            'Revenue Tier (% of Mapped Revenue)',\n",
    "            'Users in this Tier',\n",
    "            'Cumulative User Count',\n",
    "            'Revenue in this Tier ($)',\n",
    "            'Cumulative Revenue ($)',\n",
    "            'Cumulative % of All Purchasing Users'\n",
    "        ]\n",
    "        \n",
    "        # Format all columns for clear, readable output\n",
    "        df_cdf_user['Users in this Tier'] = df_cdf_user['Users in this Tier'].apply(lambda x: f\"{x:,}\")\n",
    "        df_cdf_user['Cumulative User Count'] = df_cdf_user['Cumulative User Count'].apply(lambda x: f\"{x:,}\")\n",
    "        df_cdf_user['Revenue in this Tier ($)'] = (df_cdf_user['Revenue in this Tier ($)'] / 100.0).apply(lambda x: f\"${x:,.2f}\")\n",
    "        df_cdf_user['Cumulative Revenue ($)'] = (df_cdf_user['Cumulative Revenue ($)'] / 100.0).apply(lambda x: f\"${x:,.2f}\")\n",
    "        df_cdf_user['Cumulative % of All Purchasing Users'] = df_cdf_user['Cumulative % of All Purchasing Users'].apply(lambda x: f\"{x:.2f}%\")\n",
    "        \n",
    "        show_table(df_cdf_user, \"User Revenue Distribution for Mappable Purchases\")\n",
    "    else:\n",
    "        print(\"Warning: Could not generate the CDF table for users.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nAn error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User-Vendor Panel Shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Analyzing the Business Coverage of the 'Power Panel' ###\n",
      "\n",
      "--- Population & Universe Definitions ---\n",
      "  Analysis Period: 2025-07-07 to 2025-07-14\n",
      "  User Population: Top 100,000 users by total spend (across all purchases).\n",
      "  Vendor Universe: Top 5,000 vendors by total MAPPED sales revenue.\n",
      "  Mapping Source:  AUCTIONS_RESULTS table (30-day lookback)\n",
      "-----------------------------------------------------\n",
      "\n",
      "--- Running query to calculate business coverage... ---\n",
      "\n",
      "Business Coverage of the 'Power Panel' Population\n",
      "=================================================\n",
      "+------------------------------------------------------------+----------------+\n",
      "| Metric                                                     | Value          |\n",
      "+============================================================+================+\n",
      "| 1. Grand Total Revenue (Entire Platform)                   | $34,515,572.49 |\n",
      "+------------------------------------------------------------+----------------+\n",
      "| 2. Total Mapped Revenue (Ad-Engaged Ecosystem)             | $5,064,090.00  |\n",
      "+------------------------------------------------------------+----------------+\n",
      "| -> % of Grand Total                                        | 14.67%         |\n",
      "+------------------------------------------------------------+----------------+\n",
      "| 3. Revenue from Top 100k Users buying from Top 5k Vendors  | $2,683,376.20  |\n",
      "+------------------------------------------------------------+----------------+\n",
      "| -> % of Grand Total (Panel's overall business share)       | 7.77%          |\n",
      "+------------------------------------------------------------+----------------+\n",
      "| -> % of Mapped Revenue (Panel's share of the ad ecosystem) | 52.99%         |\n",
      "+------------------------------------------------------------+----------------+\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# ==============================================================================\n",
    "# \"BUSINESS COVERAGE\" ANALYSIS (Using Best Available Map)\n",
    "# ==============================================================================\n",
    "print(\"### Analyzing the Business Coverage of the 'Power Panel' ###\")\n",
    "\n",
    "# --- Configuration ---\n",
    "PILOT_WEEK_START = '2025-07-07'\n",
    "PILOT_WEEK_END = '2025-07-14'\n",
    "TOP_N_VENDORS = 5000\n",
    "TOP_N_USERS = 100000\n",
    "LOOKBACK_DAYS = 30\n",
    "DB_SCHEMA = \"INCREMENTALITY.INCREMENTALITY_RESEARCH\"\n",
    "\n",
    "# --- Print a clear header with the exact filters being used ---\n",
    "print(\"\\n--- Population & Universe Definitions ---\")\n",
    "print(f\"  Analysis Period: {PILOT_WEEK_START} to {PILOT_WEEK_END}\")\n",
    "print(f\"  User Population: Top {TOP_N_USERS:,} users by total spend (across all purchases).\")\n",
    "print(f\"  Vendor Universe: Top {TOP_N_VENDORS:,} vendors by total MAPPED sales revenue.\")\n",
    "print(f\"  Mapping Source:  AUCTIONS_RESULTS table ({LOOKBACK_DAYS}-day lookback)\")\n",
    "print(\"-----------------------------------------------------\")\n",
    "\n",
    "# --- SQL Query to Calculate Revenue Coverage with the Improved Map ---\n",
    "coverage_query = f\"\"\"\n",
    "WITH\n",
    "-- 1. Build our best possible product-vendor map\n",
    "improved_product_vendor_map AS (\n",
    "    SELECT PRODUCT_ID, VENDOR_ID FROM (\n",
    "        SELECT PRODUCT_ID, VENDOR_ID, ROW_NUMBER() OVER (PARTITION BY PRODUCT_ID ORDER BY COUNT(*) DESC) as rn\n",
    "        FROM {DB_SCHEMA}.AUCTIONS_RESULTS\n",
    "        WHERE CREATED_AT >= DATEADD(day, -{LOOKBACK_DAYS}, '{PILOT_WEEK_START}') AND CREATED_AT < '{PILOT_WEEK_END}'\n",
    "        GROUP BY 1, 2\n",
    "    ) WHERE rn = 1\n",
    "),\n",
    "-- 2. Get the list of Top Users by their total spend (on all products)\n",
    "top_users_cte AS (\n",
    "    SELECT USER_ID FROM {DB_SCHEMA}.PURCHASES\n",
    "    WHERE PURCHASED_AT >= '{PILOT_WEEK_START}' AND PURCHASED_AT < '{PILOT_WEEK_END}'\n",
    "    GROUP BY 1 ORDER BY SUM(QUANTITY * UNIT_PRICE) DESC LIMIT {TOP_N_USERS}\n",
    "),\n",
    "-- 3. Get the list of Top Vendors by their total MAPPED sales revenue\n",
    "top_vendors_cte AS (\n",
    "    SELECT pv.VENDOR_ID\n",
    "    FROM {DB_SCHEMA}.PURCHASES p\n",
    "    INNER JOIN improved_product_vendor_map pv ON p.PRODUCT_ID = pv.PRODUCT_ID\n",
    "    WHERE p.PURCHASED_AT >= '{PILOT_WEEK_START}' AND p.PURCHASED_AT < '{PILOT_WEEK_END}'\n",
    "    GROUP BY 1 ORDER BY SUM(p.QUANTITY * p.UNIT_PRICE) DESC LIMIT {TOP_N_VENDORS}\n",
    "),\n",
    "-- 4. Pre-aggregate all purchases with mapped vendors\n",
    "all_purchases_mapped AS (\n",
    "    SELECT\n",
    "        p.USER_ID,\n",
    "        pv.VENDOR_ID, -- Note: This will be NULL for unmapped products\n",
    "        (p.QUANTITY * p.UNIT_PRICE) AS revenue_cents\n",
    "    FROM {DB_SCHEMA}.PURCHASES p\n",
    "    LEFT JOIN improved_product_vendor_map pv ON p.PRODUCT_ID = pv.PRODUCT_ID\n",
    "    WHERE p.PURCHASED_AT >= '{PILOT_WEEK_START}' AND p.PURCHASED_AT < '{PILOT_WEEK_END}'\n",
    ")\n",
    "-- 5. Calculate the final coverage metrics\n",
    "SELECT\n",
    "    -- Grand total revenue (mapped + unmapped)\n",
    "    SUM(revenue_cents) / 100.0 AS grand_total_revenue,\n",
    "    -- Total MAPPED revenue\n",
    "    SUM(IFF(VENDOR_ID IS NOT NULL, revenue_cents, 0)) / 100.0 AS total_mapped_revenue,\n",
    "    -- Revenue from Top Users (across all their purchases)\n",
    "    SUM(IFF(USER_ID IN (SELECT USER_ID FROM top_users_cte), revenue_cents, 0)) / 100.0 AS revenue_from_top_users,\n",
    "    -- Revenue FOR Top Vendors (only their mapped purchases)\n",
    "    SUM(IFF(VENDOR_ID IN (SELECT VENDOR_ID FROM top_vendors_cte), revenue_cents, 0)) / 100.0 AS revenue_for_top_vendors,\n",
    "    -- Revenue from Top Users buying from Top Vendors (only mapped purchases)\n",
    "    SUM(IFF(USER_ID IN (SELECT USER_ID FROM top_users_cte) AND VENDOR_ID IN (SELECT VENDOR_ID FROM top_vendors_cte), revenue_cents, 0)) / 100.0 AS revenue_top_user_top_vendor\n",
    "FROM all_purchases_mapped;\n",
    "\"\"\"\n",
    "\n",
    "print(f\"\\n--- Running query to calculate business coverage... ---\")\n",
    "try:\n",
    "    df_coverage = run_query(coverage_query)\n",
    "    \n",
    "    if not df_coverage.empty:\n",
    "        r = df_coverage.iloc[0]\n",
    "        grand_total = r['GRAND_TOTAL_REVENUE']\n",
    "        mapped_total = r['TOTAL_MAPPED_REVENUE']\n",
    "        \n",
    "        # Calculate percentages against BOTH grand total and mapped total\n",
    "        panel_coverage_of_grand_total = (r['REVENUE_TOP_USER_TOP_VENDOR'] / grand_total * 100) if grand_total > 0 else 0\n",
    "        panel_coverage_of_mapped_total = (r['REVENUE_TOP_USER_TOP_VENDOR'] / mapped_total * 100) if mapped_total > 0 else 0\n",
    "\n",
    "        summary_data = {\n",
    "            \"Metric\": [\n",
    "                \"1. Grand Total Revenue (Entire Platform)\",\n",
    "                \"2. Total Mapped Revenue (Ad-Engaged Ecosystem)\",\n",
    "                \"   -> % of Grand Total\",\n",
    "                \"3. Revenue from Top 100k Users buying from Top 5k Vendors\",\n",
    "                \"   -> % of Grand Total (Panel's overall business share)\",\n",
    "                \"   -> % of Mapped Revenue (Panel's share of the ad ecosystem)\"\n",
    "            ],\n",
    "            \"Value\": [\n",
    "                f\"${grand_total:,.2f}\",\n",
    "                f\"${mapped_total:,.2f}\",\n",
    "                f\"{(mapped_total / grand_total * 100):.2f}%\",\n",
    "                f\"${r['REVENUE_TOP_USER_TOP_VENDOR']:,.2f}\",\n",
    "                f\"{panel_coverage_of_grand_total:.2f}%\",\n",
    "                f\"{panel_coverage_of_mapped_total:.2f}%\"\n",
    "            ]\n",
    "        }\n",
    "        summary_df = pd.DataFrame(summary_data)\n",
    "        show_table(summary_df, \"Business Coverage of the 'Power Panel' Population\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nAn error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Sizing the Final Panel based on Pre-Treatment Cohorts ###\n",
      "\n",
      "--- Population & Universe Definitions ---\n",
      "  Pre-Treatment Period (for defining cohorts): 2025-06-07 to 2025-07-07\n",
      "  Analysis Period (for measuring eligibility): 2025-07-07 to 2025-07-14\n",
      "  User Cohort: Top 100,000 users by spend in pre-treatment period.\n",
      "  Vendor Cohort: Top 4,000 vendors by sales in pre-treatment period.\n",
      "-----------------------------------------------------\n",
      "\n",
      "--- Running query to find the count of eligible users from the pre-treatment cohort... ---\n",
      "\n",
      "Final Panel Sizing Facts (Based on Pre-Treatment Cohorts)\n",
      "=========================================================\n",
      "+-----------------------------------------------------------+----------------------------------------------------------------------------------------+-------------+\n",
      "| Fact                                                      | Description                                                                            | Value       |\n",
      "+===========================================================+========================================================================================+=============+\n",
      "| 1. Pre-Treatment User Cohort Size                         | Top 100,000 spenders from the month prior to the pilot.                                | 100,000     |\n",
      "+-----------------------------------------------------------+----------------------------------------------------------------------------------------+-------------+\n",
      "| 2. Eligible Users from Cohort (Impressions in pilot week) | The number of users from the pre-defined cohort who were active during the pilot week. | 72,922      |\n",
      "+-----------------------------------------------------------+----------------------------------------------------------------------------------------+-------------+\n",
      "| 3. Pre-Treatment Vendor Cohort Size                       | Top 4,000 sellers from the month prior to the pilot.                                   | 4,000       |\n",
      "+-----------------------------------------------------------+----------------------------------------------------------------------------------------+-------------+\n",
      "| 4. Final Panel Rows (Eligible Users x Top Vendors)        | The final row count for the panel before negative sampling.                            | 291,688,000 |\n",
      "+-----------------------------------------------------------+----------------------------------------------------------------------------------------+-------------+\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# ==============================================================================\n",
    "# FINAL PANEL SIZING (Using Pre-Treatment Population Definition)\n",
    "# ==============================================================================\n",
    "print(\"### Sizing the Final Panel based on Pre-Treatment Cohorts ###\")\n",
    "\n",
    "# --- Configuration ---\n",
    "PILOT_WEEK_START = '2025-07-07'\n",
    "PILOT_WEEK_END = '2025-07-14'\n",
    "# Define the 30-day pre-treatment period\n",
    "PRE_TREATMENT_END = PILOT_WEEK_START\n",
    "PRE_TREATMENT_START = (datetime.strptime(PRE_TREATMENT_END, '%Y-%m-%d') - timedelta(days=30)).strftime('%Y-%m-%d')\n",
    "\n",
    "TOP_N_VENDORS = 4000\n",
    "TOP_N_USERS = 100000\n",
    "DB_SCHEMA = \"INCREMENTALITY.INCREMENTALITY_RESEARCH\"\n",
    "\n",
    "# --- Print a clear header with the exact filters being used ---\n",
    "print(\"\\n--- Population & Universe Definitions ---\")\n",
    "print(f\"  Pre-Treatment Period (for defining cohorts): {PRE_TREATMENT_START} to {PRE_TREATMENT_END}\")\n",
    "print(f\"  Analysis Period (for measuring eligibility): {PILOT_WEEK_START} to {PILOT_WEEK_END}\")\n",
    "print(f\"  User Cohort: Top {TOP_N_USERS:,} users by spend in pre-treatment period.\")\n",
    "print(f\"  Vendor Cohort: Top {TOP_N_VENDORS:,} vendors by sales in pre-treatment period.\")\n",
    "print(\"-----------------------------------------------------\")\n",
    "\n",
    "# --- SQL Query to Find the \"Eligible Top Users\" from our Pre-Defined Cohort ---\n",
    "query_eligible_top_users = f\"\"\"\n",
    "WITH\n",
    "-- Step 1: Identify the top 100,000 users by total spend in the PRE-TREATMENT month\n",
    "top_users_pretreatment_cte AS (\n",
    "    SELECT USER_ID\n",
    "    FROM {DB_SCHEMA}.PURCHASES\n",
    "    WHERE PURCHASED_AT >= '{PRE_TREATMENT_START}' AND PURCHASED_AT < '{PRE_TREATMENT_END}'\n",
    "    GROUP BY USER_ID\n",
    "    ORDER BY SUM(QUANTITY * UNIT_PRICE) DESC\n",
    "    LIMIT {TOP_N_USERS}\n",
    ")\n",
    "-- Step 2: From that pre-defined cohort, count how many were \"eligible\" (had impressions)\n",
    "-- during the ACTUAL PILOT WEEK.\n",
    "SELECT\n",
    "    COUNT(DISTINCT i.USER_ID) as eligible_top_user_count\n",
    "FROM {DB_SCHEMA}.IMPRESSIONS i\n",
    "-- We only care about users who are in our pre-defined cohort\n",
    "INNER JOIN top_users_pretreatment_cte top_users ON i.USER_ID = top_users.USER_ID\n",
    "WHERE i.OCCURRED_AT >= '{PILOT_WEEK_START}' AND i.OCCURRED_AT < '{PILOT_WEEK_END}';\n",
    "\"\"\"\n",
    "\n",
    "print(f\"\\n--- Running query to find the count of eligible users from the pre-treatment cohort... ---\")\n",
    "try:\n",
    "    df_eligible_top_users = run_query(query_eligible_top_users)\n",
    "    \n",
    "    if not df_eligible_top_users.empty:\n",
    "        num_eligible_top_users = df_eligible_top_users['ELIGIBLE_TOP_USER_COUNT'].iloc[0]\n",
    "        num_top_vendors = TOP_N_VENDORS\n",
    "        estimated_final_size = num_eligible_top_users * num_top_vendors\n",
    "\n",
    "        # --- Create and Display Summary Table of Facts ---\n",
    "        summary_data = {\n",
    "            \"Fact\": [\n",
    "                \"1. Pre-Treatment User Cohort Size\",\n",
    "                \"2. Eligible Users from Cohort (Impressions in pilot week)\",\n",
    "                \"3. Pre-Treatment Vendor Cohort Size\",\n",
    "                \"4. Final Panel Rows (Eligible Users x Top Vendors)\"\n",
    "            ],\n",
    "            \"Description\": [\n",
    "                f\"Top {TOP_N_USERS:,} spenders from the month prior to the pilot.\",\n",
    "                \"The number of users from the pre-defined cohort who were active during the pilot week.\",\n",
    "                f\"Top {TOP_N_VENDORS:,} sellers from the month prior to the pilot.\",\n",
    "                \"The final row count for the panel before negative sampling.\"\n",
    "            ],\n",
    "            \"Value\": [\n",
    "                f\"{TOP_N_USERS:,}\",\n",
    "                f\"{num_eligible_top_users:,}\",\n",
    "                f\"{num_top_vendors:,}\",\n",
    "                f\"{estimated_final_size:,}\"\n",
    "            ]\n",
    "        }\n",
    "        summary_df = pd.DataFrame(summary_data)\n",
    "        \n",
    "        show_table(summary_df, \"Final Panel Sizing Facts (Based on Pre-Treatment Cohorts)\")\n",
    "    else:\n",
    "        print(\"Warning: Could not determine the count of eligible top users.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nAn error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# panel and downsampling sizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Sizing Panel & Calculating Weights for Negative Sampling ###\n",
      "\n",
      "--- Population & Universe Definitions ---\n",
      "  Pre-Treatment Period: 2025-06-07 to 2025-07-07\n",
      "  Analysis Period:      2025-07-07 to 2025-07-14\n",
      "  User Cohort:          Top 100,000 spenders from pre-treatment period.\n",
      "  Vendor Cohort:          Top 4,000 sellers from pre-treatment period.\n",
      "-----------------------------------------------------\n",
      "\n",
      "--- Running query to get sizing facts... ---\n",
      "\n",
      "Final Sizing & Weight Calculation Facts\n",
      "=======================================\n",
      "+-----------------------------------------------------------------+-------------+\n",
      "| Fact                                                            | Value       |\n",
      "+=================================================================+=============+\n",
      "| 1. Eligible Users from Cohort (active in pilot week)            | 72,935      |\n",
      "+-----------------------------------------------------------------+-------------+\n",
      "| 2. Vendor Cohort Size                                           | 4,000       |\n",
      "+-----------------------------------------------------------------+-------------+\n",
      "| 3. Total Potential Panel Size (Users x Vendors)                 | 291,740,000 |\n",
      "+-----------------------------------------------------------------+-------------+\n",
      "| 4. 'Positive' Interactions (Purchases in pilot week)            | 10,908      |\n",
      "+-----------------------------------------------------------------+-------------+\n",
      "| 5. 'Negative' Interactions (Potential but no purchase)          | 291,729,092 |\n",
      "+-----------------------------------------------------------------+-------------+\n",
      "| 6. Negatives to Sample (at a 5:1 ratio)                         | 54,540      |\n",
      "+-----------------------------------------------------------------+-------------+\n",
      "| 7. Final Downsampled Panel Size (Positives + Sampled Negatives) | 65,448      |\n",
      "+-----------------------------------------------------------------+-------------+\n",
      "| 8. Statistical Weight for each Negative Sample                  | 5348.90     |\n",
      "+-----------------------------------------------------------------+-------------+\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# ==============================================================================\n",
    "# SIZING & WEIGHT CALCULATION (Using Pre-Treatment Cohorts)\n",
    "# ==============================================================================\n",
    "print(\"### Sizing Panel & Calculating Weights for Negative Sampling ###\")\n",
    "\n",
    "# --- Configuration ---\n",
    "PILOT_WEEK_START = '2025-07-07'\n",
    "PILOT_WEEK_END = '2025-07-14'\n",
    "PRE_TREATMENT_END = PILOT_WEEK_START\n",
    "PRE_TREATMENT_START = (datetime.strptime(PRE_TREATMENT_END, '%Y-%m-%d') - timedelta(days=30)).strftime('%Y-%m-%d')\n",
    "\n",
    "TOP_N_VENDORS = 4000\n",
    "TOP_N_USERS = 100000\n",
    "NEGATIVE_TO_POSITIVE_RATIO = 5\n",
    "DB_SCHEMA = \"INCREMENTALITY.INCREMENTALITY_RESEARCH\"\n",
    "LOOKBACK_DAYS_MAP = 30 # For our best product-vendor map\n",
    "\n",
    "# --- Print a clear header ---\n",
    "print(\"\\n--- Population & Universe Definitions ---\")\n",
    "print(f\"  Pre-Treatment Period: {PRE_TREATMENT_START} to {PRE_TREATMENT_END}\")\n",
    "print(f\"  Analysis Period:      {PILOT_WEEK_START} to {PILOT_WEEK_END}\")\n",
    "print(f\"  User Cohort:          Top {TOP_N_USERS:,} spenders from pre-treatment period.\")\n",
    "print(f\"  Vendor Cohort:          Top {TOP_N_VENDORS:,} sellers from pre-treatment period.\")\n",
    "print(\"-----------------------------------------------------\")\n",
    "\n",
    "# --- SQL Query to Gather All Necessary Facts for Weight Calculation ---\n",
    "sizing_query = f\"\"\"\n",
    "WITH\n",
    "-- 1. Define the User Cohort from the pre-treatment period\n",
    "top_users_pretreatment_cte AS (\n",
    "    SELECT USER_ID FROM {DB_SCHEMA}.PURCHASES\n",
    "    WHERE PURCHASED_AT >= '{PRE_TREATMENT_START}' AND PURCHASED_AT < '{PRE_TREATMENT_END}'\n",
    "    GROUP BY 1 ORDER BY SUM(QUANTITY * UNIT_PRICE) DESC LIMIT {TOP_N_USERS}\n",
    "),\n",
    "-- 2. Define the Vendor Cohort from the pre-treatment period (using our best map logic)\n",
    "top_vendors_pretreatment_cte AS (\n",
    "    SELECT VENDOR_ID FROM (\n",
    "        SELECT pv.VENDOR_ID, SUM(p.QUANTITY * p.UNIT_PRICE) AS rev\n",
    "        FROM {DB_SCHEMA}.PURCHASES p\n",
    "        INNER JOIN (\n",
    "            SELECT PRODUCT_ID, VENDOR_ID FROM (\n",
    "                SELECT PRODUCT_ID, VENDOR_ID, ROW_NUMBER() OVER (PARTITION BY PRODUCT_ID ORDER BY COUNT(*) DESC) as rn\n",
    "                FROM {DB_SCHEMA}.AUCTIONS_RESULTS\n",
    "                WHERE CREATED_AT >= DATEADD(day, -{LOOKBACK_DAYS_MAP}, '{PRE_TREATMENT_END}') AND CREATED_AT < '{PRE_TREATMENT_END}'\n",
    "                GROUP BY 1, 2\n",
    "            ) WHERE rn = 1\n",
    "        ) pv ON p.PRODUCT_ID = pv.PRODUCT_ID\n",
    "        WHERE p.PURCHASED_AT >= '{PRE_TREATMENT_START}' AND p.PURCHASED_AT < '{PRE_TREATMENT_END}'\n",
    "        GROUP BY 1\n",
    "    ) ORDER BY rev DESC LIMIT {TOP_N_VENDORS}\n",
    "),\n",
    "-- 3. Find the subset of the user cohort who were \"eligible\" during the pilot week\n",
    "eligible_users_in_pilot_cte AS (\n",
    "    SELECT DISTINCT i.USER_ID\n",
    "    FROM {DB_SCHEMA}.IMPRESSIONS i\n",
    "    INNER JOIN top_users_pretreatment_cte tu ON i.USER_ID = tu.USER_ID\n",
    "    WHERE i.OCCURRED_AT >= '{PILOT_WEEK_START}' AND i.OCCURRED_AT < '{PILOT_WEEK_END}'\n",
    "),\n",
    "-- 4. Count the \"positive\" interactions (purchases) during the pilot week for our cohorts\n",
    "positive_interactions_in_pilot_cte AS (\n",
    "    SELECT COUNT(DISTINCT p.USER_ID, pv.VENDOR_ID) as positive_interaction_count\n",
    "    FROM {DB_SCHEMA}.PURCHASES p\n",
    "    INNER JOIN (\n",
    "        SELECT PRODUCT_ID, VENDOR_ID FROM (\n",
    "            SELECT PRODUCT_ID, VENDOR_ID, ROW_NUMBER() OVER (PARTITION BY PRODUCT_ID ORDER BY COUNT(*) DESC) as rn\n",
    "            FROM {DB_SCHEMA}.AUCTIONS_RESULTS\n",
    "            WHERE CREATED_AT >= DATEADD(day, -{LOOKBACK_DAYS_MAP}, '{PILOT_WEEK_START}') AND CREATED_AT < '{PILOT_WEEK_END}'\n",
    "            GROUP BY 1, 2\n",
    "        ) WHERE rn = 1\n",
    "    ) pv ON p.PRODUCT_ID = pv.PRODUCT_ID\n",
    "    WHERE p.USER_ID IN (SELECT USER_ID FROM eligible_users_in_pilot_cte)\n",
    "      AND pv.VENDOR_ID IN (SELECT VENDOR_ID FROM top_vendors_pretreatment_cte)\n",
    "      AND p.PURCHASED_AT >= '{PILOT_WEEK_START}' AND p.PURCHASED_AT < '{PILOT_WEEK_END}'\n",
    ")\n",
    "-- 5. Final SELECT to get all the numbers we need\n",
    "SELECT\n",
    "    (SELECT COUNT(*) FROM eligible_users_in_pilot_cte) AS eligible_user_count,\n",
    "    (SELECT COUNT(*) FROM top_vendors_pretreatment_cte) AS top_vendor_count,\n",
    "    (SELECT positive_interaction_count FROM positive_interactions_in_pilot_cte) AS positive_interaction_count\n",
    ";\n",
    "\"\"\"\n",
    "\n",
    "print(f\"\\n--- Running query to get sizing facts... ---\")\n",
    "try:\n",
    "    df_facts = run_query(sizing_query)\n",
    "    \n",
    "    if not df_facts.empty:\n",
    "        # Extract the facts\n",
    "        num_eligible_users = df_facts['ELIGIBLE_USER_COUNT'].iloc[0]\n",
    "        num_top_vendors = df_facts['TOP_VENDOR_COUNT'].iloc[0]\n",
    "        num_positives = df_facts['POSITIVE_INTERACTION_COUNT'].iloc[0]\n",
    "        \n",
    "        # Calculate the derived metrics for our final panel\n",
    "        total_potential_panel_size = num_eligible_users * num_top_vendors\n",
    "        total_negatives = total_potential_panel_size - num_positives\n",
    "        num_sampled_negatives = num_positives * NEGATIVE_TO_POSITIVE_RATIO\n",
    "        final_panel_size = num_positives + num_sampled_negatives\n",
    "        sampling_fraction = num_sampled_negatives / total_negatives if total_negatives > 0 else 0\n",
    "        negative_weight = total_negatives / num_sampled_negatives if num_sampled_negatives > 0 else 1\n",
    "\n",
    "        # --- Create and Display Final Summary Table of Facts ---\n",
    "        summary_data = {\n",
    "            \"Fact\": [\n",
    "                \"1. Eligible Users from Cohort (active in pilot week)\",\n",
    "                \"2. Vendor Cohort Size\",\n",
    "                \"3. Total Potential Panel Size (Users x Vendors)\",\n",
    "                \"4. 'Positive' Interactions (Purchases in pilot week)\",\n",
    "                \"5. 'Negative' Interactions (Potential but no purchase)\",\n",
    "                \"6. Negatives to Sample (at a 5:1 ratio)\",\n",
    "                \"7. Final Downsampled Panel Size (Positives + Sampled Negatives)\",\n",
    "                \"8. Statistical Weight for each Negative Sample\"\n",
    "            ],\n",
    "            \"Value\": [\n",
    "                f\"{num_eligible_users:,}\", f\"{num_top_vendors:,}\", f\"{total_potential_panel_size:,}\",\n",
    "                f\"{num_positives:,}\", f\"{total_negatives:,}\", f\"{num_sampled_negatives:,}\",\n",
    "                f\"{final_panel_size:,}\", f\"{negative_weight:.2f}\"\n",
    "            ]\n",
    "        }\n",
    "        summary_df = pd.DataFrame(summary_data)\n",
    "        show_table(summary_df, \"Final Sizing & Weight Calculation Facts\")\n",
    "    else:\n",
    "        print(\"Warning: Could not retrieve sizing facts.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nAn error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data pull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Building Final Panel via Optimized SQL (1-Week, 1% Negative Sample) ###\n",
      "\n",
      "--- Methodology ---\n",
      "  Cohorts defined from pre-treatment period: 2025-06-07 to 2025-07-07\n",
      "  Analysis of activity during pilot week:    2025-07-07 to 2025-07-14\n",
      "  User Cohort: Top 100,000 click-active spenders.\n",
      "  Vendor Cohort: Top 1,000 sellers.\n",
      "  Sampling: Keeping all positives and a 1% random sample of negatives.\n",
      "---------------------\n",
      "\n",
      "--- Executing final panel build query on Snowflake... ---\n",
      "✅ Panel with 41,049,173 rows extracted.\n",
      "\n",
      "💾 --- Saving final panel to 'final_analysis_panel.parquet'... ---\n",
      "✅ Panel saved. Ready for analysis.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# ==============================================================================\n",
    "# FINAL PANEL BUILD & SAVE (Optimized SQL with Type-Casting Fix)\n",
    "# ==============================================================================\n",
    "print(\"### Building Final Panel via Optimized SQL (1-Week, 1% Negative Sample) ###\")\n",
    "\n",
    "# --- Configuration ---\n",
    "PILOT_WEEK_START = '2025-07-07'\n",
    "PILOT_WEEK_END = '2025-07-14'\n",
    "PRE_TREATMENT_START = (datetime.strptime(PILOT_WEEK_START, '%Y-%m-%d') - timedelta(days=30)).strftime('%Y-%m-%d')\n",
    "PRE_TREATMENT_END = PILOT_WEEK_START\n",
    "\n",
    "TOP_N_VENDORS = 1000\n",
    "TOP_N_USERS = 100000\n",
    "NEGATIVE_SAMPLING_FRACTION = 0.01\n",
    "DB_SCHEMA = 'INCREMENTALITY.INCREMENTALITY_RESEARCH'\n",
    "MAP_LOOKBACK_DAYS = 30\n",
    "\n",
    "# --- Print a clear header of the methodology ---\n",
    "print(\"\\n--- Methodology ---\")\n",
    "print(f\"  Cohorts defined from pre-treatment period: {PRE_TREATMENT_START} to {PRE_TREATMENT_END}\")\n",
    "print(f\"  Analysis of activity during pilot week:    {PILOT_WEEK_START} to {PILOT_WEEK_END}\")\n",
    "print(f\"  User Cohort: Top {TOP_N_USERS:,} click-active spenders.\")\n",
    "print(f\"  Vendor Cohort: Top {TOP_N_VENDORS:,} sellers.\")\n",
    "print(f\"  Sampling: Keeping all positives and a {NEGATIVE_SAMPLING_FRACTION:.0%} random sample of negatives.\")\n",
    "print(\"---------------------\")\n",
    "\n",
    "# --- The Final, Optimized, and Corrected SQL Query ---\n",
    "final_panel_query = f\"\"\"\n",
    "WITH\n",
    "-- ==============================================================================\n",
    "-- STEP 1: DEFINE COHORTS & MAPS (with Type Casting)\n",
    "-- ==============================================================================\n",
    "top_vendors_pretreatment_cte AS (\n",
    "    SELECT VENDOR_ID FROM (\n",
    "        SELECT pv.VENDOR_ID, SUM(p.QUANTITY * p.UNIT_PRICE) AS rev\n",
    "        FROM {DB_SCHEMA}.PURCHASES p\n",
    "        INNER JOIN (\n",
    "            -- MAP with CASTING\n",
    "            SELECT CAST(PRODUCT_ID AS VARCHAR) as PRODUCT_ID, CAST(VENDOR_ID AS VARCHAR) as VENDOR_ID\n",
    "            FROM (\n",
    "                SELECT PRODUCT_ID, VENDOR_ID, ROW_NUMBER() OVER (PARTITION BY PRODUCT_ID ORDER BY COUNT(*) DESC) as rn\n",
    "                FROM {DB_SCHEMA}.AUCTIONS_RESULTS\n",
    "                WHERE CREATED_AT >= DATEADD(day, -{MAP_LOOKBACK_DAYS}, '{PRE_TREATMENT_END}') AND CREATED_AT < '{PRE_TREATMENT_END}'\n",
    "                GROUP BY 1, 2\n",
    "            ) WHERE rn = 1\n",
    "        ) pv ON p.PRODUCT_ID = pv.PRODUCT_ID\n",
    "        WHERE p.PURCHASED_AT >= '{PRE_TREATMENT_START}' AND p.PURCHASED_AT < '{PRE_TREATMENT_END}' GROUP BY 1\n",
    "    ) ORDER BY rev DESC LIMIT {TOP_N_VENDORS}\n",
    "),\n",
    "top_click_active_users_pretreatment_cte AS (\n",
    "    SELECT USER_ID FROM (\n",
    "        SELECT p.USER_ID, SUM(p.QUANTITY * p.UNIT_PRICE) as total_spend\n",
    "        FROM {DB_SCHEMA}.PURCHASES p\n",
    "        WHERE p.USER_ID IN (\n",
    "            SELECT DISTINCT USER_ID FROM {DB_SCHEMA}.CLICKS\n",
    "            WHERE OCCURRED_AT >= '{PRE_TREATMENT_START}' AND OCCURRED_AT < '{PRE_TREATMENT_END}'\n",
    "        )\n",
    "        AND p.PURCHASED_AT >= '{PRE_TREATMENT_START}' AND p.PURCHASED_AT < '{PRE_TREATMENT_END}'\n",
    "        GROUP BY 1\n",
    "    ) ORDER BY total_spend DESC LIMIT {TOP_N_USERS}\n",
    "),\n",
    "eligible_users_in_pilot_cte AS (\n",
    "    SELECT DISTINCT i.USER_ID\n",
    "    FROM {DB_SCHEMA}.IMPRESSIONS i\n",
    "    WHERE i.USER_ID IN (SELECT USER_ID FROM top_click_active_users_pretreatment_cte)\n",
    "      AND i.OCCURRED_AT >= '{PILOT_WEEK_START}' AND i.OCCURRED_AT < '{PILOT_WEEK_END}'\n",
    "),\n",
    "pilot_week_map_cte AS (\n",
    "    -- MAP with CASTING\n",
    "    SELECT CAST(PRODUCT_ID AS VARCHAR) as PRODUCT_ID, CAST(VENDOR_ID AS VARCHAR) as VENDOR_ID\n",
    "    FROM (\n",
    "        SELECT PRODUCT_ID, VENDOR_ID, ROW_NUMBER() OVER (PARTITION BY PRODUCT_ID ORDER BY COUNT(*) DESC) as rn\n",
    "        FROM {DB_SCHEMA}.AUCTIONS_RESULTS\n",
    "        WHERE CREATED_AT >= DATEADD(day, -{MAP_LOOKBACK_DAYS}, '{PILOT_WEEK_START}') AND CREATED_AT < '{PILOT_WEEK_END}'\n",
    "        GROUP BY 1, 2\n",
    "    ) WHERE rn = 1\n",
    "),\n",
    "-- ==============================================================================\n",
    "-- STEP 2: IDENTIFY POSITIVES & NEGATIVES DURING PILOT WEEK\n",
    "-- ==============================================================================\n",
    "base_panel_cte AS (\n",
    "    SELECT u.USER_ID, v.VENDOR_ID FROM eligible_users_in_pilot_cte u CROSS JOIN top_vendors_pretreatment_cte v\n",
    "),\n",
    "positive_interactions_cte AS (\n",
    "    SELECT DISTINCT p.USER_ID, pv.VENDOR_ID\n",
    "    FROM {DB_SCHEMA}.PURCHASES p\n",
    "    INNER JOIN pilot_week_map_cte pv ON p.PRODUCT_ID = pv.PRODUCT_ID\n",
    "    WHERE p.USER_ID IN (SELECT USER_ID FROM eligible_users_in_pilot_cte)\n",
    "      AND pv.VENDOR_ID IN (SELECT VENDOR_ID FROM top_vendors_pretreatment_cte)\n",
    "      AND p.PURCHASED_AT >= '{PILOT_WEEK_START}' AND p.PURCHASED_AT < '{PILOT_WEEK_END}'\n",
    "),\n",
    "-- ==============================================================================\n",
    "-- STEP 3: ASSEMBLE FINAL PANEL WITH SAMPLED NEGATIVES\n",
    "-- ==============================================================================\n",
    "final_panel_keys AS (\n",
    "    SELECT USER_ID, VENDOR_ID, 1.0 AS weight FROM positive_interactions_cte\n",
    "    UNION ALL\n",
    "    SELECT USER_ID, VENDOR_ID, (1.0 / {NEGATIVE_SAMPLING_FRACTION}) AS weight\n",
    "    FROM (\n",
    "        SELECT USER_ID, VENDOR_ID FROM base_panel_cte\n",
    "        EXCEPT\n",
    "        SELECT USER_ID, VENDOR_ID FROM positive_interactions_cte\n",
    "    )\n",
    "    WHERE MOD(HASH(USER_ID, VENDOR_ID), 100) < ({NEGATIVE_SAMPLING_FRACTION} * 100)\n",
    "),\n",
    "-- ==============================================================================\n",
    "-- STEP 4: PRE-AGGREGATE FEATURES & ATTACH\n",
    "-- ==============================================================================\n",
    "clicks_agg_pilot AS (\n",
    "    SELECT USER_ID, VENDOR_ID, COUNT(*) as total_clicks\n",
    "    FROM {DB_SCHEMA}.CLICKS\n",
    "    WHERE OCCURRED_AT >= '{PILOT_WEEK_START}' AND OCCURRED_AT < '{PILOT_WEEK_END}'\n",
    "    GROUP BY 1, 2\n",
    "),\n",
    "purchases_agg_pilot AS (\n",
    "    SELECT USER_ID, VENDOR_ID, SUM(QUANTITY * UNIT_PRICE) as total_revenue, COUNT(DISTINCT PURCHASE_ID) as total_sales\n",
    "    FROM (\n",
    "        SELECT p.USER_ID, pv.VENDOR_ID, p.QUANTITY, p.UNIT_PRICE, p.PURCHASE_ID\n",
    "        FROM {DB_SCHEMA}.PURCHASES p\n",
    "        INNER JOIN pilot_week_map_cte pv ON p.PRODUCT_ID = pv.PRODUCT_ID\n",
    "        WHERE p.PURCHASED_AT >= '{PILOT_WEEK_START}' AND p.PURCHASED_AT < '{PILOT_WEEK_END}'\n",
    "    )\n",
    "    GROUP BY 1, 2\n",
    ")\n",
    "-- Final SELECT to join all features to our downsampled panel keys\n",
    "SELECT\n",
    "    DATE_TRUNC('WEEK', TO_TIMESTAMP_NTZ('{PILOT_WEEK_START}')) AS week,\n",
    "    fpk.user_id,\n",
    "    fpk.vendor_id,\n",
    "    fpk.weight,\n",
    "    COALESCE(c.total_clicks, 0) AS total_clicks,\n",
    "    COALESCE(p.total_revenue, 0) / 100.0 AS total_revenue,\n",
    "    COALESCE(p.total_sales, 0) AS total_sales\n",
    "FROM final_panel_keys fpk\n",
    "LEFT JOIN clicks_agg_pilot c ON fpk.user_id = c.user_id AND fpk.vendor_id = c.vendor_id\n",
    "LEFT JOIN purchases_agg_pilot p ON fpk.user_id = p.user_id AND fpk.vendor_id = p.vendor_id;\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    print(\"\\n--- Executing final panel build query on Snowflake... ---\")\n",
    "    df_final_panel = run_query(final_panel_query)\n",
    "\n",
    "    parquet_filename_final = 'final_analysis_panel.parquet'\n",
    "    if not df_final_panel.empty:\n",
    "        print(f\"✅ Panel with {len(df_final_panel):,} rows extracted.\")\n",
    "        \n",
    "        df_to_save = df_final_panel.copy()\n",
    "        df_to_save.columns = [col.lower() for col in df_to_save.columns]\n",
    "        df_to_save['week'] = pd.to_datetime(df_to_save['week'])\n",
    "        for col in df_to_save.select_dtypes(include=['object']).columns:\n",
    "            df_to_save[col] = df_to_save[col].astype(str)\n",
    "\n",
    "        print(f\"\\n💾 --- Saving final panel to '{parquet_filename_final}'... ---\")\n",
    "        df_to_save.to_parquet(parquet_filename_final, engine='pyarrow', index=False)\n",
    "        print(f\"✅ Panel saved. Ready for analysis.\")\n",
    "    else:\n",
    "        print(\"Warning: The panel query returned no data.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nAn error occurred while building the panel: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# analysis - feols, feglm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### FINAL ANALYSIS: Core Fixed Effects Models (10% Negative Sampling) ###\n",
      "\n",
      "--- Initializing rpy2 and importing R's `fixest` package... ---\n",
      "✅ R's `fixest` package loaded successfully.\n",
      "\n",
      "--- Loading data from 'final_analysis_panel.parquet' for modeling... ---\n",
      "✅ Successfully loaded DataFrame with 41,049,173 rows.\n",
      "   Columns: ['week', 'user_id', 'vendor_id', 'weight', 'total_clicks', 'total_revenue', 'total_sales']\n",
      "\n",
      "Using revenue column: total_revenue\n",
      "Using clicks column: total_clicks\n",
      "\n",
      "--- Converting columns to numeric ---\n",
      "  weight dtype: float64\n",
      "  total_revenue dtype: float64\n",
      "  total_clicks dtype: int64\n",
      "\n",
      "--- Applying negative sampling strategy ---\n",
      "  Positive revenue observations: 5,770 (keeping all)\n",
      "  Zero revenue observations: 41,043,403 (sampling 10%)\n",
      "✅ Final analysis dataset: 4,110,110 rows\n",
      "   Reduction: 90.0%\n",
      "\n",
      "--- Running Core Fixed Effects Models on Sampled Data ---\n",
      "\n",
      "--- Preparing data in R ---\n",
      "[1] \"Sampled observations: 4110110\"\n",
      "[1] \"Unique users: 81286\"\n",
      "[1] \"Unique vendors: 1000\"\n",
      "[1] \"Percent with purchases (in sample): 0.14 %\"\n",
      "[1] \"Using original weights: TRUE\"\n",
      "[1] \"Average final weight: 998.6\"\n",
      "\n",
      "============================================================\n",
      "MODEL 1: OLS FOR REVENUE\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "R callback write-console: Error: in feols(ihs_revenue ~ ihs_clicks | user_id + vendor...: \n",
      "The only variable, 'ihs_clicks', is collinear with the fixed effects.\n",
      "Without doubt, your model is misspecified.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ERROR running the models in R: Error: in feols(ihs_revenue ~ ihs_clicks | user_id + vendor...: \n",
      "The only variable, 'ihs_clicks', is collinear with the fixed effects.\n",
      "Without doubt, your model is misspecified.\n",
      "\n",
      "\n",
      "--- Analysis completed ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/b7/1tvk5qmx0ds9c6gk2lrlhv380000gn/T/ipykernel_39849/899109279.py\", line 166, in <module>\n",
      "    ro.r(f\"\"\"\n",
      "    ~~~~^^^^^\n",
      "    start_time <- Sys.time()\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    ...<18 lines>...\n",
      "    print(paste(\"→ Time taken:\", round(difftime(end_time, start_time, units = \"secs\"), 1), \"seconds\"))\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "    \"\"\")\n",
      "    ^^^^\n",
      "  File \"/Users/pranjal/Code/topsort-incrementality/venv/lib/python3.13/site-packages/rpy2/robjects/__init__.py\", line 552, in __call__\n",
      "    res, visible = rinterface.evalr_expr_with_visible(   # type: ignore\n",
      "                   ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^\n",
      "        r_expr\n",
      "        ^^^^^^\n",
      "    )\n",
      "    ^\n",
      "  File \"/Users/pranjal/Code/topsort-incrementality/venv/lib/python3.13/site-packages/rpy2/rinterface/__init__.py\", line 205, in evalr_expr_with_visible\n",
      "    raise embedded.RRuntimeError(_rinterface._geterrmessage())\n",
      "rpy2.rinterface_lib.embedded.RRuntimeError: Error: in feols(ihs_revenue ~ ihs_clicks | user_id + vendor...: \n",
      "The only variable, 'ihs_clicks', is collinear with the fixed effects.\n",
      "Without doubt, your model is misspecified.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "  import pandas as pd\n",
    "  import numpy as np\n",
    "  import os\n",
    "  from IPython.display import display, Markdown\n",
    "\n",
    "  # Set R environment variables BEFORE importing rpy2\n",
    "  os.environ['R_HOME'] = '/Library/Frameworks/R.framework/Resources'\n",
    "  os.environ['DYLD_LIBRARY_PATH'] = f\"{os.environ['R_HOME']}/lib:{os.environ.get('DYLD_LIBRARY_PATH', '')}\"\n",
    "\n",
    "  # rpy2 imports for the Python-to-R bridge\n",
    "  import rpy2.robjects as ro\n",
    "  from rpy2.robjects.packages import importr, isinstalled\n",
    "  from rpy2.robjects import pandas2ri\n",
    "  from rpy2.robjects.conversion import localconverter\n",
    "\n",
    "  # ==============================================================================\n",
    "  # FINAL ANALYSIS: Core Models with Negative Sampling\n",
    "  # ==============================================================================\n",
    "  print(\"### FINAL ANALYSIS: Core Fixed Effects Models (10% Negative Sampling) ###\")\n",
    "\n",
    "  # --- 1. Set up rpy2 Environment ---\n",
    "  try:\n",
    "      print(\"\\n--- Initializing rpy2 and importing R's `fixest` package... ---\")\n",
    "\n",
    "      utils = importr('utils')\n",
    "      if not isinstalled('fixest'):\n",
    "          print(\"Installing fixest package...\")\n",
    "          utils.install_packages('fixest', repos='https://cran.rstudio.com/')\n",
    "\n",
    "      fixest = importr('fixest')\n",
    "      print(\"✅ R's `fixest` package loaded successfully.\")\n",
    "      rpy2_setup_failed = False\n",
    "  except Exception as e:\n",
    "      print(f\"ERROR: Failed to initialize rpy2 or load `fixest`: {e}\")\n",
    "      rpy2_setup_failed = True\n",
    "\n",
    "  if not rpy2_setup_failed:\n",
    "      # --- 2. Load and Sample the Data ---\n",
    "      parquet_filename = 'final_analysis_panel.parquet'\n",
    "      try:\n",
    "          print(f\"\\n--- Loading data from '{parquet_filename}' for modeling... ---\")\n",
    "          df_full = pd.read_parquet(parquet_filename, engine='pyarrow')\n",
    "          print(f\"✅ Successfully loaded DataFrame with {len(df_full):,} rows.\")\n",
    "          print(f\"   Columns: {df_full.columns.tolist()}\")\n",
    "\n",
    "          # Detect column names\n",
    "          if 'total_revenue_vendor_product' in df_full.columns:\n",
    "              revenue_col = 'total_revenue_vendor_product'\n",
    "          elif 'total_revenue' in df_full.columns:\n",
    "              revenue_col = 'total_revenue'\n",
    "          else:\n",
    "              revenue_col = 'revenue'\n",
    "\n",
    "          if 'total_clicks_promoted' in df_full.columns:\n",
    "              clicks_col = 'total_clicks_promoted'\n",
    "          elif 'total_clicks' in df_full.columns:\n",
    "              clicks_col = 'total_clicks'\n",
    "          else:\n",
    "              clicks_col = 'clicks'\n",
    "\n",
    "          print(f\"\\nUsing revenue column: {revenue_col}\")\n",
    "          print(f\"Using clicks column: {clicks_col}\")\n",
    "\n",
    "          # Convert columns to numeric\n",
    "          print(\"\\n--- Converting columns to numeric ---\")\n",
    "          df_full[revenue_col] = pd.to_numeric(df_full[revenue_col], errors='coerce')\n",
    "          df_full[clicks_col] = pd.to_numeric(df_full[clicks_col], errors='coerce')\n",
    "\n",
    "          # Convert weight column if it exists\n",
    "          if 'weight' in df_full.columns:\n",
    "              df_full['weight'] = pd.to_numeric(df_full['weight'], errors='coerce')\n",
    "              print(f\"  weight dtype: {df_full['weight'].dtype}\")\n",
    "\n",
    "          # Check data types\n",
    "          print(f\"  {revenue_col} dtype: {df_full[revenue_col].dtype}\")\n",
    "          print(f\"  {clicks_col} dtype: {df_full[clicks_col].dtype}\")\n",
    "\n",
    "          # Drop NaN values\n",
    "          before_rows = len(df_full)\n",
    "          df_full = df_full.dropna(subset=[revenue_col, clicks_col])\n",
    "          after_rows = len(df_full)\n",
    "          if before_rows != after_rows:\n",
    "              print(f\"  Dropped {before_rows - after_rows} rows with NaN values\")\n",
    "\n",
    "          # SAMPLING STRATEGY: Keep all positives, sample 10% of negatives\n",
    "          print(\"\\n--- Applying negative sampling strategy ---\")\n",
    "          positives = df_full[df_full[revenue_col] > 0]\n",
    "          negatives = df_full[df_full[revenue_col] == 0]\n",
    "\n",
    "          print(f\"  Positive revenue observations: {len(positives):,} (keeping all)\")\n",
    "          print(f\"  Zero revenue observations: {len(negatives):,} (sampling 10%)\")\n",
    "\n",
    "          # Sample 10% of negatives\n",
    "          np.random.seed(42)\n",
    "          sampled_negatives = negatives.sample(frac=0.1)\n",
    "\n",
    "          # Combine positives and sampled negatives\n",
    "          df_analysis = pd.concat([positives, sampled_negatives]).sort_index()\n",
    "\n",
    "          # Add sampling weight column in Python\n",
    "          df_analysis['sampling_weight'] = np.where(df_analysis[revenue_col] > 0, 1, 10)\n",
    "\n",
    "          print(f\"✅ Final analysis dataset: {len(df_analysis):,} rows\")\n",
    "          print(f\"   Reduction: {100 * (1 - len(df_analysis)/len(df_full)):.1f}%\")\n",
    "\n",
    "      except Exception as e:\n",
    "          print(f\"ERROR during data loading/sampling: {e}\")\n",
    "          import traceback\n",
    "          traceback.print_exc()\n",
    "          df_analysis = pd.DataFrame()\n",
    "\n",
    "      if not df_analysis.empty:\n",
    "          # --- 3. Run Models on Sampled Data ---\n",
    "          try:\n",
    "              print(\"\\n--- Running Core Fixed Effects Models on Sampled Data ---\")\n",
    "\n",
    "              # Use the conversion context\n",
    "              with localconverter(ro.default_converter + pandas2ri.converter):\n",
    "                  r_df = pandas2ri.py2rpy(df_analysis)\n",
    "                  ro.globalenv['df_for_r'] = r_df\n",
    "\n",
    "              # Prepare data in R\n",
    "              print(\"\\n--- Preparing data in R ---\")\n",
    "              ro.r(f\"\"\"\n",
    "              library(fixest)\n",
    "              \n",
    "              df_analysis <- df_for_r\n",
    "              \n",
    "              # Ensure all numeric columns are properly typed in R\n",
    "              df_analysis[['{revenue_col}']] <- as.numeric(as.character(df_analysis[['{revenue_col}']]))\n",
    "              df_analysis[['{clicks_col}']] <- as.numeric(as.character(df_analysis[['{clicks_col}']]))\n",
    "              df_analysis$sampling_weight <- as.numeric(as.character(df_analysis$sampling_weight))\n",
    "              \n",
    "              # Check if original weight exists and convert it\n",
    "              has_weights <- \"weight\" %in% names(df_analysis)\n",
    "              if (has_weights) {{\n",
    "                  df_analysis$weight <- as.numeric(as.character(df_analysis$weight))\n",
    "                  # Combine original weight with sampling weight\n",
    "                  df_analysis$final_weight <- df_analysis$weight * df_analysis$sampling_weight\n",
    "              }} else {{\n",
    "                  # Use only sampling weight\n",
    "                  df_analysis$final_weight <- df_analysis$sampling_weight\n",
    "              }}\n",
    "              \n",
    "              # Prepare variables\n",
    "              df_analysis$ihs_clicks <- asinh(df_analysis[['{clicks_col}']])\n",
    "              df_analysis$ihs_revenue <- asinh(df_analysis[['{revenue_col}']])\n",
    "              df_analysis$purchase_binary <- as.integer(df_analysis[['{revenue_col}']] > 0)\n",
    "              df_analysis$user_id <- as.factor(df_analysis$user_id)\n",
    "              df_analysis$vendor_id <- as.factor(df_analysis$vendor_id)\n",
    "              \n",
    "              print(paste(\"Sampled observations:\", nrow(df_analysis)))\n",
    "              print(paste(\"Unique users:\", length(unique(df_analysis$user_id))))\n",
    "              print(paste(\"Unique vendors:\", length(unique(df_analysis$vendor_id))))\n",
    "              print(paste(\"Percent with purchases (in sample):\", \n",
    "                         round(100 * mean(df_analysis$purchase_binary), 2), \"%\"))\n",
    "              print(paste(\"Using original weights:\", has_weights))\n",
    "              print(paste(\"Average final weight:\", round(mean(df_analysis$final_weight), 2)))\n",
    "              \"\"\")\n",
    "\n",
    "              # Run OLS model\n",
    "              print(\"\\n\" + \"=\"*60)\n",
    "              print(\"MODEL 1: OLS FOR REVENUE\")\n",
    "              print(\"=\"*60)\n",
    "\n",
    "              ro.r(f\"\"\"\n",
    "              start_time <- Sys.time()\n",
    "              \n",
    "              model_ols <- feols(ihs_revenue ~ ihs_clicks | user_id + vendor_id, \n",
    "                                data = df_analysis, \n",
    "                                weights = ~ final_weight, \n",
    "                                cluster = ~ user_id)\n",
    "              \n",
    "              print(summary(model_ols))\n",
    "              \n",
    "              ols_coef <- coef(model_ols)[\"ihs_clicks\"]\n",
    "              ols_se <- se(model_ols)[\"ihs_clicks\"]\n",
    "              ols_pval <- pvalue(model_ols)[\"ihs_clicks\"]\n",
    "              \n",
    "              print(\"\")\n",
    "              print(paste(\"→ Coefficient:\", round(ols_coef, 4)))\n",
    "              print(paste(\"→ Std Error:\", round(ols_se, 4)))\n",
    "              print(paste(\"→ P-value:\", format(ols_pval, scientific = TRUE, digits = 3)))\n",
    "              \n",
    "              end_time <- Sys.time()\n",
    "              print(paste(\"→ Time taken:\", round(difftime(end_time, start_time, units = \"secs\"), 1), \"seconds\"))\n",
    "              \"\"\")\n",
    "\n",
    "              print(\"\\n✅ OLS model completed\\n\")\n",
    "\n",
    "              # Run Logit model\n",
    "              print(\"=\"*60)\n",
    "              print(\"MODEL 2: LOGIT FOR PURCHASE PROBABILITY\")\n",
    "              print(\"=\"*60)\n",
    "\n",
    "              ro.r(f\"\"\"\n",
    "              start_time <- Sys.time()\n",
    "              \n",
    "              model_logit <- feglm(purchase_binary ~ ihs_clicks | user_id + vendor_id, \n",
    "                                  family = binomial(), \n",
    "                                  data = df_analysis, \n",
    "                                  weights = ~ final_weight, \n",
    "                                  cluster = ~ user_id)\n",
    "              \n",
    "              print(summary(model_logit))\n",
    "              \n",
    "              logit_coef <- coef(model_logit)[\"ihs_clicks\"]\n",
    "              logit_se <- se(model_logit)[\"ihs_clicks\"]\n",
    "              logit_pval <- pvalue(model_logit)[\"ihs_clicks\"]\n",
    "              odds_ratio <- exp(logit_coef)\n",
    "              \n",
    "              print(\"\")\n",
    "              print(paste(\"→ Coefficient:\", round(logit_coef, 4)))\n",
    "              print(paste(\"→ Std Error:\", round(logit_se, 4)))\n",
    "              print(paste(\"→ P-value:\", format(logit_pval, scientific = TRUE, digits = 3)))\n",
    "              print(paste(\"→ Odds ratio:\", round(odds_ratio, 4)))\n",
    "              \n",
    "              end_time <- Sys.time()\n",
    "              print(paste(\"→ Time taken:\", round(difftime(end_time, start_time, units = \"secs\"), 1), \"seconds\"))\n",
    "              \"\"\")\n",
    "\n",
    "              print(\"\\n✅ Both models completed successfully\")\n",
    "              print(\"\\n\" + \"=\"*60)\n",
    "              print(\"Note: Results use 10% negative sampling with weight adjustment\")\n",
    "\n",
    "          except Exception as e:\n",
    "              print(f\"\\nERROR running the models in R: {e}\")\n",
    "              import traceback\n",
    "              traceback.print_exc()\n",
    "\n",
    "          print(\"\\n--- Analysis completed ---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### DATA STRUCTURE DIAGNOSIS ###\n",
      "\n",
      "Converting columns to numeric...\n",
      "\n",
      "Total rows: 41,049,173\n",
      "Columns: ['week', 'user_id', 'vendor_id', 'weight', 'total_clicks', 'total_revenue', 'total_sales']\n",
      "\n",
      "============================================================\n",
      "KEY FINDINGS\n",
      "============================================================\n",
      "1. Rows per user-vendor: 1.0\n",
      "   → This is CROSS-SECTIONAL data (one obs per user-vendor)\n",
      "\n",
      "2. Total clicks statistics:\n",
      "   - Sum: 0\n",
      "   - Max: 0\n",
      "   - Non-zero: 0\n",
      "   → NO CLICK DATA (all zeros!)\n",
      "\n",
      "3. Revenue statistics:\n",
      "   - Positive revenue obs: 5,770\n",
      "   - Max revenue: $10,833.00\n",
      "\n",
      "4. Sales statistics:\n",
      "   - Positive sales obs: 5,770\n",
      "   - Max sales: 13\n",
      "\n",
      "============================================================\n",
      "COLUMN CHECK\n",
      "============================================================\n",
      "Available columns: ['week', 'user_id', 'vendor_id', 'weight', 'total_clicks', 'total_revenue', 'total_sales']\n",
      "\n",
      "Found click columns: ['total_clicks']\n",
      "  total_clicks: sum=0, max=0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load data\n",
    "parquet_filename = 'final_analysis_panel.parquet'\n",
    "df = pd.read_parquet(parquet_filename, engine='pyarrow')\n",
    "\n",
    "print(\"### DATA STRUCTURE DIAGNOSIS ###\\n\")\n",
    "\n",
    "# Convert to numeric first\n",
    "print(\"Converting columns to numeric...\")\n",
    "df['total_revenue'] = pd.to_numeric(df['total_revenue'], errors='coerce')\n",
    "df['total_clicks'] = pd.to_numeric(df['total_clicks'], errors='coerce')\n",
    "df['total_sales'] = pd.to_numeric(df['total_sales'], errors='coerce')\n",
    "\n",
    "print(f\"\\nTotal rows: {len(df):,}\")\n",
    "print(f\"Columns: {df.columns.tolist()}\")\n",
    "\n",
    "# Check the actual data\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"KEY FINDINGS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"1. Rows per user-vendor: {df.groupby(['user_id', 'vendor_id']).size().mean():.1f}\")\n",
    "print(f\"   → This is CROSS-SECTIONAL data (one obs per user-vendor)\\n\")\n",
    "\n",
    "print(f\"2. Total clicks statistics:\")\n",
    "print(f\"   - Sum: {df['total_clicks'].sum()}\")\n",
    "print(f\"   - Max: {df['total_clicks'].max()}\")\n",
    "print(f\"   - Non-zero: {(df['total_clicks'] > 0).sum()}\")\n",
    "print(f\"   → NO CLICK DATA (all zeros!)\\n\")\n",
    "\n",
    "print(f\"3. Revenue statistics:\")\n",
    "print(f\"   - Positive revenue obs: {(df['total_revenue'] > 0).sum():,}\")\n",
    "print(f\"   - Max revenue: ${df['total_revenue'].max():,.2f}\")\n",
    "\n",
    "print(f\"\\n4. Sales statistics:\")\n",
    "print(f\"   - Positive sales obs: {(df['total_sales'] > 0).sum():,}\")\n",
    "print(f\"   - Max sales: {df['total_sales'].max()}\")\n",
    "\n",
    "# Check if we have the right columns\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"COLUMN CHECK\")\n",
    "print(\"=\"*60)\n",
    "available_cols = df.columns.tolist()\n",
    "print(\"Available columns:\", available_cols)\n",
    "\n",
    "# Look for potential click columns\n",
    "potential_click_cols = [col for col in available_cols if 'click' in col.lower()]\n",
    "potential_impression_cols = [col for col in available_cols if 'impression' in col.lower()]\n",
    "\n",
    "if potential_click_cols:\n",
    "    print(f\"\\nFound click columns: {potential_click_cols}\")\n",
    "    for col in potential_click_cols:\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "        print(f\"  {col}: sum={df[col].sum()}, max={df[col].max()}\")\n",
    "\n",
    "if potential_impression_cols:\n",
    "    print(f\"\\nFound impression columns: {potential_impression_cols}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
