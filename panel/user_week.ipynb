{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc71c62d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Generating User-Week Panel for ALL WEEKS ---\n",
      "✅ Connected to Snowflake\n",
      "Executing query...\n",
      "✅ Query successful. Fetched 32,060,768 rows.\n",
      "✅ Data successfully processed and saved to 'user_panel_full_history.parquet'\n",
      "✅ Snowflake connection closed.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "import snowflake.connector\n",
    "\n",
    "# Load environment variables from a .env file\n",
    "load_dotenv()\n",
    "\n",
    "def connect_to_snowflake():\n",
    "    \"\"\"Establishes a connection to Snowflake using environment variables.\"\"\"\n",
    "    try:\n",
    "        conn = snowflake.connector.connect(\n",
    "            user=os.getenv('SNOWFLAKE_USER'),\n",
    "            password=os.getenv('SNOWFLAKE_PASSWORD'),\n",
    "            account=os.getenv('SNOWFLAKE_ACCOUNT'),\n",
    "            warehouse=os.getenv('SNOWFLAKE_WAREHOUSE', 'COMPUTE_WH'),\n",
    "            database='INCREMENTALITY',\n",
    "            schema='INCREMENTALITY_RESEARCH'\n",
    "        )\n",
    "        print(\"✅ Connected to Snowflake\")\n",
    "        return conn\n",
    "    except snowflake.connector.Error as e:\n",
    "        print(f\"❌ Could not connect to Snowflake: {e}\", file=sys.stderr)\n",
    "        return None\n",
    "\n",
    "def build_user_panel_query(start_date=None, end_date=None):\n",
    "    \"\"\"\n",
    "    Builds the SQL query for the user-week panel.\n",
    "    If start_date and end_date are provided, it adds a WHERE clause to filter by date.\n",
    "    Otherwise, it queries the entire history.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Conditionally create the WHERE clauses\n",
    "    if start_date and end_date:\n",
    "        clicks_where_clause = f\"WHERE OCCURRED_AT >= '{start_date}' AND OCCURRED_AT < '{end_date}'\"\n",
    "        purchases_where_clause = f\"WHERE PURCHASED_AT >= '{start_date}' AND PURCHASED_AT < '{end_date}'\"\n",
    "    else:\n",
    "        clicks_where_clause = \"\"\n",
    "        purchases_where_clause = \"\"\n",
    "\n",
    "    query = f\"\"\"\n",
    "    WITH\n",
    "    -- Step 1: Aggregate clicks per user for the specified period.\n",
    "    CLICKS_WEEKLY AS (\n",
    "        SELECT\n",
    "            USER_ID,\n",
    "            DATE_TRUNC('WEEK', OCCURRED_AT) AS week,\n",
    "            COUNT(DISTINCT INTERACTION_ID) AS click_count\n",
    "        FROM CLICKS\n",
    "        {clicks_where_clause}\n",
    "        GROUP BY USER_ID, week\n",
    "    ),\n",
    "\n",
    "    -- Step 2: Aggregate purchases and revenue per user for the specified period.\n",
    "    PURCHASES_WEEKLY AS (\n",
    "        SELECT\n",
    "            USER_ID,\n",
    "            DATE_TRUNC('WEEK', PURCHASED_AT) AS week,\n",
    "            COUNT(DISTINCT PURCHASE_ID) AS purchase_count,\n",
    "            COALESCE(SUM(QUANTITY * UNIT_PRICE), 0) AS total_revenue_cents\n",
    "        FROM PURCHASES\n",
    "        {purchases_where_clause}\n",
    "        GROUP BY USER_ID, week\n",
    "    )\n",
    "\n",
    "    -- Final Step: Join user clicks and purchases for the period.\n",
    "    SELECT\n",
    "        COALESCE(c.week, p.week) AS week,\n",
    "        COALESCE(c.user_id, p.user_id) AS user_id,\n",
    "        COALESCE(c.click_count, 0) AS clicks,\n",
    "        COALESCE(p.purchase_count, 0) AS purchases,\n",
    "        (COALESCE(p.total_revenue_cents, 0) / 100)::DECIMAL(18, 2) AS revenue_dollars\n",
    "        \n",
    "    FROM CLICKS_WEEKLY AS c\n",
    "    FULL OUTER JOIN PURCHASES_WEEKLY AS p\n",
    "        ON c.user_id = p.user_id AND c.week = p.week\n",
    "    ORDER BY\n",
    "        user_id,\n",
    "        week;\n",
    "    \"\"\"\n",
    "    return query\n",
    "\n",
    "def fetch_user_panel_data(conn, query):\n",
    "    \"\"\"Executes a query and returns the result as a Pandas DataFrame.\"\"\"\n",
    "    print(\"Executing query...\")\n",
    "    cursor = conn.cursor()\n",
    "    try:\n",
    "        cursor.execute(query)\n",
    "        if cursor.description:\n",
    "            results = cursor.fetchall()\n",
    "            columns = [desc[0] for desc in cursor.description]\n",
    "            df = pd.DataFrame(results, columns=columns)\n",
    "            print(f\"✅ Query successful. Fetched {len(df):,} rows.\")\n",
    "            return df\n",
    "        return pd.DataFrame()\n",
    "    except snowflake.connector.Error as e:\n",
    "        print(f\"\\n❌ ERROR executing query: {e}\", file=sys.stderr)\n",
    "        return pd.DataFrame()\n",
    "    finally:\n",
    "        cursor.close()\n",
    "\n",
    "def process_and_save_data(df, filename):\n",
    "    \"\"\"Processes and saves the DataFrame to a Parquet file.\"\"\"\n",
    "    if df.empty:\n",
    "        print(\"No data to save.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        df.columns = [col.lower() for col in df.columns]\n",
    "        df.to_parquet(filename, index=False, engine='pyarrow')\n",
    "        print(f\"✅ Data successfully processed and saved to '{filename}'\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed to save data to '{filename}': {e}\", file=sys.stderr)\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to orchestrate the data extraction and saving process.\n",
    "    \"\"\"\n",
    "    # --- CONFIGURATION ---\n",
    "    # Choose ONE of the following configurations.\n",
    "\n",
    "    # OPTION 1: Full History (all weeks)\n",
    "    START_DATE = None\n",
    "    END_DATE = None\n",
    "    OUTPUT_FILENAME = \"user_panel_full_history.parquet\"\n",
    "    \n",
    "    # # OPTION 2: Specific Date Range (e.g., first two weeks of July 2025)\n",
    "    # START_DATE = '2025-07-01'\n",
    "    # END_DATE = '2025-07-15'   # End date is exclusive\n",
    "    # OUTPUT_FILENAME = \"user_panel_2_weeks_july_2025.parquet\"\n",
    "    \n",
    "    # ---------------------\n",
    "\n",
    "    if START_DATE:\n",
    "        print(f\"--- Generating User-Week Panel for {START_DATE} to {END_DATE} ---\")\n",
    "    else:\n",
    "        print(\"--- Generating User-Week Panel for ALL WEEKS ---\")\n",
    "\n",
    "    conn = connect_to_snowflake()\n",
    "    if not conn:\n",
    "        sys.exit(1)\n",
    "\n",
    "    try:\n",
    "        # 1. Build the appropriate SQL query\n",
    "        query = build_user_panel_query(START_DATE, END_DATE)\n",
    "        \n",
    "        # 2. Fetch data from Snowflake\n",
    "        user_panel_df = fetch_user_panel_data(conn, query)\n",
    "        \n",
    "        # 3. Process and save the data\n",
    "        process_and_save_data(user_panel_df, OUTPUT_FILENAME)\n",
    "\n",
    "    finally:\n",
    "        if conn and not conn.is_closed():\n",
    "            conn.close()\n",
    "            print(\"✅ Snowflake connection closed.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fbbd5a42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Loading and preparing data from 'user_panel_full_history.parquet' ---\n",
      "✅ Data loaded and prepared successfully.\n",
      "\n",
      "--- EDA 1: High-Level Overview ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/b7/1tvk5qmx0ds9c6gk2lrlhv380000gn/T/ipykernel_60208/1927247169.py:48: DeprecationWarning: `pl.count()` is deprecated. Please use `pl.len()` instead.\n",
      "(Deprecated in version 0.20.5)\n",
      "  pl.count().alias(\"Total Columns\"), # In Polars, pl.count() counts columns\n",
      "/var/folders/b7/1tvk5qmx0ds9c6gk2lrlhv380000gn/T/ipykernel_60208/1927247169.py:51: DeprecationWarning: `DataFrame.melt` is deprecated; use `DataFrame.unpivot` instead, with `index` instead of `id_vars` and `on` instead of `value_vars`\n",
      "  ).melt().rename({\"variable\": \"Metric\", \"value\": \"Value\"})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset Dimensions\n",
      "==================\n",
      "+---------------------------------+----------+\n",
      "| Metric                          |    Value |\n",
      "+=================================+==========+\n",
      "| Total Observations (User-Weeks) | 31657200 |\n",
      "+---------------------------------+----------+\n",
      "| Total Columns                   | 31657200 |\n",
      "+---------------------------------+----------+\n",
      "| Unique Users                    |  9183985 |\n",
      "+---------------------------------+----------+\n",
      "| Unique Weeks                    |       27 |\n",
      "+---------------------------------+----------+\n",
      "\n",
      "Column Data Types\n",
      "=================\n",
      "+--------------------+------------------------------------------+\n",
      "| column             | data_type                                |\n",
      "+====================+==========================================+\n",
      "| week               | Datetime(time_unit='ns', time_zone=None) |\n",
      "+--------------------+------------------------------------------+\n",
      "| user_id            | String                                   |\n",
      "+--------------------+------------------------------------------+\n",
      "| clicks             | Int64                                    |\n",
      "+--------------------+------------------------------------------+\n",
      "| purchases          | Int64                                    |\n",
      "+--------------------+------------------------------------------+\n",
      "| revenue_dollars    | Float64                                  |\n",
      "+--------------------+------------------------------------------+\n",
      "| log_revenue_plus_1 | Float64                                  |\n",
      "+--------------------+------------------------------------------+\n",
      "| log_clicks_plus_1  | Float64                                  |\n",
      "+--------------------+------------------------------------------+\n",
      "\n",
      "--- EDA 2: Distribution of User-Week Metrics ---\n",
      "\n",
      "Overall Distribution of Key Metrics (per User per Week)\n",
      "=======================================================\n",
      "+-------------+----------------+---------------+-------------------+\n",
      "| statistic   |         clicks |     purchases |   revenue_dollars |\n",
      "+=============+================+===============+===================+\n",
      "| count       |    3.16572e+07 |   3.16572e+07 |       3.16572e+07 |\n",
      "+-------------+----------------+---------------+-------------------+\n",
      "| null_count  |    0           |   0           |       0           |\n",
      "+-------------+----------------+---------------+-------------------+\n",
      "| mean        |    4.63451     |   0.664045    |      29.7909      |\n",
      "+-------------+----------------+---------------+-------------------+\n",
      "| std         |   12.1768      |   1.53977     |     154.139       |\n",
      "+-------------+----------------+---------------+-------------------+\n",
      "| min         |    0           |   0           |       0           |\n",
      "+-------------+----------------+---------------+-------------------+\n",
      "| 25%         |    1           |   0           |       0           |\n",
      "+-------------+----------------+---------------+-------------------+\n",
      "| 50%         |    2           |   0           |       0           |\n",
      "+-------------+----------------+---------------+-------------------+\n",
      "| 75%         |    4           |   1           |      25           |\n",
      "+-------------+----------------+---------------+-------------------+\n",
      "| max         | 4035           | 540           |  166184           |\n",
      "+-------------+----------------+---------------+-------------------+\n",
      "\n",
      "User-Weeks with Zero Revenue: 61.81% of observations have zero revenue.\n",
      "\n",
      "Distribution for Purchasing User-Weeks ONLY\n",
      "===========================================\n",
      "+-------------+----------------+---------------+-------------------+\n",
      "| statistic   |         clicks |     purchases |   revenue_dollars |\n",
      "+=============+================+===============+===================+\n",
      "| count       |    1.20886e+07 |   1.20886e+07 |       1.20886e+07 |\n",
      "+-------------+----------------+---------------+-------------------+\n",
      "| null_count  |    0           |   0           |       0           |\n",
      "+-------------+----------------+---------------+-------------------+\n",
      "| mean        |    5.59634     |   1.73898     |      78.0156      |\n",
      "+-------------+----------------+---------------+-------------------+\n",
      "| std         |   15.5257      |   2.08315     |     241.779       |\n",
      "+-------------+----------------+---------------+-------------------+\n",
      "| min         |    0           |   1           |       3           |\n",
      "+-------------+----------------+---------------+-------------------+\n",
      "| 25%         |    0           |   1           |      20           |\n",
      "+-------------+----------------+---------------+-------------------+\n",
      "| 50%         |    1           |   1           |      38           |\n",
      "+-------------+----------------+---------------+-------------------+\n",
      "| 75%         |    5           |   2           |      75           |\n",
      "+-------------+----------------+---------------+-------------------+\n",
      "| max         | 4035           | 540           |  166184           |\n",
      "+-------------+----------------+---------------+-------------------+\n",
      "\n",
      "--- EDA 3: Correlation Between Clicks and Revenue ---\n",
      "\n",
      "Correlation Matrix (Raw Values)\n",
      "===============================\n",
      "+-----------------+-----------+-------------+-------------------+\n",
      "| index           |    clicks |   purchases |   revenue_dollars |\n",
      "+=================+===========+=============+===================+\n",
      "| clicks          | 1         |    0.213796 |         0.0742036 |\n",
      "+-----------------+-----------+-------------+-------------------+\n",
      "| purchases       | 0.213796  |    1        |         0.45974   |\n",
      "+-----------------+-----------+-------------+-------------------+\n",
      "| revenue_dollars | 0.0742036 |    0.45974  |         1         |\n",
      "+-----------------+-----------+-------------+-------------------+\n",
      "\n",
      "Correlation Matrix (Log-Transformed Values)\n",
      "===========================================\n",
      "+--------------------+---------------------+----------------------+\n",
      "| index              |   log_clicks_plus_1 |   log_revenue_plus_1 |\n",
      "+====================+=====================+======================+\n",
      "| log_clicks_plus_1  |           1         |           -0.0721496 |\n",
      "+--------------------+---------------------+----------------------+\n",
      "| log_revenue_plus_1 |          -0.0721496 |            1         |\n",
      "+--------------------+---------------------+----------------------+\n",
      "\n",
      "Interpretation: Correlation shows the strength of the linear relationship between user clicks\n",
      "and their spending. The log-transformed value is often more stable for skewed data like revenue.\n",
      "\n",
      "--- EDA 4: Top User Analysis (Aggregated Across the Period) ---\n",
      "\n",
      "Top 15 Users by Total Revenue\n",
      "=============================\n",
      "+-------------------------------------------+----------------+-------------------+-----------------+----------------+\n",
      "| user_id                                   |   total_clicks |   total_purchases | total_revenue   |   weeks_active |\n",
      "+===========================================+================+===================+=================+================+\n",
      "| ext1:2e3a2fea-47f4-488f-9877-4fddb9b2c2ce |             36 |               360 | $691,041.00     |             13 |\n",
      "+-------------------------------------------+----------------+-------------------+-----------------+----------------+\n",
      "| ext1:6792dc49-72b5-49a6-979d-3c5c642b1e3b |            185 |               550 | $484,276.00     |             26 |\n",
      "+-------------------------------------------+----------------+-------------------+-----------------+----------------+\n",
      "| ext1:cddf1a1b-4ab3-4740-9681-7fb49bd84999 |            617 |               573 | $428,814.00     |             22 |\n",
      "+-------------------------------------------+----------------+-------------------+-----------------+----------------+\n",
      "| ext1:6d4e19cc-05aa-48b2-9330-b305ab0813ec |             77 |               159 | $376,976.00     |             26 |\n",
      "+-------------------------------------------+----------------+-------------------+-----------------+----------------+\n",
      "| ext1:3f953486-61fe-4af9-944e-006e5df74d61 |            542 |               260 | $276,910.00     |             26 |\n",
      "+-------------------------------------------+----------------+-------------------+-----------------+----------------+\n",
      "| ext1:893abd71-f8b3-4a55-a89d-6cd0aa7c7d64 |            275 |               431 | $236,009.00     |             25 |\n",
      "+-------------------------------------------+----------------+-------------------+-----------------+----------------+\n",
      "| ext1:f82e7a91-8d04-48eb-ab01-0ad94e817758 |            175 |               142 | $214,750.00     |             26 |\n",
      "+-------------------------------------------+----------------+-------------------+-----------------+----------------+\n",
      "| ext1:7f717924-80a0-4af9-a143-37894b688127 |             71 |               230 | $213,848.00     |             24 |\n",
      "+-------------------------------------------+----------------+-------------------+-----------------+----------------+\n",
      "| ext1:ce8dc18e-972b-4853-bc8a-ad73f847031c |            682 |               237 | $208,651.00     |             27 |\n",
      "+-------------------------------------------+----------------+-------------------+-----------------+----------------+\n",
      "| ext1:6253ed97-3b9e-4bc6-9934-00e41b6d53a5 |            280 |               432 | $198,340.00     |             26 |\n",
      "+-------------------------------------------+----------------+-------------------+-----------------+----------------+\n",
      "| ext1:26055f69-f14d-4bfa-a51f-ceee6bf69816 |             26 |                38 | $196,486.00     |             20 |\n",
      "+-------------------------------------------+----------------+-------------------+-----------------+----------------+\n",
      "| ext1:17f94d05-6a1a-426c-91dc-f553a8c937b1 |            162 |               861 | $191,054.00     |             23 |\n",
      "+-------------------------------------------+----------------+-------------------+-----------------+----------------+\n",
      "| ext1:bee45c70-463e-423f-a4d5-e0867b3a937a |            237 |               800 | $184,031.00     |             26 |\n",
      "+-------------------------------------------+----------------+-------------------+-----------------+----------------+\n",
      "| ext1:d84f665c-9136-4014-8387-2d2cad2c191d |            778 |               642 | $183,894.00     |             26 |\n",
      "+-------------------------------------------+----------------+-------------------+-----------------+----------------+\n",
      "| ext1:e068f113-6c40-47e2-9766-89cb435086a9 |            202 |               382 | $178,380.00     |             26 |\n",
      "+-------------------------------------------+----------------+-------------------+-----------------+----------------+\n",
      "\n",
      "Interpretation: This table helps identify if a small number of 'whale' users are responsible\n",
      "for a large portion of the period's total revenue.\n",
      "\n",
      "--- EDA 5: Identifying Potential Holdout & User Groups ---\n",
      "   Aggregating data to user level for analysis...\n",
      "   Analyzed 9,183,985 unique users.\n",
      "\n",
      "Summary of Key User Segments\n",
      "============================\n",
      "+----------------------------------------+--------------+-----------------------+\n",
      "| User Segment                           |   User Count |   Percentage of Total |\n",
      "+========================================+==============+=======================+\n",
      "| Total Unique Users                     |      9183985 |                100    |\n",
      "+----------------------------------------+--------------+-----------------------+\n",
      "| Organic Purchasers (Zero Clicks)       |      1656111 |                 18.03 |\n",
      "+----------------------------------------+--------------+-----------------------+\n",
      "| Late Joiners (Joined after first week) |      8612239 |                 93.77 |\n",
      "+----------------------------------------+--------------+-----------------------+\n",
      "\n",
      "Interpretation:\n",
      " - Organic Purchasers: This group represents your baseline organic conversion. They buy without\n",
      "   clicking on tracked ads. They are a perfect 'never-treated' control group for analysis.\n",
      " - Late Joiners: This shows how many users are new to the platform within the dataset's timeframe.\n",
      "   A high number indicates a growing user base but can complicate longitudinal analysis.\n",
      "\n",
      "Behavior of 'Organic Purchaser' Segment\n",
      "=======================================\n",
      "+-----------------------+---------------------+---------------------+\n",
      "|   avg_total_purchases |   avg_total_revenue | sum_total_revenue   |\n",
      "+=======================+=====================+=====================+\n",
      "|               1.29457 |             71.0831 | $117,721,486        |\n",
      "+-----------------------+---------------------+---------------------+\n",
      "\n",
      "Interpretation:\n",
      " - This table shows the average behavior and total economic value of users who purchase\n",
      "   without ever clicking on a tracked advertisement.\n"
     ]
    }
   ],
   "source": [
    "import polars as pl\n",
    "from tabulate import tabulate\n",
    "\n",
    "# --- Helper Function (Modified to work with Polars) ---\n",
    "def show_table(df, title=\"\"):\n",
    "    \"\"\"Prints a Polars DataFrame in a formatted grid table by converting it to Pandas.\"\"\"\n",
    "    if title:\n",
    "        print(f\"\\n{title}\")\n",
    "        print(\"=\"*len(title))\n",
    "    # Tabulate works best with Pandas, so we convert just for printing\n",
    "    print(tabulate(df.to_pandas(), headers='keys', tablefmt='grid', showindex=False))\n",
    "\n",
    "# --- Prerequisite: Load and Prepare Data ---\n",
    "try:\n",
    "    parquet_filename = 'user_panel_full_history.parquet'\n",
    "    \n",
    "    print(f\"--- Loading and preparing data from '{parquet_filename}' ---\")\n",
    "    \n",
    "    # Load data using Polars and chain data preparation steps\n",
    "    df_analysis = pl.read_parquet(parquet_filename).with_columns(\n",
    "        \n",
    "        # --- FIX: Cast the Decimal column to Float64 at the source ---\n",
    "        # This prevents overflow errors in all downstream aggregations (like .sum()).\n",
    "        pl.col(\"revenue_dollars\").cast(pl.Float64),\n",
    "        \n",
    "        # Create log-transformed variables (these work fine on the new Float64 column)\n",
    "        pl.col(\"revenue_dollars\").log1p().alias(\"log_revenue_plus_1\"),\n",
    "        pl.col(\"clicks\").log1p().alias(\"log_clicks_plus_1\")\n",
    "    )\n",
    "    print(f\"✅ Data loaded and prepared successfully.\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"❌ ERROR: The file '{parquet_filename}' was not found.\")\n",
    "    print(\"   Please run the data generation script first or check the filename.\")\n",
    "    df_analysis = None\n",
    "\n",
    "# --- Main EDA Block ---\n",
    "# All analysis is contained within this block to ensure it only runs if data loading was successful.\n",
    "if df_analysis is not None and len(df_analysis) > 0:\n",
    "\n",
    "    # --- EDA 1: High-Level Overview ---\n",
    "    print(\"\\n--- EDA 1: High-Level Overview ---\")\n",
    "    num_rows, num_cols = df_analysis.shape\n",
    "    \n",
    "    # Use Polars expressions to get unique counts\n",
    "    summary_stats = df_analysis.select(\n",
    "        pl.len().alias(\"Total Observations (User-Weeks)\"),\n",
    "        pl.count().alias(\"Total Columns\"), # In Polars, pl.count() counts columns\n",
    "        pl.col(\"user_id\").n_unique().alias(\"Unique Users\"),\n",
    "        pl.col(\"week\").n_unique().alias(\"Unique Weeks\")\n",
    "    ).melt().rename({\"variable\": \"Metric\", \"value\": \"Value\"})\n",
    "    \n",
    "    show_table(summary_stats, \"Dataset Dimensions\")\n",
    "    \n",
    "    # Get schema information\n",
    "    schema_df = pl.DataFrame({\n",
    "        \"column\": df_analysis.columns,\n",
    "        \"data_type\": df_analysis.dtypes\n",
    "    })\n",
    "    show_table(schema_df, \"Column Data Types\")\n",
    "\n",
    "    # --- EDA 2: Distribution of Core Metrics ---\n",
    "    print(\"\\n--- EDA 2: Distribution of User-Week Metrics ---\")\n",
    "    \n",
    "    # Get descriptive statistics using Polars\n",
    "    desc_stats = df_analysis.select(['clicks', 'purchases', 'revenue_dollars']).describe()\n",
    "    show_table(desc_stats, \"Overall Distribution of Key Metrics (per User per Week)\")\n",
    "               \n",
    "    # Calculate zero revenue percentage using Polars expression\n",
    "    zero_revenue_pct = df_analysis.select(\n",
    "        (pl.col('revenue_dollars') == 0).mean() * 100\n",
    "    ).item()\n",
    "    print(f\"\\nUser-Weeks with Zero Revenue: {zero_revenue_pct:.2f}% of observations have zero revenue.\")\n",
    "    \n",
    "    # Filter for purchasing users and get their stats\n",
    "    df_purchasers = df_analysis.filter(pl.col('revenue_dollars') > 0)\n",
    "    show_table(df_purchasers.select(['clicks', 'purchases', 'revenue_dollars']).describe(),\n",
    "               \"Distribution for Purchasing User-Weeks ONLY\")\n",
    "\n",
    "    # --- EDA 3: Correlation Analysis ---\n",
    "    print(\"\\n--- EDA 3: Correlation Between Clicks and Revenue ---\")\n",
    "    \n",
    "    # Polars calculates correlations pairwise. For a matrix, it's often easiest to bridge to Pandas.\n",
    "    corr_raw_pd = df_analysis.select(['clicks', 'purchases', 'revenue_dollars']).to_pandas().corr()\n",
    "    show_table(pl.from_pandas(corr_raw_pd.reset_index()), \"Correlation Matrix (Raw Values)\")\n",
    "    \n",
    "    corr_log_pd = df_analysis.select(['log_clicks_plus_1', 'log_revenue_plus_1']).to_pandas().corr()\n",
    "    show_table(pl.from_pandas(corr_log_pd.reset_index()), \"Correlation Matrix (Log-Transformed Values)\")\n",
    "    \n",
    "    print(\"\\nInterpretation: Correlation shows the strength of the linear relationship between user clicks\")\n",
    "    print(\"and their spending. The log-transformed value is often more stable for skewed data like revenue.\")\n",
    "\n",
    "    # --- EDA 4: Top User Summary ---\n",
    "    print(\"\\n--- EDA 4: Top User Analysis (Aggregated Across the Period) ---\")\n",
    "    \n",
    "    # Perform aggregation using Polars' group_by().agg() syntax\n",
    "    user_summary = (\n",
    "        df_analysis.group_by('user_id')\n",
    "        .agg(\n",
    "            pl.col('clicks').sum().alias('total_clicks'),\n",
    "            pl.col('purchases').sum().alias('total_purchases'),\n",
    "            pl.col('revenue_dollars').sum().alias('total_revenue'),\n",
    "            pl.col('week').n_unique().alias('weeks_active')\n",
    "        )\n",
    "        .sort('total_revenue', descending=True)\n",
    "        .head(15)\n",
    "    )\n",
    "    \n",
    "    # Format the revenue column for better readability using `map_elements`\n",
    "    user_summary = user_summary.with_columns(\n",
    "        pl.col('total_revenue').map_elements(lambda x: f\"${x:,.2f}\", return_dtype=pl.String)\n",
    "    )\n",
    "    \n",
    "    show_table(user_summary, \"Top 15 Users by Total Revenue\")\n",
    "    print(\"\\nInterpretation: This table helps identify if a small number of 'whale' users are responsible\")\n",
    "    print(\"for a large portion of the period's total revenue.\")\n",
    "\n",
    "else:\n",
    "    print(\"\\nCould not perform EDA because the DataFrame is empty or could not be loaded.\")\n",
    "\n",
    "# --- EDA 5: Identifying Potential Holdout & User Groups ---\n",
    "print(\"\\n--- EDA 5: Identifying Potential Holdout & User Groups ---\")\n",
    "\n",
    "# To analyze lifetime behavior, we first need to aggregate the panel to the user level.\n",
    "# This creates a summary for every unique user in the dataset.\n",
    "print(\"   Aggregating data to user level for analysis...\")\n",
    "user_summary_full = (\n",
    "    df_analysis.group_by('user_id')\n",
    "    .agg(\n",
    "        pl.col('clicks').sum().alias('total_clicks'),\n",
    "        pl.col('purchases').sum().alias('total_purchases'),\n",
    "        pl.col('revenue_dollars').sum().alias('total_revenue'),\n",
    "        pl.col('week').min().alias('first_seen_week')\n",
    "    )\n",
    ")\n",
    "total_users = len(user_summary_full)\n",
    "print(f\"   Analyzed {total_users:,} unique users.\")\n",
    "\n",
    "# --- Analysis 1: Find \"Organic Purchasers\" (Potential Holdout Group) ---\n",
    "# These are users who convert without any recorded clicks. They are crucial for\n",
    "# understanding the baseline conversion rate of your platform.\n",
    "\n",
    "organic_purchasers = user_summary_full.filter(\n",
    "    (pl.col(\"total_clicks\") == 0) & (pl.col(\"total_purchases\") > 0)\n",
    ")\n",
    "count_organic = len(organic_purchasers)\n",
    "\n",
    "# --- Analysis 2: Find \"Late Joiners\" ---\n",
    "# These are users who were not present at the start of the data period.\n",
    "# A large number of late joiners means the panel is not \"balanced\", which is\n",
    "# important to know for certain types of models.\n",
    "\n",
    "first_week_in_dataset = df_analysis.select(pl.col(\"week\").min()).item()\n",
    "\n",
    "late_joiners = user_summary_full.filter(\n",
    "    pl.col(\"first_seen_week\") > first_week_in_dataset\n",
    ")\n",
    "count_late = len(late_joiners)\n",
    "\n",
    "# --- Present the Results ---\n",
    "\n",
    "summary_data = pl.DataFrame({\n",
    "    \"User Segment\": [\n",
    "        \"Total Unique Users\",\n",
    "        \"Organic Purchasers (Zero Clicks)\",\n",
    "        \"Late Joiners (Joined after first week)\"\n",
    "    ],\n",
    "    \"User Count\": [\n",
    "        total_users,\n",
    "        count_organic,\n",
    "        count_late\n",
    "    ]\n",
    "}).with_columns(\n",
    "    # Calculate percentage of total for each group\n",
    "    (pl.col(\"User Count\") / total_users * 100).round(2).alias(\"Percentage of Total\")\n",
    ")\n",
    "\n",
    "show_table(summary_data, \"Summary of Key User Segments\")\n",
    "\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\" - Organic Purchasers: This group represents your baseline organic conversion. They buy without\")\n",
    "print(\"   clicking on tracked ads. They are a perfect 'never-treated' control group for analysis.\")\n",
    "print(\" - Late Joiners: This shows how many users are new to the platform within the dataset's timeframe.\")\n",
    "print(\"   A high number indicates a growing user base but can complicate longitudinal analysis.\")\n",
    "\n",
    "# --- Deeper Dive into the Organic Purchasers ---\n",
    "if count_organic > 0:\n",
    "    # Calculate the economic value of this 'holdout' group\n",
    "    organic_purchaser_stats = organic_purchasers.select(\n",
    "        pl.col(\"total_purchases\").mean().alias(\"avg_total_purchases\"),\n",
    "        pl.col(\"total_revenue\").mean().alias(\"avg_total_revenue\"),\n",
    "        pl.col(\"total_revenue\").sum().alias(\"sum_total_revenue\")\n",
    "    )\n",
    "    \n",
    "    # Format the sum of revenue for readability\n",
    "    formatted_stats = organic_purchaser_stats.with_columns(\n",
    "        pl.col('sum_total_revenue').map_elements(lambda x: f\"${x:,.0f}\", return_dtype=pl.String)\n",
    "    )\n",
    "    \n",
    "    show_table(formatted_stats, \"Behavior of 'Organic Purchaser' Segment\")\n",
    "    print(\"\\nInterpretation:\")\n",
    "    print(\" - This table shows the average behavior and total economic value of users who purchase\")\n",
    "    print(\"   without ever clicking on a tracked advertisement.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b408fc4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ rpy2 environment and 'fixest' library loaded successfully.\n",
      "--- Loading panel data from 'user_panel_full_history.parquet' ---\n",
      "--- Sub-sampling 25% of users... ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/b7/1tvk5qmx0ds9c6gk2lrlhv380000gn/T/ipykernel_60208/249180285.py:52: DeprecationWarning: `is_in` with a collection of the same datatype is ambiguous and deprecated.\n",
      "Please use `implode` to return to previous behavior.\n",
      "\n",
      "See https://github.com/pola-rs/polars/issues/22149 for more information.\n",
      "  df = df.filter(pl.col(\"user_id\").is_in(sampled_users))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Sampled data contains 7,912,264 rows from 2,295,996 users.\n",
      "\n",
      "--- Estimating Model: log(revenue+1) ~ log(clicks+1) | user_id + week ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pranjal/Code/topsort-incrementality/venv/lib/python3.13/site-packages/rpy2/robjects/pandas2ri.py:65: UserWarning: Error while trying to convert the column \"revenue_dollars\". Fall back to string conversion. The error is: <class 'decimal.Decimal'>\n",
      "  warnings.warn('Error while trying to convert '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       model_user_fe\n",
      "Dependent Var.:   log_revenue_plus_1\n",
      "                                    \n",
      "log_clicks_plus_1 0.0282*** (0.0015)\n",
      "Fixed-Effects:    ------------------\n",
      "user_id                          Yes\n",
      "week                             Yes\n",
      "_________________ __________________\n",
      "S.E.: Clustered          by: user_id\n",
      "Observations               7,912,264\n",
      "R2                           0.46165\n",
      "Within R2                    0.00014\n",
      "---\n",
      "Signif. codes: 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n",
      "\n",
      "✅ Fixed-effects model estimated successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import polars as pl\n",
    "\n",
    "# --- 1. Set up rpy2 Environment for fixest ---\n",
    "# --- 1. Set up rpy2 Environment for fixest ---\n",
    "# This function isolates the rpy2 setup.\n",
    "def setup_rpy2():\n",
    "    \"\"\"Initializes the rpy2 environment and returns key objects.\"\"\"\n",
    "    try:\n",
    "        import rpy2.robjects as ro\n",
    "        from rpy2.robjects.packages import importr\n",
    "        from rpy2.robjects import pandas2ri\n",
    "        from rpy2.robjects.conversion import localconverter\n",
    "\n",
    "        # --- LINE REMOVED ---\n",
    "        # pandas2ri.activate() # This line was deprecated and is not needed.\n",
    "        # The 'with localconverter(...)' context manager in the modeling\n",
    "        # function is the correct and modern way to handle conversion.\n",
    "        \n",
    "        # Load R libraries\n",
    "        importr('fixest')\n",
    "        \n",
    "        print(\"✅ rpy2 environment and 'fixest' library loaded successfully.\")\n",
    "        return True, ro, pandas2ri, localconverter\n",
    "    except (ImportError, RuntimeError) as e:\n",
    "        print(f\"❌ ERROR: Failed to set up rpy2 or load R libraries: {e}\", file=sys.stderr)\n",
    "        print(\"   Please ensure R is installed and `fixest` is available in the R environment.\", file=sys.stderr)\n",
    "        return False, None, None, None\n",
    "\n",
    "# --- 2. Load, Prepare, and Sub-sample Data ---\n",
    "def load_and_prepare_data(filename, sample_fraction=1.0):\n",
    "    \"\"\"\n",
    "    Loads user-panel data using Polars, prepares log-transformed columns,\n",
    "    and optionally samples a fraction of users.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"--- Loading panel data from '{filename}' ---\")\n",
    "        df = pl.read_parquet(filename)\n",
    "\n",
    "        # Prepare log-transformed variables using Polars expressions\n",
    "        df = df.with_columns(\n",
    "            pl.col(\"revenue_dollars\").log1p().alias(\"log_revenue_plus_1\"),\n",
    "            pl.col(\"clicks\").log1p().alias(\"log_clicks_plus_1\")\n",
    "        )\n",
    "\n",
    "        # Handle sub-sampling at the user level\n",
    "        if sample_fraction < 1.0:\n",
    "            print(f\"--- Sub-sampling {sample_fraction:.0%} of users... ---\")\n",
    "            unique_users = df.get_column(\"user_id\").unique()\n",
    "            sampled_users = unique_users.sample(fraction=sample_fraction, shuffle=True)\n",
    "            df = df.filter(pl.col(\"user_id\").is_in(sampled_users))\n",
    "            print(f\"✅ Sampled data contains {df.height:,} rows from {sampled_users.len():,} users.\")\n",
    "        else:\n",
    "            print(f\"✅ Loaded and prepared full panel with {df.height:,} rows.\")\n",
    "        \n",
    "        return df\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"❌ ERROR: The file '{filename}' was not found.\", file=sys.stderr)\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"❌ ERROR during data loading or preparation: {e}\", file=sys.stderr)\n",
    "        return None\n",
    "\n",
    "# --- 3. Run the Fixed-Effects Model in R ---\n",
    "def run_fixed_effects_model(df, ro, pandas2ri, localconverter):\n",
    "    \"\"\"\n",
    "    Transfers the DataFrame to R and runs a two-way fixed-effects model.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Estimating Model: log(revenue+1) ~ log(clicks+1) | user_id + week ---\")\n",
    "    \n",
    "    try:\n",
    "        # Convert Polars to Pandas for rpy2 compatibility\n",
    "        df_pd = df.to_pandas()\n",
    "\n",
    "        # Transfer the data to the R global environment\n",
    "        with localconverter(ro.default_converter + pandas2ri.converter):\n",
    "            ro.globalenv['df_for_r'] = pandas2ri.py2rpy(df_pd)\n",
    "\n",
    "        # Execute the R code for the fixest model\n",
    "        ro.r(\"\"\"\n",
    "        library(fixest)\n",
    "        df_panel <- df_for_r\n",
    "        \n",
    "        # Estimate the model with user and week fixed-effects.\n",
    "        # Standard errors are clustered at the user level to account for\n",
    "        # correlation in errors for the same user over time.\n",
    "        model_user_fe <- feols(\n",
    "            log_revenue_plus_1 ~ log_clicks_plus_1 | user_id + week, \n",
    "            data = df_panel, \n",
    "            vcov = ~user_id\n",
    "        )\n",
    "        \n",
    "        # Print the results table\n",
    "        print(etable(model_user_fe, digits = 4))\n",
    "        \"\"\")\n",
    "        \n",
    "        print(\"\\n✅ Fixed-effects model estimated successfully.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ ERROR running the fixest model in R: {e}\", file=sys.stderr)\n",
    "\n",
    "# --- Main Execution Block ---\n",
    "def main():\n",
    "    \"\"\"Orchestrates the loading, sampling, and modeling process.\"\"\"\n",
    "    \n",
    "    # --- CONFIGURATION ---\n",
    "    # Set the input filename and the fraction of users to sample (1.0 for all users).\n",
    "    FILENAME = 'user_panel_full_history.parquet'\n",
    "    SAMPLE_FRACTION = 0.25  # Use 10% of users for a quick test. Set to 1.0 for the full dataset.\n",
    "    # ---------------------\n",
    "\n",
    "    rpy2_is_ready, ro, pandas2ri, localconverter = setup_rpy2()\n",
    "    if not rpy2_is_ready:\n",
    "        sys.exit(1)\n",
    "\n",
    "    df_analysis = load_and_prepare_data(FILENAME, sample_fraction=SAMPLE_FRACTION)\n",
    "\n",
    "    if df_analysis is not None and not df_analysis.is_empty():\n",
    "        run_fixed_effects_model(df_analysis, ro, pandas2ri, localconverter)\n",
    "    else:\n",
    "        print(\"Skipping model estimation due to data loading issues.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "236031cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ rpy2 environment and 'lme4' library loaded successfully.\n",
      "--- Loading panel data from 'user_panel_full_history.parquet' ---\n",
      "--- Sub-sampling 10.0% of users for modeling... ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/b7/1tvk5qmx0ds9c6gk2lrlhv380000gn/T/ipykernel_60208/1008257996.py:44: DeprecationWarning: `is_in` with a collection of the same datatype is ambiguous and deprecated.\n",
      "Please use `implode` to return to previous behavior.\n",
      "\n",
      "See https://github.com/pola-rs/polars/issues/22149 for more information.\n",
      "  df = df.filter(pl.col(\"user_id\").is_in(sampled_users))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Sampled data contains 3,166,474 rows from 918,398 users.\n",
      "\n",
      "--- Preparing data for mixed-effects model ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/b7/1tvk5qmx0ds9c6gk2lrlhv380000gn/T/ipykernel_60208/1008257996.py:65: DeprecationWarning: `is_in` with a collection of the same datatype is ambiguous and deprecated.\n",
      "Please use `implode` to return to previous behavior.\n",
      "\n",
      "See https://github.com/pola-rs/polars/issues/22149 for more information.\n",
      "  df_clean = df.filter(pl.col(\"user_id\").is_in(users_with_enough_data))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Using 2,697,618 rows from 449,542 users with >1 observation.\n",
      "✅ Scaled 'log_clicks' to improve model stability.\n",
      "\n",
      "--- Estimating Mixed-Effects Model with `lme4` to get individual slopes (βu) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "R callback write-console: In addition:   \n",
      "R callback write-console: Warning message:\n",
      "  \n",
      "R callback write-console: In checkConv(attr(opt, \"derivs\"), opt$par, ctrl = control$checkConv,  :  \n",
      "R callback write-console: \n",
      "   \n",
      "R callback write-console:  Model is nearly unidentifiable: very large eigenvalue\n",
      " - Rescale variables?\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Mixed-effects model estimated successfully.\n",
      "\n",
      "--- Summary of Regularized User-Specific Slopes (βu) ---\n",
      "shape: (9, 3)\n",
      "┌────────────┬─────────────────────────────────┬───────────┐\n",
      "│ statistic  ┆ user_id                         ┆ beta_u    │\n",
      "│ ---        ┆ ---                             ┆ ---       │\n",
      "│ str        ┆ str                             ┆ f64       │\n",
      "╞════════════╪═════════════════════════════════╪═══════════╡\n",
      "│ count      ┆ 449542                          ┆ 449542.0  │\n",
      "│ null_count ┆ 0                               ┆ 0.0       │\n",
      "│ mean       ┆ null                            ┆ -0.292076 │\n",
      "│ std        ┆ null                            ┆ 0.485608  │\n",
      "│ min        ┆ ext1:000006d9-686c-44a0-a494-1… ┆ -3.118796 │\n",
      "│ 25%        ┆ null                            ┆ -0.618303 │\n",
      "│ 50%        ┆ null                            ┆ -0.25332  │\n",
      "│ 75%        ┆ null                            ┆ -0.049514 │\n",
      "│ max        ┆ ext1:fffff588-b1c8-4e2c-bf84-e… ┆ 2.511748  │\n",
      "└────────────┴─────────────────────────────────┴───────────┘\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import polars as pl\n",
    "\n",
    "# --- 1. Set up rpy2 Environment ---\n",
    "def setup_rpy2():\n",
    "    \"\"\"Initializes the rpy2 environment and returns key objects.\"\"\"\n",
    "    try:\n",
    "        import rpy2.robjects as ro\n",
    "        from rpy2.robjects.packages import importr\n",
    "        from rpy2.robjects import pandas2ri\n",
    "        from rpy2.robjects.conversion import localconverter\n",
    "        \n",
    "        # Load R libraries needed for the model\n",
    "        importr('lme4')\n",
    "        \n",
    "        print(\"✅ rpy2 environment and 'lme4' library loaded successfully.\")\n",
    "        return True, ro, pandas2ri, localconverter\n",
    "    except (ImportError, RuntimeError) as e:\n",
    "        print(f\"❌ ERROR: Failed to set up rpy2 or load R libraries: {e}\", file=sys.stderr)\n",
    "        print(\"   Please ensure R is installed and `lme4` is available in the R environment.\", file=sys.stderr)\n",
    "        return False, None, None, None\n",
    "\n",
    "# --- 2. Load and Prepare Data ---\n",
    "def load_and_prepare_data(filename, sample_fraction=1.0):\n",
    "    \"\"\"\n",
    "    Loads user-panel data using Polars, prepares log columns, and samples users.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"--- Loading panel data from '{filename}' ---\")\n",
    "        df = (\n",
    "            pl.read_parquet(filename)\n",
    "            .with_columns(\n",
    "                pl.col(\"revenue_dollars\").cast(pl.Float64),\n",
    "                pl.col(\"revenue_dollars\").log1p().alias(\"log_revenue_plus_1\"),\n",
    "                pl.col(\"clicks\").log1p().alias(\"log_clicks_plus_1\")\n",
    "            )\n",
    "        )\n",
    "\n",
    "        if sample_fraction < 1.0:\n",
    "            print(f\"--- Sub-sampling {sample_fraction:.1%} of users for modeling... ---\")\n",
    "            unique_users = df.get_column(\"user_id\").unique()\n",
    "            sampled_users = unique_users.sample(fraction=sample_fraction, shuffle=True)\n",
    "            df = df.filter(pl.col(\"user_id\").is_in(sampled_users))\n",
    "            print(f\"✅ Sampled data contains {df.height:,} rows from {sampled_users.len():,} users.\")\n",
    "        else:\n",
    "            print(f\"✅ Loaded and prepared full panel with {df.height:,} rows.\")\n",
    "        \n",
    "        return df\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"❌ ERROR: The file '{filename}' was not found.\", file=sys.stderr)\n",
    "        return None\n",
    "\n",
    "# --- 3. Run Mixed-Effects Model to get Beta_u ---\n",
    "def run_mixed_effects_model(df, ro, pandas2ri, localconverter):\n",
    "    \"\"\"\n",
    "    Runs a mixed-effects model in R (lme4) to estimate user-specific slopes (beta_u).\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Preparing data for mixed-effects model ---\")\n",
    "    \n",
    "    # 1. Filter for users with enough data to estimate a slope\n",
    "    user_counts = df.group_by(\"user_id\").len()\n",
    "    users_with_enough_data = user_counts.filter(pl.col(\"len\") > 1).get_column(\"user_id\")\n",
    "    df_clean = df.filter(pl.col(\"user_id\").is_in(users_with_enough_data))\n",
    "    \n",
    "    # 2. Scale the predictor variable for model stability\n",
    "    log_clicks_std = df_clean.get_column(\"log_clicks_plus_1\").std()\n",
    "    df_model_ready = df_clean.with_columns(\n",
    "        ((pl.col(\"log_clicks_plus_1\") - pl.col(\"log_clicks_plus_1\").mean()) / log_clicks_std)\n",
    "        .alias(\"log_clicks_scaled\")\n",
    "    )\n",
    "    print(f\"✅ Using {df_model_ready.height:,} rows from {users_with_enough_data.len():,} users with >1 observation.\")\n",
    "    print(f\"✅ Scaled 'log_clicks' to improve model stability.\")\n",
    "\n",
    "    print(\"\\n--- Estimating Mixed-Effects Model with `lme4` to get individual slopes (βu) ---\")\n",
    "    try:\n",
    "        # Transfer data and scaling factor to R\n",
    "        with localconverter(ro.default_converter + pandas2ri.converter):\n",
    "            ro.globalenv['df_for_r'] = df_model_ready.to_pandas()\n",
    "            ro.globalenv['std_log_clicks_r'] = log_clicks_std\n",
    "\n",
    "        # Execute R code to run the model and extract coefficients\n",
    "        ro.r(\"\"\"\n",
    "        library(lme4)\n",
    "        \n",
    "        # This formula estimates a fixed effect for scaled clicks, plus a random\n",
    "        # intercept and slope for each user, and a random intercept for each week.\n",
    "        # The '||' syntax assumes uncorrelated random effects for stability.\n",
    "        lmer_model <- lmer(\n",
    "            log_revenue_plus_1 ~ log_clicks_scaled + (log_clicks_scaled || user_id) + (1 | week),\n",
    "            data = df_for_r,\n",
    "            control = lmerControl(optimizer = \"bobyqa\", optCtrl = list(maxfun = 2e5))\n",
    "        )\n",
    "        \n",
    "        # Extract the final (regularized) coefficients for each user\n",
    "        user_coefs <- coef(lmer_model)$user_id\n",
    "        \n",
    "        # Get the scaled slope (beta) for each user\n",
    "        beta_u_scaled <- user_coefs[, \"log_clicks_scaled\"]\n",
    "        \n",
    "        # Un-scale the beta to make it interpretable in original units\n",
    "        # beta_original = beta_scaled / std_dev\n",
    "        beta_u_unscaled <- beta_u_scaled / std_log_clicks_r\n",
    "        \n",
    "        # Create a final data frame with user_id and their unscaled beta\n",
    "        results_df <- data.frame(\n",
    "            user_id = rownames(user_coefs), \n",
    "            beta_u = beta_u_unscaled\n",
    "        )\n",
    "        \"\"\")\n",
    "        \n",
    "        print(\"\\n✅ Mixed-effects model estimated successfully.\")\n",
    "        \n",
    "        # Bring the results back into a Polars DataFrame\n",
    "        with localconverter(ro.default_converter + pandas2ri.converter):\n",
    "            results_pd = ro.r['results_df']\n",
    "        return pl.from_pandas(results_pd)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ ERROR running the lme4 model in R: {e}\", file=sys.stderr)\n",
    "        return None\n",
    "\n",
    "# --- Main Execution Block ---\n",
    "def main():\n",
    "    \"\"\"Orchestrates the data loading and modeling process.\"\"\"\n",
    "    \n",
    "    # --- CONFIGURATION ---\n",
    "    FILENAME = 'user_panel_full_history.parquet'\n",
    "    # WARNING: lme4 is very slow. A 1% sample is a good starting point.\n",
    "    # Increasing this significantly will dramatically increase runtime and memory usage.\n",
    "    SAMPLE_FRACTION = 0.25  # Use 1% of users.\n",
    "    # ---------------------\n",
    "\n",
    "    rpy2_is_ready, ro, pandas2ri, localconverter = setup_rpy2()\n",
    "    if not rpy2_is_ready:\n",
    "        sys.exit(1)\n",
    "\n",
    "    df_analysis = load_and_prepare_data(FILENAME, sample_fraction=SAMPLE_FRACTION)\n",
    "\n",
    "    if df_analysis is not None and not df_analysis.is_empty():\n",
    "        beta_u_df = run_mixed_effects_model(df_analysis, ro, pandas2ri, localconverter)\n",
    "        \n",
    "        if beta_u_df is not None:\n",
    "            print(\"\\n--- Summary of Regularized User-Specific Slopes (βu) ---\")\n",
    "            # Use the .describe() method on the results DataFrame\n",
    "            summary_table = beta_u_df.describe()\n",
    "            print(summary_table)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f28e54",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
