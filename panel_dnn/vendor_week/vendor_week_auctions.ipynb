{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vendor-Week Panel Data Extraction: Auctions\n",
    "\n",
    "This notebook orchestrates the extraction of vendor auction data from Snowflake, processing it one day at a time and then aggregating to weekly panels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Environment loaded successfully\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timedelta\n",
    "from tqdm import tqdm\n",
    "from dotenv import load_dotenv\n",
    "import snowflake.connector\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Set up plotting style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"‚úÖ Environment loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define Analysis Period and Create Date Ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÖ Analysis Period: 2025-03-14 to 2025-09-07\n",
      "üìä Total days: 178\n",
      "üìà Total weeks: 25\n",
      "\n",
      "üóìÔ∏è First 5 weeks:\n",
      "  Week of 2025-03-17 to 2025-03-23\n",
      "  Week of 2025-03-24 to 2025-03-30\n",
      "  Week of 2025-03-31 to 2025-04-06\n",
      "  Week of 2025-04-07 to 2025-04-13\n",
      "  Week of 2025-04-14 to 2025-04-20\n"
     ]
    }
   ],
   "source": [
    "# Define the analysis period\n",
    "ANALYSIS_START_DATE = '2025-03-14'\n",
    "ANALYSIS_END_DATE = '2025-09-07'\n",
    "\n",
    "# Generate daily date range\n",
    "start_date = pd.to_datetime(ANALYSIS_START_DATE)\n",
    "end_date = pd.to_datetime(ANALYSIS_END_DATE)\n",
    "date_list = pd.date_range(start=start_date, end=end_date, freq='D')\n",
    "\n",
    "# Generate weekly date range\n",
    "week_list = pd.date_range(start=start_date, end=end_date, freq='W-MON')\n",
    "\n",
    "print(f\"üìÖ Analysis Period: {ANALYSIS_START_DATE} to {ANALYSIS_END_DATE}\")\n",
    "print(f\"üìä Total days: {len(date_list)}\")\n",
    "print(f\"üìà Total weeks: {len(week_list)}\")\n",
    "print(f\"\\nüóìÔ∏è First 5 weeks:\")\n",
    "for week in week_list[:5]:\n",
    "    week_end = week + timedelta(days=6)\n",
    "    print(f\"  Week of {week.strftime('%Y-%m-%d')} to {week_end.strftime('%Y-%m-%d')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Extract Daily Auction Data\n",
    "\n",
    "Run the extraction script to pull auction data one day at a time. This process is resumable - if it fails, you can run it again and it will skip already processed days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "DAILY AUCTION DATA EXTRACTION\n",
      "============================================================\n",
      "Analysis period: 2025-03-14 to 2025-09-07\n",
      "Output directory: data/vendor_daily_pulls/auctions\n",
      "\n",
      "‚úÖ Connected to Snowflake\n",
      "üìä Processing 178 days of data...\n",
      "\n",
      "Extracting daily auction data:   1%|        | 1/178 [04:08<12:13:00, 248.48s/it]^C\n",
      "\n",
      "Force exit\n",
      "Extracting daily auction data:   1%|        | 1/178 [06:32<19:18:06, 392.58s/it]\n"
     ]
    }
   ],
   "source": [
    "# Run the daily extraction script\n",
    "!python extract_auctions_daily.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Verify Daily Data Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Found 0 daily files\n",
      "üìÇ Expected 178 files\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Check extracted files\n",
    "daily_data_dir = Path('data/vendor_daily_pulls/auctions')\n",
    "parquet_files = sorted(daily_data_dir.glob('auctions_*.parquet'))\n",
    "\n",
    "print(f\"üìÅ Found {len(parquet_files)} daily files\")\n",
    "print(f\"üìÇ Expected {len(date_list)} files\")\n",
    "\n",
    "if parquet_files:\n",
    "    # Check coverage\n",
    "    missing_dates = []\n",
    "    for date in date_list:\n",
    "        date_str = date.strftime('%Y-%m-%d')\n",
    "        file_path = daily_data_dir / f\"auctions_{date_str}.parquet\"\n",
    "        if not file_path.exists():\n",
    "            missing_dates.append(date_str)\n",
    "    \n",
    "    if missing_dates:\n",
    "        print(f\"\\n‚ö†Ô∏è Missing {len(missing_dates)} files:\")\n",
    "        for date in missing_dates[:10]:  # Show first 10\n",
    "            print(f\"  - {date}\")\n",
    "        if len(missing_dates) > 10:\n",
    "            print(f\"  ... and {len(missing_dates) - 10} more\")\n",
    "    else:\n",
    "        print(\"\\n‚úÖ All expected files present\")\n",
    "    \n",
    "    # Sample a file to check structure\n",
    "    sample_file = parquet_files[len(parquet_files)//2]  # Middle file\n",
    "    df_sample = pd.read_parquet(sample_file)\n",
    "    \n",
    "    print(f\"\\nüìã Sample file: {sample_file.name}\")\n",
    "    print(f\"Shape: {df_sample.shape}\")\n",
    "    print(f\"\\nColumns: {', '.join(df_sample.columns)}\")\n",
    "    print(f\"\\nFirst 5 rows:\")\n",
    "    display(df_sample.head())\n",
    "    \n",
    "    print(f\"\\nSummary statistics:\")\n",
    "    display(df_sample.describe().round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Aggregate to Weekly Panels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the weekly aggregation script\n",
    "!python aggregate_to_weekly.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Load and Explore Weekly Panel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the aggregated weekly panel\n",
    "weekly_panel_file = Path('data/vendor_weekly_panels/vendor_weekly_auctions_panel.parquet')\n",
    "\n",
    "if weekly_panel_file.exists():\n",
    "    df_panel = pd.read_parquet(weekly_panel_file)\n",
    "    print(f\"‚úÖ Loaded panel with {len(df_panel):,} observations\")\n",
    "    print(f\"\\nüìä Panel structure:\")\n",
    "    print(f\"  - Vendors: {df_panel['vendor_id'].nunique():,}\")\n",
    "    print(f\"  - Weeks: {df_panel['week'].nunique():,}\")\n",
    "    print(f\"  - Date range: {df_panel['week'].min()} to {df_panel['week'].max()}\")\n",
    "    \n",
    "    # Display sample\n",
    "    print(f\"\\nüìã Sample data:\")\n",
    "    display(df_panel.head(10))\n",
    "    \n",
    "    # Check for balance\n",
    "    expected_obs = df_panel['vendor_id'].nunique() * df_panel['week'].nunique()\n",
    "    print(f\"\\nüîÑ Panel balance:\")\n",
    "    print(f\"  - Expected observations (balanced): {expected_obs:,}\")\n",
    "    print(f\"  - Actual observations: {len(df_panel):,}\")\n",
    "    print(f\"  - Panel is {'balanced ‚úÖ' if len(df_panel) == expected_obs else 'unbalanced ‚ö†Ô∏è'}\")\n",
    "else:\n",
    "    print(\"‚ùå Weekly panel file not found. Please run the aggregation script first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualize Auction Activity Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'df_panel' in locals() and not df_panel.empty:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # 1. Total auctions over time\n",
    "    weekly_totals = df_panel.groupby('week')['auctions'].sum()\n",
    "    axes[0, 0].plot(weekly_totals.index, weekly_totals.values, linewidth=2)\n",
    "    axes[0, 0].set_title('Total Auctions Over Time')\n",
    "    axes[0, 0].set_xlabel('Week')\n",
    "    axes[0, 0].set_ylabel('Total Auctions')\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Average win rate over time\n",
    "    weekly_winrate = df_panel[df_panel['bids'] > 0].groupby('week')['win_rate'].mean()\n",
    "    axes[0, 1].plot(weekly_winrate.index, weekly_winrate.values, linewidth=2, color='green')\n",
    "    axes[0, 1].set_title('Average Win Rate Over Time')\n",
    "    axes[0, 1].set_xlabel('Week')\n",
    "    axes[0, 1].set_ylabel('Win Rate')\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Distribution of auctions per vendor\n",
    "    vendor_activity = df_panel.groupby('vendor_id')['auctions'].sum()\n",
    "    axes[1, 0].hist(vendor_activity[vendor_activity > 0], bins=50, edgecolor='black', alpha=0.7)\n",
    "    axes[1, 0].set_title('Distribution of Total Auctions per Vendor')\n",
    "    axes[1, 0].set_xlabel('Total Auctions')\n",
    "    axes[1, 0].set_ylabel('Number of Vendors')\n",
    "    axes[1, 0].set_yscale('log')\n",
    "    \n",
    "    # 4. Active vendors over time\n",
    "    active_vendors = df_panel[df_panel['auctions'] > 0].groupby('week')['vendor_id'].nunique()\n",
    "    axes[1, 1].plot(active_vendors.index, active_vendors.values, linewidth=2, color='orange')\n",
    "    axes[1, 1].set_title('Number of Active Vendors Over Time')\n",
    "    axes[1, 1].set_xlabel('Week')\n",
    "    axes[1, 1].set_ylabel('Active Vendors')\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Summary statistics\n",
    "    print(\"\\nüìä Key Metrics Summary:\")\n",
    "    print(f\"  - Average auctions per vendor-week: {df_panel['auctions'].mean():.1f}\")\n",
    "    print(f\"  - Median auctions per vendor-week: {df_panel['auctions'].median():.1f}\")\n",
    "    print(f\"  - Average win rate (when bidding): {df_panel[df_panel['bids'] > 0]['win_rate'].mean():.3f}\")\n",
    "    print(f\"  - Vendors with at least one auction: {(vendor_activity > 0).sum():,} ({(vendor_activity > 0).mean():.1%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Export for Further Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'df_panel' in locals() and not df_panel.empty:\n",
    "    # Create different export formats\n",
    "    export_dir = Path('data/exports')\n",
    "    export_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # 1. Full panel for modeling\n",
    "    df_panel.to_parquet(export_dir / 'vendor_weekly_panel_full.parquet', index=False)\n",
    "    print(\"‚úÖ Exported full panel to Parquet\")\n",
    "    \n",
    "    # 2. Active vendors only (at least one auction in the period)\n",
    "    active_vendors = df_panel.groupby('vendor_id')['auctions'].sum()\n",
    "    active_vendor_ids = active_vendors[active_vendors > 0].index\n",
    "    df_active = df_panel[df_panel['vendor_id'].isin(active_vendor_ids)]\n",
    "    df_active.to_parquet(export_dir / 'vendor_weekly_panel_active_only.parquet', index=False)\n",
    "    print(f\"‚úÖ Exported active vendors panel ({len(active_vendor_ids):,} vendors)\")\n",
    "    \n",
    "    # 3. Summary statistics CSV\n",
    "    summary_stats = df_panel.groupby('vendor_id').agg({\n",
    "        'auctions': ['sum', 'mean', 'std'],\n",
    "        'bids': ['sum', 'mean'],\n",
    "        'wins': ['sum', 'mean'],\n",
    "        'win_rate': 'mean',\n",
    "        'avg_rank': 'mean'\n",
    "    }).round(2)\n",
    "    summary_stats.columns = ['_'.join(col).strip() for col in summary_stats.columns.values]\n",
    "    summary_stats.to_csv(export_dir / 'vendor_summary_statistics.csv')\n",
    "    print(\"‚úÖ Exported summary statistics to CSV\")\n",
    "    \n",
    "    print(f\"\\nüìÅ All exports saved to: {export_dir.absolute()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Data Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'df_panel' in locals() and not df_panel.empty:\n",
    "    print(\"üîç Running data quality checks...\\n\")\n",
    "    \n",
    "    # Check 1: Win rate bounds\n",
    "    invalid_winrate = df_panel[(df_panel['win_rate'] < 0) | (df_panel['win_rate'] > 1)]\n",
    "    status = \"‚úÖ\" if len(invalid_winrate) == 0 else \"‚ùå\"\n",
    "    print(f\"{status} Win rate in [0, 1]: {len(invalid_winrate)} invalid rows\")\n",
    "    \n",
    "    # Check 2: Wins <= Bids\n",
    "    invalid_wins = df_panel[df_panel['wins'] > df_panel['bids']]\n",
    "    status = \"‚úÖ\" if len(invalid_wins) == 0 else \"‚ùå\"\n",
    "    print(f\"{status} Wins ‚â§ Bids: {len(invalid_wins)} invalid rows\")\n",
    "    \n",
    "    # Check 3: Bids <= Auctions (a vendor can bid multiple times per auction)\n",
    "    # This is actually okay, so we check if bids are reasonable\n",
    "    bids_per_auction = df_panel[df_panel['auctions'] > 0]['bids'] / df_panel[df_panel['auctions'] > 0]['auctions']\n",
    "    unreasonable_bids = (bids_per_auction > 100).sum()  # More than 100 bids per auction seems unreasonable\n",
    "    status = \"‚úÖ\" if unreasonable_bids == 0 else \"‚ö†Ô∏è\"\n",
    "    print(f\"{status} Reasonable bids per auction: {unreasonable_bids} vendors with >100 bids/auction\")\n",
    "    \n",
    "    # Check 4: No negative values\n",
    "    numeric_cols = ['auctions', 'bids', 'wins', 'distinct_campaigns_bid', 'distinct_products_bid']\n",
    "    negative_values = (df_panel[numeric_cols] < 0).sum().sum()\n",
    "    status = \"‚úÖ\" if negative_values == 0 else \"‚ùå\"\n",
    "    print(f\"{status} No negative counts: {negative_values} negative values found\")\n",
    "    \n",
    "    # Check 5: Panel completeness\n",
    "    missing_values = df_panel[['vendor_id', 'week', 'auctions', 'bids', 'wins']].isna().sum()\n",
    "    total_missing = missing_values.sum()\n",
    "    status = \"‚úÖ\" if total_missing == 0 else \"‚ùå\"\n",
    "    print(f\"{status} Core columns complete: {total_missing} missing values\")\n",
    "    \n",
    "    # Check 6: Date consistency\n",
    "    df_panel['week'] = pd.to_datetime(df_panel['week'])\n",
    "    week_gaps = df_panel['week'].sort_values().unique()\n",
    "    expected_weeks = pd.date_range(start=week_gaps.min(), end=week_gaps.max(), freq='W-MON')\n",
    "    missing_weeks = set(expected_weeks) - set(week_gaps)\n",
    "    status = \"‚úÖ\" if len(missing_weeks) == 0 else \"‚ö†Ô∏è\"\n",
    "    print(f\"{status} Continuous weekly coverage: {len(missing_weeks)} missing weeks\")\n",
    "    \n",
    "    print(\"\\n‚úÖ Data quality checks complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
